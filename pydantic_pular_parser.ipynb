{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Draft or create a class system to represent the pular entries. \n",
    "Ideally this will contain a way to nest entry objects under a root\n",
    "'''\n",
    "#%pip install docx\n",
    "#%pip install python-docx #this mutates docx? \n",
    "#%pip install pydantic\n",
    "#%pip install mypy\n",
    "# %pip install numpy\n",
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "from pydantic import BaseModel, ValidationError, validator, root_validator, Field, constr\n",
    "import json\n",
    "import docx\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# output_name = f\"{current_time}_result.txt\"\n",
    "# experiment = input(\"Enter emperiment description:\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"../logs_and_outputs/initialization_placeholder.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "# # Test messages\n",
    "logger.debug(\"current_time\")\n",
    "# logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Javascript\n",
    "\n",
    "# script = '''\n",
    "# require([\"base/js/namespace\"],function(Jupyter) {\n",
    "#     Jupyter.notebook.save_checkpoint();\n",
    "# });\n",
    "# '''\n",
    "# Javascript(script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing _Docx_Paragraph__Docx_Run__Docx_Run_List\n"
     ]
    }
   ],
   "source": [
    "class Docx_Paragraph (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   # docx_document_paragraph: Optional[Any] #This should be validated below. Left optional because its inclusion causes problems with default repr and serialization\n",
    "   para_text: str = Field(..., min_length = 1) ##required, must be string, must be 1 long or more\n",
    "   para_first_line_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "   para_left_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      #extract para features, \n",
    "      new_values['para_text'] = para.text #type: ignore\n",
    "      new_values['para_first_line_indent'] = para.paragraph_format.first_line_indent #type: ignore\n",
    "      new_values['para_left_indent'] = para.paragraph_format.left_indent #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run (BaseModel):\n",
    "   \"\"\"input:   run = your_run_here\n",
    "   \n",
    "   when given a docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   run_text : str = Field(..., min_length = 1) #required, must be string, must be 1 long or more\n",
    "   run_font_name : Optional[str] = Field(...) #required, must be string or None value\n",
    "   run_font_size_pt : Optional[float] = Field(...)#Required, but must be float OR none value\n",
    "   run_bold : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "   run_italic : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      run = values.get(\"run\",False)\n",
    "      assert isinstance(run, eval('docx.text.run.Run')), 'please enter a docx run assigned to the variable \"run\", in the form of     run = your_run_here'\n",
    "      \n",
    "      new_values : Dict[str, Any] = {}\n",
    "      #loop through the runs in the paragraph and select the desired features\n",
    "      new_values['run_text'] = run.text #type: ignore\n",
    "      new_values['run_font_name'] = run.font.name #type: ignore\n",
    "      if run.font.size is not None: #type: ignore\n",
    "         new_values['run_font_size_pt'] = run.font.size.pt #type: ignore\n",
    "      else: new_values['run_font_size_pt'] = None\n",
    "      new_values['run_bold'] = run.bold #type: ignore\n",
    "      new_values['run_italic'] = run.italic #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run_List (BaseModel):\n",
    "   \"\"\"input:   run_list = your_runs_in_a_list\n",
    "   \n",
    "   when given a list of docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   #because the internals are validated, don't need to validate these other than that they were made into lists\n",
    "   run_text : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_name : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_size_pt : List[Any] = Field(...) #Required, must be list\n",
    "   run_bold : List[Any] = Field(...) #Required, must be list\n",
    "   run_italic : List[Any] = Field(...) #Required, must be list\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "      from collections import defaultdict\n",
    "      paragraph_enumeration = values.get('paragraph_enumeration',\"<<FAILURE_paragraph_enumeration>>\")\n",
    "      runs = values.get(\"run_list\",False)\n",
    "      if not runs:\n",
    "         raise ValueError('please enter a docx run list assigned to the variable \"run_list\", in the form of     run_list = your_run_list_here')\n",
    "      new_values = defaultdict(list)\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      logger_details = {'function':'parsed_run', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      \n",
    "      for run_enumumeration, run in enumerate(runs): #type: ignore\n",
    "         try:\n",
    "            parsed_run = Docx_Run(**{'run':run}) #this manner of root unpacking seems to give warnings since linter can't assess ahead of time\n",
    "            assert isinstance(parsed_run, Docx_Run), 'RUNTIME_ERR - the docx run object did not return the type expected'\n",
    "            for k,v in parsed_run.dict().items():\n",
    "               new_values[k].append(v) \n",
    "\n",
    "         except BaseException as e:\n",
    "            new_e = logger_root_validation_error_messages(e, logger_details, suppress,run_enumeration=run_enumumeration)\n",
    "            raise new_e\n",
    "             \n",
    "      return new_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress, tee\n",
    "\n",
    "def pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def logger_root_validation_error_messages(e, logger_details, suppress = [], run_enumeration: Optional[int] = None) -> Union[RuntimeError, TypeError]:      \n",
    "   #TODO add ability to handle assertion errors\n",
    "   if run_enumeration is not None:\n",
    "      run_num = f\"|run#{run_enumeration}|\" #type: ignore \n",
    "   else:\n",
    "      run_num = \"\"\n",
    "   try:\n",
    "      for err in e.errors():\n",
    "         if err['type'] in suppress['type'] or err['msg'] in suppress['msg']:\n",
    "            logger.info(f\"|SUPRESSED|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"suppressed Validation Error\")\n",
    "         else:\n",
    "            logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"un-suppressed Validation Error\")\n",
    "   except:\n",
    "      logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with error: {e}\")\n",
    "      return RuntimeError(\"non-validation error\")\n",
    "   return RuntimeError(\"non-validation error\")\n",
    "\n",
    "\n",
    "def pular_str_strip_check(s:str) ->bool:\n",
    "   in_len = len(s)\n",
    "   new_s = s.strip()\n",
    "   out_len = len(new_s)\n",
    "   purported_whitespace: bool = in_len != out_len\n",
    "   return purported_whitespace\n",
    "\n",
    "\n",
    "class Docx_Paragraph_and_Runs (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "\n",
    "   class Config:\n",
    "      extra = 'allow'\n",
    "      # arbitrary_types_allowed = True\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "      \n",
    "      paragraph_enumeration: int = values.get('paragraph_enumeration',None)\n",
    "      assert isinstance(paragraph_enumeration, int), \"assertion error, bad paragraph count/paragraph_enumeration value passed. Please pass an integer\"\n",
    "      new_values['paragraph_enumeration'] = paragraph_enumeration\n",
    "\n",
    "      \n",
    "      #setting up error and logger handling\n",
    "      #suppress these errors\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      #try to extract para features, \n",
    "      logger_details = {'function':'Docx_Paragraph', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      try: \n",
    "         parsed_paras = Docx_Paragraph(**{'paragraph':para}) #type: ignore\n",
    "         for k,v in parsed_paras.dict().items():\n",
    "            new_values[k] = v\n",
    "      # except ValidationError as e:\n",
    "      #    logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "\n",
    "      #try to extract runs features\n",
    "      logger_details = {'function':'Docx_Run_List', 'paragraph_enumeration':paragraph_enumeration }    \n",
    "      try:\n",
    "         parsed_runs = Docx_Run_List(**{'run_list':para.runs, 'paragraph_enumeration':paragraph_enumeration}) #type: ignore\n",
    "         for k,v in parsed_runs.dict().items():\n",
    "            new_values[k] = v\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "         \n",
    "      return new_values\n",
    "      \n",
    "\n",
    "   def interogate__para_text(self) -> str:\n",
    "      t = getattr(self, 'para_text', \"\")\n",
    "      # \n",
    "      if len(t) == 0:\n",
    "         logger.warning('interogator did not find para_text')\n",
    "      #    print(\"no para_text with:\\n\\t\", self.dict())\n",
    "      return t\n",
    "\n",
    "   def paragraph_logger(self,level:int,msg:str,print_bool:bool):\n",
    "      if print_bool:\n",
    "         print(msg)\n",
    "      else:\n",
    "         logger.log(level,msg)\n",
    "\n",
    "\n",
    "   def single_run_feature_identify(self,params:Dict[str,Any]) -> Tuple[bool,List[bool],List[Any]]: \n",
    "      enumeration : Optional[int] = getattr(self,\"paragraph_enumeration\",None)\n",
    "      assert isinstance(enumeration, int),f\"bad value for 'paragraph_enumeration' {enumeration}\"\n",
    "      feature = params['docxFeature']\n",
    "      assert isinstance(feature,str),f\"bad value for parameter 'docxFeature'. Check params: {params}\"\n",
    "\n",
    "      list_from_runs: List[Optional[Union[float,bool]]] = getattr(self,feature,[None]) \n",
    "      mask: List[bool] = [True if x == params['value'] else False for x in list_from_runs]\n",
    "\n",
    "      if any(mask):\n",
    "         return True, mask, list_from_runs  #has Feature\n",
    "      else:\n",
    "         return False, mask, list_from_runs #does not have feature\n",
    "\n",
    "   def modify_run_lists(self, drop_runs: Optional[List[int]] = None, add_runs: Optional[Tuple[int, List[List[Any]]]] = None, merge_runs : Optional[bool] = None) -> Optional[Dict[str, List[List[Any]]]]:\n",
    "      \"\"\"given a list of indexes as 'drop' will drop those indexes from runlists, and return those dropped\n",
    "      given a tuple with an integer index and list of lists (run aligned), will add those to entries to the runlists at that index\n",
    "      given bool merge, will greedy merge all runs with the same run features EXCEPT run_text. Run_texts will be concatenated\n",
    "      \"\"\"\n",
    "      run_list_req_features: List[str] = Docx_Run_List.schema()['required']\n",
    "      assert run_list_req_features[0] == 'run_list', \"first feature in the schema should be run_list\"\n",
    "      feature_run_lists : List[List[Any]] = []\n",
    "      for f in run_list_req_features:\n",
    "         feature_run_lists.append(getattr(self,f,[]))\n",
    "      pivoted_run_lists = list(map(list, zip(*feature_run_lists)))\n",
    "      number_of_runs : int = len(pivoted_run_lists)\n",
    "      if number_of_runs < 1:\n",
    "         raise ValueError('this paragraph does not have values in the run lists')\n",
    "\n",
    "      if drop_runs is not None:\n",
    "         dropped_runs = {}\n",
    "         for ind in drop_runs:\n",
    "            dropped_runs[ind] = pivoted_run_lists.pop(ind)\n",
    "         if number_of_runs == len(pivoted_run_lists):\n",
    "            raise RuntimeError('the runs_lists were not shortened as expected')\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "      if add_runs is not None:\n",
    "         insert_ind = add_runs[0]\n",
    "         add_lists = add_runs[1]\n",
    "         assert len(add_lists[0]) == number_of_runs, \"the added list of lists must have runs of the same length (feature space) as run_lists features in the schema: Docx_Run_List.schema()['required']\"\n",
    "         if insert_ind == -1:\n",
    "            insert_ind = number_of_runs\n",
    "         for lst in add_lists:\n",
    "            pivoted_run_lists.insert(insert_ind,lst)\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "      \n",
    "      if merge_runs is not None:\n",
    "         still_merging = True\n",
    "         i = 0\n",
    "         merge_occured = False\n",
    "         while still_merging:\n",
    "            for a,b in pairwise(list(range(len(pivoted_run_lists)))):\n",
    "               if pivoted_run_lists[a][1:] == pivoted_run_lists[b][1:]:\n",
    "                  pivoted_run_lists[b][0] = pivoted_run_lists[a][0] + pivoted_run_lists[b][0]\n",
    "                  pivoted_run_lists.pop(a)\n",
    "                  merge_occured = True\n",
    "               else: pass\n",
    "            if not merge_occured:\n",
    "               still_merging = False\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "      if any([drop_runs is not None, add_runs is not None, merge_runs is not None]):\n",
    "         for i, f in enumerate(run_list_req_features):\n",
    "            self.__setattr__(f,feature_run_lists[i])\n",
    "\n",
    "\n",
    "      \n",
    "   # def cleaner(self,params:Dict[str,Any]):\n",
    "\n",
    "   #    def remove_para_leading_whitespace(self):\n",
    "   #       this_run_len = len(self.run_text[0])\n",
    "   #       stripped_run = self.run_text[0].lstrip()\n",
    "   #       # if len(stripped_run) == 0:\n",
    "   #          # self\n",
    "\n",
    "# run_text=['WOOP-  ', 'woopude  ', 'var.- woofude; V. woof- (1)  ', 'Dcz  C<FJ>,Z<FJ,FT>']\n",
    "# run_font_name=[None, 'TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt']\n",
    "# run_font_size_pt=[12.0, None, None, 8.0]\n",
    "# run_italic=[None, None, True, None]\n",
    "# run_bold=[None, True, None, None]\n",
    "\n",
    "\n",
    "\n",
    "# features = [run_text,run_font_name,run_font_size_pt,run_italic,run_bold]\n",
    "# # print(features)\n",
    "# rotated_features = list(zip(*features))\n",
    "# # print(rotated_features)\n",
    "# rotated_features2 = list(zip(*rotated_features))\n",
    "# rotated_features3 = list(map(list, zip(*rotated_features)))\n",
    "# print(rotated_features2)\n",
    "# print(rotated_features3)\n",
    "# # print(list(pairwise(rotated_features3)))\n",
    "# print(features[:0]+features[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fula_Entry (BaseModel): \n",
    "   entity_word: Optional[str]\n",
    "   features: Optional[Dict[str,str]] = {}\n",
    "   paragraphs_list: List[Any]\n",
    "   paragraphs_extr : List[Docx_Paragraph_and_Runs]\n",
    "   sub_roots = []\n",
    "   lemmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "output_name = f\"logs_and_outputs/{current_time}_docxFileParseResult.txt\"\n",
    "experiment = input(\"Enter emperiment description:\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "#Run docx module to parse the docx file\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "document = Document(docx_filename)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "char_counts = Counter()\n",
    "\n",
    "docx_object_list = []\n",
    "parsed_object_list = []\n",
    "failed_paras_ind = []\n",
    "\n",
    "for i, para in enumerate(document.paragraphs):\n",
    "   docx_object_list.append((i,para))\n",
    "   try:\n",
    "      entryObj = Docx_Paragraph_and_Runs(**{'paragraph': para, 'paragraph_enumeration': i})\n",
    "      char_counts.update(entryObj.interogate__para_text())\n",
    "      parsed_object_list.append((i,entryObj))\n",
    "      if char_counts.get(repr('\\n'),False):\n",
    "         print(i)\n",
    "   except BaseException as e:\n",
    "      failed_paras_ind.append((i,para))\n",
    "      if not e.args[0][0].exc.args[0] == 'suppressed Validation Error':\n",
    "         print('\\npara number: ',i)\n",
    "\n",
    "      # p_text = entryObj.interogate__para_text()\n",
    "      # if not set(p_text).isdisjoint(low_freq_odd_chars):\n",
    "      #    msg = 'rare_characters\\t\\t'+p_text\n",
    "      #    entryObj.paragraph_logger(level=40,msg = msg, print_bool=False)\n",
    "print(failed_paras_ind)\n",
    "print('i can print')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32040\n"
     ]
    }
   ],
   "source": [
    "print(len(parsed_object_list))\n",
    "\n",
    "with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "output_name = f\"logs_and_outputs/{current_time}_docxFileParseResult.txt\"\n",
    "experiment = input(\"Enter emperiment description:\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "#Run docx module to parse the docx file\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "document = Document(docx_filename)\n",
    "\n",
    "# docx_object_list = []\n",
    "# parsed_object_list = []\n",
    "# failed_paras_ind = []\n",
    "root_ind_list = []\n",
    "lemma_ind_list = []\n",
    "rejected_ind_list = []\n",
    "\n",
    "for i, entryObj in parsed_object_list:\n",
    "\n",
    "   try:\n",
    "      featureConfig = {\n",
    "      'root': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0},\n",
    "      'lemma': {'docxFeature': 'run_bold',\n",
    "               'strSummary':'fontBold', \n",
    "               'value':True},\n",
    "      }\n",
    "      \n",
    "      is_root = entryObj.single_run_feature_identify(featureConfig['root'])\n",
    "      if is_root:\n",
    "         print('\\n\\nroot at para number: ',i)\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t',paraText)\n",
    "         root_ind_list.append(i)\n",
    "\n",
    "      is_lemma = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      if is_lemma:\n",
    "         entryObj.interogate__para_text()\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t\\tp#',i,'\\t\\t',paraText)\n",
    "         lemma_ind_list.append(i)\n",
    "\n",
    "   except BaseException as e:\n",
    "      rejected_ind_list.append(i)\n",
    "      if not e.args[0][0].exc.args[0] == 'suppressed Validation Error':\n",
    "         print('\\npara number: ',i)\n",
    "\n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "print('roots: ',len(root_ind_list))\n",
    "print('lemmas: ',len(lemma_ind_list))\n",
    "print('additional rejects: ',len(rejected_ind_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#these frequencies were copied from a previous run, and only from successfully parsed objects\n",
    "#the lowest frequencies were reviewed and selections pulled from those\n",
    "   # low_freq_odd_chars = ('\\t', 72), ('5', 67), ('`', 64), ('&', 49), ('ù', 30), ('ï', 26), ('X', 25), ('!', 15), ('\"', 14), ('ò', 8), ('=', 4), ('Q', 4), ('\\xa0', 1)\n",
    "   # low_freq_odd_chars = [x[0] for x in low_freq_odd_chars]\n",
    "#numbers do not appear to be used outside of scholarly references and some multiple-root instances\n",
    "   # nums = list(range(10))\n",
    "#X for example, is almost only in english or french glosses, or scholarly references)\n",
    "   #('X', 25),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning notes\n",
    "# `new Kunari' - region in western Niger ; `nouveau Kounari' - région dans l'ouest du Niger\n",
    "   #here the ` seems to be used at the beginning of a quotation, and a normal apostrophe at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# # char_counts\n",
    "sorted_char_val = sorted(char_counts.items(), key=lambda item: (-item[1], item[0]))\n",
    "print(sorted_char_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
