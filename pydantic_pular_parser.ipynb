{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Draft or create a class system to represent the pular entries. \n",
    "Ideally this will contain a way to nest entry objects under a root\n",
    "'''\n",
    "#%pip install docx\n",
    "#%pip install python-docx #this mutates docx? \n",
    "#%pip install pydantic\n",
    "#%pip install mypy\n",
    "# %pip install numpy\n",
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "from pydantic import BaseModel, ValidationError, validator, root_validator, Field, constr\n",
    "import json\n",
    "import docx\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from itertools import compress, tee, chain\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "from verbalexpressions import VerEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# output_name = f\"{current_time}_result.txt\"\n",
    "# experiment = input(\"Enter emperiment description:\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/initialization_placeholder.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "# # Test messages\n",
    "logger.debug(\"current_time\")\n",
    "# logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "    \n",
    "def logger_root_validation_error_messages(e, logger_details, suppress = [], run_enumeration: Optional[int] = None) -> Union[RuntimeError, TypeError]:      \n",
    "   #TODO add ability to handle assertion errors\n",
    "   if run_enumeration is not None:\n",
    "      run_num = f\"|run#{run_enumeration}|\" #type: ignore \n",
    "   else:\n",
    "      run_num = \"\"\n",
    "   try:\n",
    "      for err in e.errors():\n",
    "         if err['type'] in suppress['type'] or err['msg'] in suppress['msg']:\n",
    "            logger.info(f\"|SUPRESSED|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"suppressed Validation Error\")\n",
    "         else:\n",
    "            logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"un-suppressed Validation Error\")\n",
    "   except:\n",
    "      logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with error: {e}\")\n",
    "      return RuntimeError(\"non-validation error\")\n",
    "   return RuntimeError(\"non-validation error\")\n",
    "\n",
    "def pular_str_strip_check(s:str) ->bool:\n",
    "   in_len = len(s)\n",
    "   new_s = s.strip()\n",
    "   out_len = len(new_s)\n",
    "   purported_whitespace: bool = in_len != out_len\n",
    "   return purported_whitespace\n",
    "\n",
    "def closest(ranger, target): #any target indeces occuring before the first ranger index will be ignored\n",
    "   if not isinstance(target,np.ndarray):\n",
    "      target = np.array(target)\n",
    "   for a,b in ranger:\n",
    "      begin = np.searchsorted(target,a)\n",
    "      end = np.searchsorted(target,b)\n",
    "      _, out, target = np.split(target, [begin,end])\n",
    "      yield list(out)\n",
    "   yield list(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docx_Paragraph (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   # docx_document_paragraph: Optional[Any] #This should be validated below. Left optional because its inclusion causes problems with default repr and serialization\n",
    "   para_text: str = Field(..., min_length = 1) ##required, must be string, must be 1 long or more\n",
    "   para_first_line_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "   para_left_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      #extract para features, \n",
    "      new_values['para_text'] = para.text #type: ignore\n",
    "      new_values['para_first_line_indent'] = para.paragraph_format.first_line_indent #type: ignore\n",
    "      new_values['para_left_indent'] = para.paragraph_format.left_indent #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run (BaseModel):\n",
    "   \"\"\"input:   run = your_run_here\n",
    "   \n",
    "   when given a docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   run_text : str = Field(..., min_length = 1) #required, must be string, must be 1 long or more\n",
    "   run_font_name : Optional[str] = Field(...) #required, must be string or None value\n",
    "   run_font_size_pt : Optional[float] = Field(...)#Required, but must be float OR none value\n",
    "   run_bold : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "   run_italic : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      run = values.get(\"run\",False)\n",
    "      assert isinstance(run, eval('docx.text.run.Run')), 'please enter a docx run assigned to the variable \"run\", in the form of     run = your_run_here'\n",
    "      \n",
    "      new_values : Dict[str, Any] = {}\n",
    "      #loop through the runs in the paragraph and select the desired features\n",
    "      new_values['run_text'] = run.text #type: ignore\n",
    "      new_values['run_font_name'] = run.font.name #type: ignore\n",
    "      if run.font.size is not None: #type: ignore\n",
    "         new_values['run_font_size_pt'] = run.font.size.pt #type: ignore\n",
    "      else: new_values['run_font_size_pt'] = None\n",
    "      new_values['run_bold'] = run.bold #type: ignore\n",
    "      new_values['run_italic'] = run.italic #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run_List (BaseModel): #TODO refactor this to use run-aligned lists, so run obj can be used directly, and have its schema raised\n",
    "   \"\"\"input:   run_list = your_runs_in_a_list\n",
    "   \n",
    "   when given a list of docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   #because the internals are validated, don't need to validate these other than that they were made into lists\n",
    "   run_text : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_name : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_size_pt : List[Any] = Field(...) #Required, must be list\n",
    "   run_bold : List[Any] = Field(...) #Required, must be list\n",
    "   run_italic : List[Any] = Field(...) #Required, must be list\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "      from collections import defaultdict\n",
    "      paragraph_enumeration = values.get('paragraph_enumeration',\"<<FAILURE_paragraph_enumeration>>\")\n",
    "      runs = values.get(\"run_list\",False)\n",
    "      if not runs:\n",
    "         raise ValueError('please enter a docx run list assigned to the variable \"run_list\", in the form of     run_list = your_run_list_here')\n",
    "      new_values = defaultdict(list)\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      logger_details = {'function':'parsed_run', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      \n",
    "      for run_enumumeration, run in enumerate(runs): #type: ignore\n",
    "         try:\n",
    "            parsed_run = Docx_Run(**{'run':run}) #this manner of root unpacking seems to give warnings since linter can't assess ahead of time\n",
    "            assert isinstance(parsed_run, Docx_Run), 'RUNTIME_ERR - the docx run object did not return the type expected'\n",
    "            for k,v in parsed_run.dict().items():\n",
    "               new_values[k].append(v) \n",
    "\n",
    "         except BaseException as e:\n",
    "            new_e = logger_root_validation_error_messages(e, logger_details, suppress,run_enumeration=run_enumumeration)\n",
    "            raise new_e\n",
    "             \n",
    "      return new_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docx_Run.schema()\n",
    "# Docx_Run_List.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docx_Paragraph_and_Runs (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "\n",
    "   class Config:\n",
    "      extra = 'allow'\n",
    "      # arbitrary_types_allowed = True\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "      \n",
    "      paragraph_enumeration: int = values.get('paragraph_enumeration',None)\n",
    "      assert isinstance(paragraph_enumeration, int), \"assertion error, bad paragraph count/paragraph_enumeration value passed. Please pass an integer\"\n",
    "      new_values['paragraph_enumeration'] = paragraph_enumeration\n",
    "\n",
    "      \n",
    "      #setting up error and logger handling\n",
    "      #suppress these errors\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      #try to extract para features, \n",
    "      logger_details = {'function':'Docx_Paragraph', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      try: \n",
    "         parsed_paras = Docx_Paragraph(**{'paragraph':para}) #type: ignore\n",
    "         for k,v in parsed_paras.dict().items():\n",
    "            new_values[k] = v\n",
    "      # except ValidationError as e:\n",
    "      #    logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "\n",
    "      #try to extract runs features\n",
    "      logger_details = {'function':'Docx_Run_List', 'paragraph_enumeration':paragraph_enumeration }    \n",
    "      try:\n",
    "         parsed_runs = Docx_Run_List(**{'run_list':para.runs, 'paragraph_enumeration':paragraph_enumeration}) #type: ignore\n",
    "         for k,v in parsed_runs.dict().items():\n",
    "            new_values[k] = v\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "         \n",
    "      return new_values\n",
    "      \n",
    "   def interogate__para_text(self) -> str:\n",
    "      t = getattr(self, 'para_text', \"\")\n",
    "      # \n",
    "      if len(t) == 0:\n",
    "         logger.warning('interogator did not find para_text')\n",
    "      #    print(\"no para_text with:\\n\\t\", self.dict())\n",
    "      return t\n",
    "\n",
    "   def paragraph_logger(self,level:int,msg:str,print_bool:bool):\n",
    "      if print_bool:\n",
    "         print(msg)\n",
    "      else:\n",
    "         logger.log(level,msg)\n",
    "\n",
    "   def single_run_feature_identify(self,params:Dict[str,Any]) -> Tuple[bool,Tuple[List[bool],List[Any]],Tuple[List[bool],List[Optional[str]]]]: \n",
    "      \"\"\"if regex provided, must be in param dict with name 'text_regex_at_feature', and must be passed as a r'pattern' raw string\n",
    "      return tuple of ('feature boolean', feature_Tuple[boolean mask, feature list], regex_tuple[boolean mask, regex match list])\n",
    "      \"\"\"\n",
    "      enumeration : Optional[int] = getattr(self,\"paragraph_enumeration\",None)\n",
    "      assert isinstance(enumeration, int),f\"bad value for 'paragraph_enumeration' {enumeration}\"\n",
    "      run_texts : Optional[List[str]] = getattr(self,'run_text',None)\n",
    "      assert run_texts is not None, f\"bad value for 'run_text' {self.__repr__()}\"\n",
    "      feature = params['docxFeature']\n",
    "      assert isinstance(feature,str),f\"bad value for parameter 'docxFeature'. Check params: {params}\"\n",
    "      text_regex_at_feature = params.get('text_regex_at_feature',False)\n",
    "      regex_mask: List[bool] = []\n",
    "      regex_matches: List[Optional[str]] = []\n",
    "\n",
    "      values_from_runs: List[Optional[Union[float,bool]]] = getattr(self,feature,[None]) \n",
    "      value_mask: List[bool] = [True if x == params['value'] else False for x in values_from_runs]\n",
    "      \n",
    "      if any(value_mask):\n",
    "         # print('text and value mask: ',run_texts,value_mask)\n",
    "         # if text_regex_at_feature:\n",
    "            # pattern = text_regex_at_feature\n",
    "            # for text in run_texts:\n",
    "            #    match = re.search(pattern, text) #type: ignore\n",
    "            #    if match is not None:\n",
    "            #       regex_mask.append(True)\n",
    "            #       regex_matches.append(match.group(0))\n",
    "            #       # print(repr(self))\n",
    "            #    else:\n",
    "            #       regex_mask.append(False)\n",
    "            #       regex_matches.append(None)\n",
    "            # print('regex and match: ',regex_mask,regex_matches)\n",
    "            # # print(f'inside regex bool for para#{enumeration}\\tregex_mask_is: {regex_mask}\\t\\tvalue_mask is: {value_mask}')\n",
    "            # if not any(compress(value_mask,regex_mask)):\n",
    "            #    return False, (value_mask, values_from_runs), (regex_mask, regex_matches) #does not have feature\n",
    "         return True, (value_mask, run_texts), (regex_mask, regex_matches)  #has Feature\n",
    "      else:\n",
    "         return False, (value_mask, run_texts), (regex_mask, regex_matches) #does not have feature\n",
    "\n",
    "   def modify_run_lists(self, drop_runs: Optional[List[int]] = None, add_runs: Optional[Tuple[int, List[List[Any]]]] = None, merge_runs : bool = False): #-> Optional[Dict[str, List[List[Any]]]]\n",
    "      \"\"\"given a list of indexes as 'drop' will drop those indexes from runlists, and return those dropped\n",
    "      given a tuple with an integer index and list of lists (run aligned), will add those to entries to the runlists at that index\n",
    "      given bool merge, will greedy merge all runs with the same run features EXCEPT run_text. Run_texts will be concatenated\n",
    "      \"\"\"\n",
    "      run_list_req_features: List[str] = Docx_Run_List.schema()['required']\n",
    "      assert run_list_req_features[0] == 'run_text', \"first feature in the schema should be run_text\"\n",
    "      para_enumeration = getattr(self, 'paragraph_enumeration',None)\n",
    "      assert para_enumeration is not None, 'paragraph did not have an enumeration value'\n",
    "\n",
    "      feature_run_lists : List[List[Any]] = []\n",
    "      for f in run_list_req_features:\n",
    "         feature_run_lists.append(getattr(self,f,[]))\n",
    "      pivoted_run_lists = list(map(list, zip(*feature_run_lists)))\n",
    "      number_of_runs : int = len(pivoted_run_lists)\n",
    "      if number_of_runs < 1:\n",
    "         raise ValueError('this paragraph does not have values in the run lists')\n",
    "      \n",
    "      beginning_repr = self.__repr__()\n",
    "\n",
    "      if drop_runs is not None:\n",
    "         dropped_runs = {}\n",
    "         num_dropped = 0\n",
    "         for ind in drop_runs:\n",
    "            mut_ind = ind - num_dropped #mutate pivot indexes as the pivot array is mutated\n",
    "            dropped_runs[ind] = pivoted_run_lists.pop(mut_ind) #mutates pivoted_run_lists\n",
    "            num_dropped +=1\n",
    "         if number_of_runs == len(pivoted_run_lists):\n",
    "            raise RuntimeError('the runs_lists were not shortened as expected')\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         # print(dropped_runs,pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "         logger.info(f'para#{para_enumeration} had runs# {drop_runs} dropped. New run_text is: {feature_run_lists[0]}')\n",
    "\n",
    "      if add_runs is not None:\n",
    "         insert_ind : int = add_runs[0]\n",
    "         add_lists = add_runs[1]\n",
    "         assert len(add_lists[0]) == number_of_runs, \"the added list of lists must have runs of the same length (feature space) as run_lists features in the schema: Docx_Run_List.schema()['required']\"\n",
    "         if insert_ind == -1:\n",
    "            insert_ind = number_of_runs\n",
    "         for lst in add_lists:\n",
    "            pivoted_run_lists.insert(insert_ind,lst)\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "      \n",
    "      merge_occured = False\n",
    "      if merge_runs is not False:\n",
    "         i = 0\n",
    "         still_merging = True\n",
    "         # beginning_repr = self.__repr__()\n",
    "         while still_merging:\n",
    "            pairs = list(pairwise(list(range(len(pivoted_run_lists))))) #index pairs\n",
    "            if len(pairs) < 1: #onely 1 run, which causes pairwise to yield empty lists since nothing to pair with\n",
    "               break\n",
    "            num_merged = 0\n",
    "            for a,b in pairs: #where a,b are indexes in the pivoted run list (each index is one run)\n",
    "               a -= num_merged #mutate pivot indexes after the pivot array has been mutated\n",
    "               b -= num_merged\n",
    "               if pivoted_run_lists[a][1:] == pivoted_run_lists[b][1:]: #if all features EXCEPT run_text are the same #TODO add ability to config which features to merge on\n",
    "                  pivoted_run_lists[b][0] = pivoted_run_lists[a][0] + pivoted_run_lists[b][0]\n",
    "                  pivoted_run_lists.pop(a)\n",
    "                  num_merged +=1\n",
    "                  merge_occured = True #flag for end of function, to determine if any changes need to be set to 'self'\n",
    "               else: pass \n",
    "            if num_merged < 1: #if no merges where made in this iteration, merging is done. Else keep while loop since new merges may occur with new neighbors\n",
    "               still_merging = False\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "      if any([drop_runs is not None, add_runs is not None, merge_occured]):\n",
    "         for i, f in enumerate(run_list_req_features):\n",
    "            self.__setattr__(f,feature_run_lists[i])\n",
    "\n",
    "   def cleaner(self, execute_defaults: bool = True) -> bool : #params:Optional[Dict[str,Any]],\n",
    "      \"\"\"defaults to running \"remove_para_leading_whitespace\". This removes leading runs that are blank, and strips the first text run of any LEADING whitespace, if any is present.\n",
    "      the params dict is not implemented currently\n",
    "      returns bool value. True means cleaner would yield a valid para. False currently indicates all runs in para are whitespace.\n",
    "      \"\"\"\n",
    "      #TODO aggregate these getattrs so that every function doesn't need to get it themselves. Or simplify this with a function that has an assert bool to require it or not\n",
    "      para_enumeration = getattr(self, 'paragraph_enumeration',None)\n",
    "      assert para_enumeration is not None, 'paragraph did not have an enumeration value'\n",
    "\n",
    "      def remove_para_leading_whitespace(start_ind : int = 0): #run \n",
    "         # try: #expect to fail when reaches the end of the list\n",
    "         para_text : Optional[str] = getattr(self, 'para_text',None)\n",
    "         if isinstance(para_text,str):\n",
    "            if len(para_text.strip()) == 0: #if para's text is ONLY whitespace\n",
    "               return False\n",
    "         run_text_list : List[str] = getattr(self, 'run_text',[''])\n",
    "         num_runs = len(run_text_list)\n",
    "\n",
    "         ind = start_ind\n",
    "         droppable_runs : List[int] = [] #TODO this dropable section doesnt seem to be working correctly.\n",
    "         while ind < num_runs:\n",
    "            this_run_text = run_text_list[ind]\n",
    "            stripped_run = this_run_text.lstrip() #TODO pass config to this to allow control of what can and can't be dropped.\n",
    "            if len(stripped_run) == 0: #found ALL whitespace run. Need to iterate to see if next run is blank or has any leading whitespace\n",
    "               droppable_runs.append(ind) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "               logger.info(f'paragraph#{para_enumeration} with text \"{para_text}\" had a run#{ind} with ONLY whitespace')\n",
    "            elif len(stripped_run) < len(this_run_text): #found run that is NOT ALL whitespace, but had SOME. Will only happen once. Can stop now since this is the true beginning of this paragraph\n",
    "               run_text_list[ind] = stripped_run\n",
    "               self.__setattr__(\"run_text\", run_text_list) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "               logger.info(f'paragraph#{para_enumeration} with text \"{para_text}\" had leading whitespace removed')\n",
    "               break\n",
    "            else: #Can stop now since this is the true beginning of this paragraph\n",
    "               break\n",
    "            ind +=1\n",
    "            \n",
    "         if len(droppable_runs) > 0: #if a whole run_text was whitespace only\n",
    "            if len(droppable_runs) == num_runs: #if the whole paragraph was whitespace only\n",
    "               raise RuntimeError(f'for paragraph#{para_enumeration}, all runs purported droppable whitespace, but para_text purported not')\n",
    "            self.modify_run_lists(drop_runs = droppable_runs) #this removes whole runs, not just modifying the run_text.\n",
    "            logger.info(f'paragraph#{para_enumeration} with text \"{para_text}\" tried to drop a run whitespace')\n",
    "\n",
    "      if execute_defaults:\n",
    "         remove_para_leading_whitespace()\n",
    "         self.modify_run_lists(merge_runs = True)\n",
    "\n",
    "      return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fula_Entry (BaseModel): \n",
    "   entity_word: List[str] #root, subroot, lemma\n",
    "   features: Optional[Dict[str,str]] = {} #contains features for this entity, ie: txt file features like location, POS, etc. Only applicable directly. Lemmas have POS, roots do not, etc\n",
    "   paragraphs_list: Dict[int,Any] #para enumeration, docx para obj\n",
    "   paragraphs_extr : List[Docx_Paragraph_and_Runs] #class defined above\n",
    "   sub_roots : List['Fula_Entry'] = [] #self reference\n",
    "   lemmas : List['Fula_Entry'] = [] #self reference\n",
    "\n",
    "   \n",
    "\n",
    "## from __future__ import annotations\n",
    "# from typing import ForwardRef\n",
    "# Fula_Entry = ForwardRef('Fula_Entry')\n",
    "# root = root_ind_list[0]\n",
    "# lemma = lemma_ind_list[0]\n",
    "# test_entry = Fula_Entry()\n",
    "# print(len(root_ind_list),'\\t',root_ind_list)\n",
    "# print(lemma_ind_list)\n",
    "# print(lemma_ind_list[8:])\n",
    "# print(len(list(pairwise(root_ind_list))),'\\t',list(pairwise(root_ind_list)))\n",
    "\n",
    "# def paragraph_splitter(self): #almost certainly only going to be only the lemmas from roots\n",
    "      #single_run_feature_identify(condition) -> mask\n",
    "      #find index in mask where to split\n",
    "      #create a clone of the object (default para_enumeration)\n",
    "         #need to change class to allow all para enumerations to be float\n",
    "         #default para_enumeration float split size (float p_e.##?)\n",
    "      #modify_run_lists to drop the last runs from para_A, and first runs from para_B\n",
    "      #run cleaner again to remove any leading whitespace in new para?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total paras:  32507\n",
      "parsed paras:  32040\n",
      "handled errors:  467\n",
      "failed paras:  0\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# output_name = f\"logs_and_outputs/{current_time}_docxFileParseResult.txt\"\n",
    "# experiment = input(\"Enter emperiment description:\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "#Run docx module to parse the docx file\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "document = Document(docx_filename)\n",
    "\n",
    "\n",
    "\n",
    "char_counts = Counter()\n",
    "\n",
    "docx_object_list = []\n",
    "parsed_object_list = []\n",
    "failed_paras_ind = []\n",
    "handled_errors = []\n",
    "\n",
    "for i, para in enumerate(document.paragraphs):\n",
    "   # if i <2170: continue\n",
    "   # elif i>2180: break\n",
    "   # else: pass\n",
    "   docx_object_list.append((i,para))\n",
    "   try:\n",
    "      entryObj = Docx_Paragraph_and_Runs(**{'paragraph': para, 'paragraph_enumeration': i})\n",
    "      char_counts.update(entryObj.interogate__para_text())\n",
    "      parsed_object_list.append((i,entryObj))\n",
    "   except ValidationError as e:\n",
    "      suppress = {\n",
    "            # 'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "            #          ],\n",
    "            'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      for err in e.errors():\n",
    "         if err['msg'] in suppress['msg']:\n",
    "            handled_errors.append((i,para))\n",
    "            pass\n",
    "   except BaseException as e:\n",
    "      print(e)\n",
    "      failed_paras_ind.append((i,para))\n",
    "      \n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment time: 2022-07-24_-_16-12-35\n",
      "Experiment note: starting over, traying to quickly get para texts and contents\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "output_name = f\"logs_and_outputs/{current_time}_objList_processing_Output.txt\"\n",
    "experiment = input(\"Enter emperiment description:\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}objList_processing.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG, etc\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2022-07-22 07:20:18,751 paragraph#2731 with text \"\"   -IR-\"\" had leading whitespace removed\n",
    "# # parsed_object_list = parsed_object_list[2731:3000]\n",
    "# # def any(self, value):\n",
    "# #    return self.add(\"([%s])\" % value)\n",
    "# # def add(self, value):\n",
    "# #    if isinstance(value, list):\n",
    "# #       self.s.extend(value)\n",
    "# #    else:\n",
    "# #       self.s.append(value)\n",
    "# #    return self\n",
    "# low_alph_chars = ''.join([x.lower() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "# up_alph_chars = ''.join([x.upper() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "# root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "# sub_root_beginnings = '-+('\n",
    "# permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "# pattern = '^['+re.escape(sub_root_beginnings)+'][^'+low_alph_chars+']+'\n",
    "# print(pattern)\n",
    "# print(re.search(pattern = pattern, string = '-KLHFGSLK JEs'))\n",
    "# pattern = '^['+re.escape(sub_root_beginnings)+']['+re.escape(permissive_root_contents)+']+'\n",
    "# print(pattern)\n",
    "# print(re.search(pattern = pattern, string = 'KLHFGSLK JEs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-a'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = ['\\t', '\\t', 'ɓaynde ', '(nde)  ', 'D  [ɓunndu]']\n",
    "# b = [False, False, True,True,False]\n",
    "b = [True, False, False] \n",
    "a = ['-a  ', 'suf,pos  ', 'F']\n",
    "''.join(chain(compress(a,b))).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap \n",
    "#--no-stderr\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "para_text_list = []\n",
    "root_ind_list = []\n",
    "subroot_ind_list = []\n",
    "lemma_ind_list = []\n",
    "some_error_ind_list = []\n",
    "reject_ind_list = []\n",
    "root_and_lemma_one_line = []\n",
    "root_lookup = {}\n",
    "lemma_lookup = {}\n",
    "\n",
    "up_alph_chars = [x.upper() for x in char_counts.keys() if x.upper() != x.lower()] #only uppercase alphabetical chars\n",
    "\n",
    "for i, entryObj in parsed_object_list:\n",
    "\n",
    "   try:\n",
    "      para_text_list.append(entryObj.para_text)\n",
    "      successful_cleaner_output:bool = entryObj.cleaner() #by default cleaner removes leading whitespace and merges adjacent runs with identical format features\n",
    "      # print('sucessful cleaner')\n",
    "      if not successful_cleaner_output:\n",
    "         reject_ind_list.append(i)\n",
    "         print(f'para# {i} IS ONLY whitespace. Need to drop it. #TODO')\n",
    "   except:\n",
    "      some_error_ind_list.append(f\"cleaner error on p: {i}. Text is {entryObj.interogate__para_text()}\")\n",
    "      print('error on cleaner')\n",
    "      raise\n",
    "   try:\n",
    "      root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "      sub_root_beginnings = '-+('\n",
    "      permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "\n",
    "      featureConfig = {\n",
    "      'root': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               # 'text_regex_at_feature': root_expression.compile()\n",
    "               },\n",
    "      'subroot': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               # 'text_regex_at_feature': subroot_expression.compile()\n",
    "               },\n",
    "      'lemma': {'docxFeature': 'run_bold',\n",
    "               'strSummary':'fontBold', \n",
    "               'value':True},\n",
    "      }\n",
    "      is_subroot = False\n",
    "      # return True, (value_mask, run_text), (regex_mask, regex_matches)\n",
    "      is_root, (mask,run_text), _ = entryObj.single_run_feature_identify(featureConfig['root'])\n",
    "      if is_root:\n",
    "         rtext = ''.join(chain(compress(run_text,mask))).strip()\n",
    "         root_lookup[i] = rtext\n",
    "         # print(rtext)\n",
    "         for j, r in enumerate(compress(run_text,mask)):\n",
    "            if j==0:\n",
    "               # low_alph_chars = ''.join([x.lower() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "               up_alph_chars = ''.join([x.upper() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "               root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "               sub_root_beginnings = '-+('\n",
    "               permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "               pattern = '^['+re.escape(sub_root_beginnings)+']['+re.escape(permissive_root_contents)+']+'\n",
    "               m = re.search(pattern = pattern, string = r)\n",
    "               if m is not None:\n",
    "                  is_subroot = True\n",
    "         if is_subroot:\n",
    "            print('\\n\\nsubroot at para number: ',i)\n",
    "            paraText = entryObj.interogate__para_text()\n",
    "            print('\\t',paraText)\n",
    "            subroot_ind_list.append(i)\n",
    "         else:\n",
    "            print('\\n\\nroot at para number: ',i)\n",
    "            paraText = entryObj.interogate__para_text()\n",
    "            print('\\t',paraText)\n",
    "            root_ind_list.append(i)\n",
    "\n",
    "      # return True, (value_mask, run_text), (regex_mask, regex_matches)\n",
    "      is_lemma, (mask, run_text), _ = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      if is_lemma:\n",
    "         print(mask,run_text)\n",
    "         ltext = ''.join(chain(compress(run_text,mask))).strip()\n",
    "         print(ltext)\n",
    "         lemma_lookup[i] = ltext\n",
    "         # entryObj.interogate__para_text()\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t\\tp#',i,'\\t\\t',paraText)\n",
    "         lemma_ind_list.append(i)\n",
    "      # if is_lemma and (is_root or is_subroot):\n",
    "      if is_lemma and is_root:\n",
    "         print(f'this para# {i} has BOTH lemma AND root')\n",
    "         root_and_lemma_one_line.append(i)\n",
    "\n",
    "   except BaseException as e:\n",
    "      \n",
    "      some_error_ind_list.append(i)\n",
    "      raise e\n",
    "\n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n",
    "\n",
    "print('roots: ',len(root_ind_list))\n",
    "print('subroots: ',len(subroot_ind_list))\n",
    "print('lemmas: ',len(lemma_ind_list))\n",
    "print('root_and_lemma_one_line: ',len(root_and_lemma_one_line))\n",
    "print('additional cleaner rejects: ',len(reject_ind_list))\n",
    "print('additional error rejects: ',len(some_error_ind_list))\n",
    "\n",
    "print('num entities: ',len(root_ind_list) + len(lemma_ind_list) + len(subroot_ind_list))\n",
    "num_good_paras_of_other_content= len(root_ind_list) + len(lemma_ind_list) - len(root_and_lemma_one_line) - len(subroot_ind_list)\\\n",
    "                                    + len(reject_ind_list) + len(some_error_ind_list)\n",
    "print('num_good_paras_of_other_content: ',num_good_paras_of_other_content)\n",
    "# # Test messages\n",
    "logger.debug(\"logger debug test\")\n",
    "logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 \t A\n",
      "5 \t a  prn,sbj,sf  DFZH  Z<->\n",
      "6 \t you (sg.)\n",
      "7 \t tu\n",
      "8 \t -a  suf,pos  F\n",
      "9 \t your (sg.) (only with certain nouns such as those which refer to close family members)\n",
      "10 \t ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)\n",
      "11 \t aan  prn,ind  DFZH  [an]:Z<->\n",
      "12 \t you (sg.) (emphatic)\n",
      "13 \t toi (emphatique)\n",
      "14 \t aɗa  prn,sbj,lf  DFZH  [aɗo]:Z\n",
      "15 \t you (sg.)\n",
      "16 \t tu\n",
      "17 \t -ɗaa, -ɗa  prn,sbj  DF\n",
      "18 \t you (sg.) (used with imperfect and subjunctive forms)\n",
      "19 \t tu (employé avec les formes imparfaites et subjonctives)\n",
      "20 \t -e  prn,obj  F\n",
      "21 \t you (suffix used in certain grammatical conditions)\n",
      "22 \t te (suffixe employé dans certaines situations grammaticales)\n",
      "23 \t ma, maa  prn,obj  FZH  Z<->\n",
      "24 \t you (sg.)\n",
      "25 \t te\n",
      "26 \t ma, maa, maaɗa  prn,pos  DFZH  Z<->\n",
      "27 \t your (sg.)\n",
      "28 \t ton, ta, tes\n",
      "29 \t AAD-   Ar\n",
      "30 \t aadi (o) / aadiiji (ɗi)  DFCZT  C<->,Z<+>  [amaana]\n",
      "31 \t agreement(FC), promise(F), convention(F), accord, pact(C), confidence(C); custom [alaada]\n",
      "32 \t serment(D), contrat(T), convention(Z), accord(CZ), pacte(C), confiance(C); coutume(Z) [alaada]\n",
      "33 \t alaada (o) / alaadaaji (ɗi)  DFzbu  Z<G,A>  [aada/aadaaji]:D,Z<FT> [laada/laadaaji]:DF  [aadi]:Z  [sunna, tawaangal]\n",
      "34 \t custom(F), habit(F), tradition, cultural custom(F), totality of habits and customs of a country or a people\n",
      "35 \t coutume(DZ), habitude(D), tradition(Z), coutume culturelle, ensemble des us et coutumes d'un pays ou d'un peuple(Z)\n",
      "36 \t   -ONDIR-\n",
      "37 \t aadondiral (ngal)  T  [kaɓɓondiral]\n",
      "38 \t alliance\n",
      "39 \t alliance\n",
      "40 \t aadondireede  v.pv+ext  T  [fiɓondireede]\n",
      "41 \t to make a contract\n",
      "42 \t contracter\n",
      "43 \t AADAMA\n",
      "44 \t Aadama  PN  DZ  Z<->\n",
      "45 \t man's first name, Adam\n",
      "46 \t prénom d'homme, Adam\n",
      "47 \t aadamanke(ejo) (o) / aadamankeeɓe (ɓe)  DCZ  C<FJ,FT,M>,Z<->  [ɓii-Aadama, neɗɗo, remme, tagaaɗo, tomotte]\n",
      "48 \t human being\n",
      "49 \t être humain\n",
      "50 \t AADE   Ar\n",
      "51 \t aade  (o)  Db\n",
      "52 \t human beings\n",
      "53 \t êtres humains\n",
      "54 \t AAF-\n",
      "55 \t aafugol (ngol)  D\n",
      "56 \t installation of a bed\n",
      "57 \t installation d'un lit\n",
      "58 \t AAJ-\n",
      "59 \t aajaade, ajaade  v.mv  D  [mooɓ(u)de, faggaade, mubbude, wubbude]\n",
      "60 \t to amass\n",
      "61 \t amasser\n",
      "62 \t AAL-\n",
      "63 \t aalude  v.av  Z\n",
      "64 \t to split, dissociate\n",
      "65 \t fendre, dissocier\n",
      "66 \t AAM- (1)\n",
      "67 \t aamde  v.av  D\n",
      "68 \t to start, create, trace the place for a dwelling\n",
      "69 \t commencer, créer, tracer l'emplacement d'une habitation\n",
      "70 \t AAM-T- (2)\n",
      "71 \t aamtude  v.av+ext  D\n",
      "72 \t to split, open (calabash)\n",
      "73 \t fendre, ouvrir (calebasse)\n",
      "74 \t AAMAR-\n",
      "75 \t aamare, amare (nge)  DZx(Turner, 1989)  Z<M,A>  cf.:[sirge]\n",
      "76 \t coloration of cattle; white cow with red spots; white cow with small colored spots(Turner, 1989)\n",
      "77 \t robe de vache(D); vache blanche tachetée de rouge(Z); vache blanche aux taches colorées\n",
      "78 \t amare / amari (R<A>): white cow with red flanks ; vache blanche aux flancs rouges\n",
      "79 \t (X<A(Jam'aare)>): (zebu) with a white coat spotted red or black ; (zébu) à robe blanche avec quelques taches rouges ou noires\n",
      "80 \t AAMIN\n",
      "81 \t aamin, aamiina    DZ  Z<A,M>\n",
      "82 \t so be it, amen!\n",
      "83 \t ainsi soit-il, amen!\n",
      "84 \t AAN  aan  prn,ind  V. a\n",
      "85 \t AAR  Fr\n",
      "86 \t aar  n.  T\n",
      "87 \t are (100 square meters)\n",
      "88 \t are (100 metres carrés)\n",
      "89 \t AAR-\n",
      "90 \t aare ?  n.  (Imperato, 1973)\n",
      "91 \t part of a design woven in kaasa blankets including diamond shapes\n",
      "92 \t partie de dessin tissé dans des couvertures kaasa comprenant des formes de diamant\n",
      "93 \t aarude  v.av  ZH  Z<FT,M>\n",
      "94 \t to split; to half-open a cashbox by lifting the lid; to open a ring\n",
      "95 \t fendre(H); entrouvrir une caisse en soulevant le couvercle(Z); ouvrir un anneau(Z)\n",
      "96 \t   -T-\n",
      "97 \t aartude  v.av+ext  DY\n",
      "98 \t to split, separate\n",
      "99 \t fendre(D), séparer(D), écarter(Y)\n",
      "100 \t AARAAB-   Ar\n",
      "101 \t Aaraabo, Aarabo (o) / Aaraɓɓe, Aaraɓe'en (ɓe)  DFcz  CZ<G,A>  [Araabo/Araaɓe]:W  [Ara(a)buujo, Laarabuujo]\n",
      "102 \t Arab\n",
      "103 \t Arabe\n",
      "104 \t aaraaɓere, aaraaɓuure (nde)  - var.- arabiya(nkoore)  Z\n",
      "105 \t Arabuujo, Araabuujo (o) / Arabuuɓe, Araabuɓe (ɓe)  - var.- Aaraabo  Z  <FJ,M>\n",
      "106 \t arabiya, arabiyankoore (nde)  DZW  Z<+>  [aaraaɓere, aaraaɓuure]\n",
      "107 \t Arabic language\n",
      "108 \t langue arabe\n",
      "109 \t AAT-\n",
      "110 \t aatude  v.av  DcZH  C<FJ>,Z<FJ,M>  cf.:[haacude, halɓude]\n",
      "111 \t to lose one's head; to scream loudly, cry out\n",
      "112 \t s'affoler(DH); crier très fort(D), pousser un grand cri(Z)\n",
      "113 \t AAW-\n",
      "114 \t aawde  v.av  FCZY  CZ<->\n",
      "115 \t to plant seeds, sow, seed, plant\n",
      "116 \t semer, ensemencer, planter\n",
      "117 \t aawdi (ndi) / aawɗi (ɗi)  FCz  C<->,Z<A,G>  [awdi/awdiiji]\n",
      "118 \t seed\n",
      "119 \t semence, graine\n",
      "120 \t aawgol (ngol)  F  cf.:[sankere]\n",
      "121 \t planting, seeding, sowing\n",
      "122 \t plantation, semailles, ensemencement, semis\n",
      "123 \t   -IR-\n",
      "124 \t aawirde (nde) / aawirɗe (ɗe)  Z\n",
      "125 \t calabash in which to put seed\n",
      "126 \t calebasse pour y mettre la semence\n",
      "127 \t AAY-\n",
      "128 \t aayaare (nde)  T  [gaayaare]\n",
      "129 \t anarchy\n",
      "130 \t anarchie\n",
      "131 \t aayiiɗo (o)  T  [gaayiiɗo]\n",
      "132 \t anarchist\n",
      "133 \t anarchiste\n",
      "134 \t AAYA   Ar\n",
      "135 \t aaya (o) / aayaaji (ɗi), aayaaje, aajeeje (ɗe)  DZ  Z<->\n",
      "136 \t verse of the Koran; three dots at the end of each verse of the Koran; dot\n",
      "137 \t verset du Coran(DZ); trois points à la fin de chaque verset du Coran(Z); point(Z)\n",
      "138 \t ABADA   Ar\n",
      "139 \t abada, abadaa, abadan  DFZ  Z<->\n",
      "140 \t never(F) (with negation); ever(F); long ago\n",
      "141 \t jamais(D) (avec la négation)(Z); jamais; il y a longtemps\n",
      "142 \t Abada mi yahaali. (F): I have never gone. ; Je ne suis jamais allé.\n",
      "143 \t abada pati (F): don't ever ; ne faîtes jamais (qqch)\n",
      "144 \t gila abada (F): since long ago, forever ; depuis longtemps, toujours\n",
      "145 \t ABAJADDA   Ar\n",
      "146 \t abajada, abajadda (o) / abajadaaji (ɗi)  DcZ  C<FJ>,Z<->  [habajaaɗa, harfeeje, karfeeje, limto]\n",
      "147 \t alphabet\n",
      "148 \t alphabet\n",
      "149 \t ABBA\n",
      "150 \t abba (o)  DFh  [baa, baaba, babba]\n",
      "151 \t father (term of address)\n",
      "152 \t père (titre)\n",
      "153 \t abbam (H): my father ; mon père\n",
      "154 \t abbiiko (H): his or her father ; son père\n",
      "155 \t ABBERE  abbere  var.- wabbere  DcZ  C<FJ>,Z<->\n",
      "157 \t ACC-\n",
      "158 \t accude  v.av  DFCZ  CZ<->\n",
      "159 \t to leave, leave alone, let alone, renounce, permit\n",
      "160 \t laisser, laisser seul, abandoner, renoncer à, permettre\n",
      "161 \t   -AN-\n",
      "162 \t accaneede  v.pv+ext  Z\n",
      "163 \t to have a bad reputation\n",
      "164 \t avoir une mauvaise réputation\n",
      "165 \t   -IR-\n",
      "166 \t accirde (neɗɗo, huunde)  v.av+ext  Z  <FT,M>\n",
      "167 \t to leave sth to sb\n",
      "168 \t laisser qqch à qqn\n",
      "169 \t AD- (1)\n",
      "170 \t adaade  v.pv  DFZ  C<FJ>,Z<->  [artaade]\n",
      "171 \t to be first, precede; to promise\n",
      "172 \t être le premier, précéder, devancer; promettre(D)\n",
      "173 \t adan  adv.  FZ  Z<->  [aran]\n",
      "174 \t at first, in the past, in the beginning\n",
      "175 \t premièrement, autrefois, à l'origine\n",
      "176 \t adiiɗo (o) / adiiɓe (ɓe)  F\n",
      "177 \t person who is or was first\n",
      "178 \t qui est ou fut le premier\n",
      "179 \t AD- (2)\n",
      "180 \t adal (ngal)  Z\n",
      "181 \t transportation, haulage\n",
      "182 \t transport, roulage\n",
      "183 \t adoowo (o) / adooɓe (ɓe)  Z  <A,M>\n",
      "184 \t porter, carter\n",
      "185 \t porteur, charretier\n",
      "186 \t adude  v.av  DcZ  C<NE>,Z<->\n",
      "187 \t to take; to gather; to carry, bring, transport (by lots)\n",
      "188 \t prendre(DC); ramasser(D); porter, apporter, transporter (par lots) (Z)\n",
      "189 \t ADADU   Ar\n",
      "190 \t adadu (o) / adaduuji (ɗi)  DZ  Z<->\n",
      "191 \t quantity, measure; sum, total; calculation; number\n",
      "192 \t quantité, mesure (D); somme, total (Z); calcul(Z); nombre(Z)\n",
      "193 \t adadu keeweendi (T): cardinal number ; nombre cardinal\n",
      "194 \t ADDIKO   Hs\n",
      "195 \t addiko  n.  Dx  [adikko]:X<A(Jam'aare)> [adiikowol]:CZ<G>  [misoro, mosoro]\n",
      "196 \t scarf, head tie\n",
      "197 \t mouchoir de tête\n",
      "198 \t ADDUDE  addude  var.- waddude  DCZ  C<FJ,FT,M>,Z<->\n",
      "199 \t ADI'ADE\n",
      "200 \t adi'ade, adii-adii  n.  D\n",
      "201 \t game of running, race\n",
      "202 \t jeu de course\n",
      "203 \t ADILIIJO   Ar\n",
      "204 \t adiliijo (o) / adili'en (ɓe)  Dz  Z<A>  [nuunɗuɗo]  cf.:[aad-]\n"
     ]
    }
   ],
   "source": [
    "# para_text_list\n",
    "for i, p in parsed_object_list[:200]:\n",
    "   print(i,'\\t',p.para_text)\n",
    "\n",
    "# 7/24/2022 5:34pm\n",
    "# total paras:  32507\n",
    "# parsed paras:  32040\n",
    "# handled errors:  467\n",
    "# failed paras:  0\n",
    "# roots:  6381\n",
    "# subroots:  713\n",
    "# lemmas:  9610\n",
    "# root_and_lemma_one_line:  1438\n",
    "# additional cleaner rejects:  0\n",
    "# additional error rejects:  0\n",
    "# num entities:  16704\n",
    "# num_good_paras_of_other_content:  13840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32040\n"
     ]
    }
   ],
   "source": [
    "print(len(parsed_object_list)) #32040\n",
    "lim = 32040\n",
    "cutroot = [x for x in root_ind_list if x < lim]\n",
    "cutsubroot = [x for x in subroot_ind_list if x < lim]\n",
    "cutlemma = [x for x in lemma_ind_list if x < lim]\n",
    "allroots = cutroot.copy()\n",
    "allroots.extend(cutsubroot)\n",
    "allroots = sorted(allroots)\n",
    "# print(list(enumerate(allroots)))\n",
    "\n",
    "# print(allroots)\n",
    "# print(cutlemma)\n",
    "# list(lemma_lookup.items())[:10]\n",
    "# all_paras = list(range(len(parsed_object_list)))\n",
    "all_paras = [i for i,obj in parsed_object_list if i < lim]\n",
    "# all_paras = list(range(lim))\n",
    "normal_para = [x for x in all_paras if all([x not in cutroot, x not in cutlemma, x not in cutsubroot])]\n",
    "# print(normal_para)\n",
    "# print(allroots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_entry_ind = list(range(3,len(lemma_ind_list)+3))\n",
    "def get_blank_dict_like_article() -> Dict[int,Any]:\n",
    "   # article_entry_ind = list(range(3,len(lemma_ind_list)+3))\n",
    "   return {i:'' for i in range(len(lemma_ind_list))}\n",
    "\n",
    "from operator import itemgetter\n",
    "def get_para_text(lst: Union[int,List[int]]) -> Union[str,List[str]]:\n",
    "   if isinstance(lst,list):\n",
    "      indices = itemgetter(*lst)\n",
    "      return list(indices(para_text_list))\n",
    "   else:\n",
    "      return para_text_list[lst]\n",
    "\n",
    "root_aligned_lemmas = list(closest(pairwise(allroots), cutlemma))\n",
    "lemma_aligned_paras = list(closest(pairwise(cutlemma),normal_para))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ_ME\n",
    "\n",
    "   Dict_Index.pdf: Source file\n",
    "\n",
    "   Fula_Dictionary: Source file transformed into .txt format\n",
    "\n",
    "fula_dictionary_abbreviations.xlsx: Fula dictionary abbreviations, not used in parsing\n",
    "\n",
    "   Fula_Dictionary.doc: Source file\n",
    "\n",
    "FulaAnnotations.txt: Parsed file\n",
    "   Entry ID\tAnnotations if any, separated by tabs\n",
    "\n",
    "FulaDialects.txt: Parsed file\n",
    "   Entry ID\tAnnotation on Dialect, if any, separated by tabs\n",
    "\n",
    "FulaEntries.txt: Parsed file (raw; words, POS not cleaned here)\n",
    "   Entry ID\tFula word\tEnglish translation\tFrench translation\n",
    "\n",
    "FulaInParentheses.txt: List of abbreviations found in between parentheses in the Fula word part of FulaEntries.txt; list gathered because we don’t know the meaning of these abbreviations\n",
    "\n",
    "FulaLemmas.txt: Parsed file\n",
    "Entry ID\tFula lemmas, separated by tabs, at least 1\n",
    "\n",
    "FulaPOSTags.txt: Parsed file\n",
    "Entry ID\tPOS Tags, if any, separated by tabs\n",
    "\n",
    "FulaRoots.txt: Parsed file\n",
    "Entry ID\tFula Root\n",
    "\n",
    "FulaSenseAnnotations.txt: Parsed file\n",
    "Entry ID\tSense ID\tAnnotations associated to the sense, if any, separated by tabs\n",
    "\n",
    "FulaSenseClassifications.txt: Parsed file\n",
    "Entry ID\tSense ID\tClassification information (in Latin), if any, separated by tabs\n",
    "\n",
    "FulaSenseEnglish.txt: Parsed file\n",
    "Entry ID\tSense ID\tEnglish cleaned translation\n",
    "\n",
    "FulaSenseFrench.txt: Parsed file\n",
    "Entry ID\tSense ID\tFrench cleaned translation\n",
    "\n",
    "FulaSynonyms.txt: Parsed file\n",
    "Entry ID\tSynonyms if any, separated by tabs\n",
    "\n",
    "POS.txt: List of the POS tags, for recognition and classification\n",
    "\n",
    "Root Origins.txt: List of the Fula Root tags, for recognition and classification\n",
    "\n",
    "WordnetConnecter1.py: First round of connection to WordNet\n",
    "\n",
    "WordnetConnecter2.py: Second round of connection to WordNet\n",
    "\n",
    "WordnetEnglish1.txt: Result of WordnetConnecter1.py\n",
    "Entry ID\tSense ID\tConfidence Score\tWordNet Synset names, separated by tabs\n",
    "\n",
    "WordnetEnglish2.txt: Result of WordnetConnecter2.py\n",
    "Entry ID\tSense ID\tSubsense ID\tConfidence Score\tWordNet Synset names, separated by tabs\n",
    "\n",
    "WordnetUnconnected1.txt: Fula Entries not connected to WordNet after WordnetConnecter1.py\n",
    "Entry ID\tSense ID\tEnglish cleaned translation\n",
    "\n",
    "WordnetUnconnected2.txt: Fula Entries not connected to WordNet after WordnetConnecter2.py\n",
    "Entry ID\tSense ID\tEnglish cleaned translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "[[6, 7], [9, 10], [12, 13], [15, 16], [18, 19], [21, 22], [24, 25], [27, 28], [31, 32], [34, 35], [38, 39], [41, 42], [45, 46], [48, 49], [52, 53], [56, 57], [60, 61], [64, 65], [68, 69], [72, 73], [76, 77, 78, 79], [82, 83], [], [87, 88], [91, 92], [94, 95], [98, 99], [102, 103], [], [], [107, 108], [111, 112], [115, 116], [118, 119], [121, 122], [125, 126], [129, 130], [132, 133], [136, 137], [140, 141, 142, 143, 144], [147, 148], [151, 152, 153, 154], [], [159, 160], [163, 164], [167, 168], [171, 172], [174, 175], [177, 178], [181, 182], [184, 185], [187, 188], [191, 192, 193], [196, 197], []]\n"
     ]
    }
   ],
   "source": [
    "txt_FulaAnnotations = get_blank_dict_like_article()\n",
    "# txt_FulaDialects = #TODO will need to process the lemma line\n",
    "txt_FulaEntries = get_blank_dict_like_article()\n",
    "   #>>>txt_FulaInParenthesis is different structure, agg of ___\n",
    "txt_FulaLemmas = get_blank_dict_like_article()\n",
    "# txt_FulaPOSTags = #TODO will need to process the lemma line\n",
    "# txt_FulaRoots.txt = #TODO need to inherit a root\n",
    "# txt_FulaSenseAnnotations = #TODO composite sense [count] classifications and annotations in lemma line?\n",
    "   # txt_FulaSenseClassifications = #TODO count of # senses\n",
    "# txt_FulaSenseEnglish [para1 after lemma, if present]\n",
    "# txt_FulaSenseFrench [para1 after lemma, if present]\n",
    "# txt_FulaSynonyms = #TODO will need to process the lemma line\n",
    "   #>>>txt_POS is different structure, agg of POS tags\n",
    "   #>>>txt_RootOrigins is different structure, agg of origins (after root)\n",
    "for i,l in enumerate(lemma_aligned_paras):\n",
    "   if len(l) > 2:\n",
    "      txt_FulaAnnotations[i] = get_para_text(l[2:])\n",
    "   entry_inds = [lemma_ind_list[i],*l]\n",
    "   # print(entry_inds)\n",
    "   txt_FulaEntries[i] = get_para_text(entry_inds)\n",
    "   # print(get_para_text(txt_FulaEntries[i]))\n",
    "   txt_FulaLemmas[i] = lemma_lookup.get(entry_inds[0])\n",
    "   print(txt_FulaLemmas[i])\n",
    "   break\n",
    "print(lemma_aligned_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lemma_list_from_ind = list(map(lemma_lookup.get,lemma_ind_list))\n",
    "# print(lemma_list_from_ind)\n",
    "# root_list_from_ind = list(map(root_lookup.get,root_ind_list))\n",
    "# print(root_list_from_ind)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_schema = {}\n",
    "for i, r in enumerate(root_ind_list):\n",
    "   # print('\\n\\n',r)\n",
    "   # r = root_lookup.get(r)\n",
    "   num_schema[r] = {}\n",
    "   for j, lemma in enumerate(root_aligned_lemmas[i]):\n",
    "      lemma = int(lemma)\n",
    "      # lemma = lemma_lookup.get(lemma)\n",
    "      num_schema[r][lemma] = []\n",
    "      # print('\\n\\t',lemma)\n",
    "      for k, parag in enumerate(lemma_aligned_paras[j]):\n",
    "         # print('\\t\\t',parag)\n",
    "         num_schema[r][lemma].append(int(parag))\n",
    "\n",
    "import pickle\n",
    "# Open a file and use dump()\n",
    "with open('entity_index_schema.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(num_schema, file)\n",
    "\n",
    "# print(json.dumps(num_schema, indent=4,ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in binary mode\n",
    "with open('entity_index_schema.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze\n",
    "    entity_index_schema = pickle.load(file, encoding='utf-8')\n",
    "\n",
    "      # p_text = entryObj.interogate__para_text()\n",
    "      # if not set(p_text).isdisjoint(low_freq_odd_chars):\n",
    "      #    msg = 'rare_characters\\t\\t'+p_text\n",
    "      #    entryObj.paragraph_logger(level=40,msg = msg, print_bool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6381"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_index_schema.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validate Whitespace behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '\\xa0', '\\t']\n"
     ]
    }
   ],
   "source": [
    "white_space_chars = [k for k in char_counts if len(k.strip()) == 0]\n",
    "print(white_space_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validate Upper/lower\n",
    "conclusion: using str.upper()/lower() functions is safe. No character in the dataset causes an error when used in those functions, and the only characters that don't cooperate to a new case are non-alphabetical characters such as numbers and punctuation. \n",
    "conclusion: using str.upper()==str.lower() is a viable way to check if a character is alphabetical or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "upper_chars:      ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ñ', 'Ŋ', 'Ɓ', 'Ɗ', 'Ƴ']\n",
      "\n",
      "lower_chars:      ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'ç', 'è', 'é', 'ê', 'î', 'ï', 'ñ', 'ò', 'ô', 'ù', 'û', 'ŋ', 'ƴ', 'ɓ', 'ɗ']\n",
      "\n",
      "non_castable:     ['\\t', ' ', '!', '\"', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '[', ']', '`', '\\xa0']\n",
      "\n",
      "error_casting:    []\n",
      "\n",
      "silent fails:     []\n",
      "\n",
      "upperWITHlowerChars:     [('A', 'a'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('E', 'e'), ('F', 'f'), ('G', 'g'), ('H', 'h'), ('I', 'i'), ('J', 'j'), ('K', 'k'), ('L', 'l'), ('M', 'm'), ('N', 'n'), ('O', 'o'), ('P', 'p'), ('Q', 'q'), ('R', 'r'), ('S', 's'), ('T', 't'), ('U', 'u'), ('V', 'v'), ('W', 'w'), ('X', 'x'), ('Y', 'y'), ('Z', 'z'), ('À', 'à'), ('Â', 'â'), ('Ç', 'ç'), ('È', 'è'), ('É', 'é'), ('Ê', 'ê'), ('Î', 'î'), ('Ï', 'ï'), ('Ñ', 'ñ'), ('Ò', 'ò'), ('Ô', 'ô'), ('Ù', 'ù'), ('Û', 'û'), ('Ŋ', 'ŋ'), ('Ɓ', 'ɓ'), ('Ɗ', 'ɗ'), ('Ƴ', 'ƴ')]\n",
      "\n",
      "found_as_one_case_only:       [(None, 'à'), (None, 'â'), (None, 'ç'), (None, 'è'), (None, 'é'), (None, 'ê'), (None, 'î'), (None, 'ï'), (None, 'ò'), (None, 'ô'), (None, 'ù'), (None, 'û')]\n"
     ]
    }
   ],
   "source": [
    "upperWITHlowerChars = set()\n",
    "upper_chars = []\n",
    "lower_chars = []\n",
    "non_castable = []\n",
    "error_casting = []\n",
    "nons = []\n",
    "found_as_one_case_only = []\n",
    "for k in char_counts:\n",
    "   try:\n",
    "      up = k.upper()\n",
    "      low = k.lower()\n",
    "      upperWITHlowerChars.add((up,low))\n",
    "      if up == low:\n",
    "         non_castable.append(k)\n",
    "         upperWITHlowerChars.remove((up,low))\n",
    "      elif k == up:\n",
    "         upper_chars.append(up)\n",
    "      elif k == low:\n",
    "         lower_chars.append(low)\n",
    "      else:\n",
    "         nons.append(k)\n",
    "   except:\n",
    "      error_casting.append(k)\n",
    "print('\\nupper_chars:     ',sorted(upper_chars))\n",
    "print('\\nlower_chars:     ',sorted(lower_chars))\n",
    "print('\\nnon_castable:    ',sorted(non_castable))\n",
    "print('\\nerror_casting:   ',sorted(error_casting))\n",
    "print('\\nsilent fails:    ',sorted(nons))\n",
    "\n",
    "print('\\nupperWITHlowerChars:    ',sorted(upperWITHlowerChars))\n",
    "for u,l in upperWITHlowerChars:\n",
    "   pair = [u,l]\n",
    "   unseen_possible_case = False\n",
    "   if l not in lower_chars and l not in non_castable:\n",
    "      pair[1] = None\n",
    "      unseen_possible_case = True\n",
    "   if u not in upper_chars and u not in non_castable:\n",
    "      pair[0] = None\n",
    "      unseen_possible_case = True\n",
    "   if unseen_possible_case:\n",
    "      found_as_one_case_only.append(tuple(pair))\n",
    "      # print(\"upper possible, but not present:     \",u)\n",
    "print('\\nfound_as_one_case_only:      ', sorted(found_as_one_case_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'U+0041', 'a', 'U+0061'),\n",
       " ('B', 'U+0042', 'b', 'U+0062'),\n",
       " ('C', 'U+0043', 'c', 'U+0063'),\n",
       " ('D', 'U+0044', 'd', 'U+0064'),\n",
       " ('E', 'U+0045', 'e', 'U+0065'),\n",
       " ('F', 'U+0046', 'f', 'U+0066'),\n",
       " ('G', 'U+0047', 'g', 'U+0067'),\n",
       " ('H', 'U+0048', 'h', 'U+0068'),\n",
       " ('I', 'U+0049', 'i', 'U+0069'),\n",
       " ('J', 'U+004A', 'j', 'U+006A'),\n",
       " ('K', 'U+004B', 'k', 'U+006B'),\n",
       " ('L', 'U+004C', 'l', 'U+006C'),\n",
       " ('M', 'U+004D', 'm', 'U+006D'),\n",
       " ('N', 'U+004E', 'n', 'U+006E'),\n",
       " ('O', 'U+004F', 'o', 'U+006F'),\n",
       " ('P', 'U+0050', 'p', 'U+0070'),\n",
       " ('Q', 'U+0051', 'q', 'U+0071'),\n",
       " ('R', 'U+0052', 'r', 'U+0072'),\n",
       " ('S', 'U+0053', 's', 'U+0073'),\n",
       " ('T', 'U+0054', 't', 'U+0074'),\n",
       " ('U', 'U+0055', 'u', 'U+0075'),\n",
       " ('V', 'U+0056', 'v', 'U+0076'),\n",
       " ('W', 'U+0057', 'w', 'U+0077'),\n",
       " ('X', 'U+0058', 'x', 'U+0078'),\n",
       " ('Y', 'U+0059', 'y', 'U+0079'),\n",
       " ('Z', 'U+005A', 'z', 'U+007A'),\n",
       " ('À', 'U+00C0', 'à', 'U+00E0'),\n",
       " ('Â', 'U+00C2', 'â', 'U+00E2'),\n",
       " ('Ç', 'U+00C7', 'ç', 'U+00E7'),\n",
       " ('È', 'U+00C8', 'è', 'U+00E8'),\n",
       " ('É', 'U+00C9', 'é', 'U+00E9'),\n",
       " ('Ê', 'U+00CA', 'ê', 'U+00EA'),\n",
       " ('Î', 'U+00CE', 'î', 'U+00EE'),\n",
       " ('Ï', 'U+00CF', 'ï', 'U+00EF'),\n",
       " ('Ñ', 'U+00D1', 'ñ', 'U+00F1'),\n",
       " ('Ò', 'U+00D2', 'ò', 'U+00F2'),\n",
       " ('Ô', 'U+00D4', 'ô', 'U+00F4'),\n",
       " ('Ù', 'U+00D9', 'ù', 'U+00F9'),\n",
       " ('Û', 'U+00DB', 'û', 'U+00FB'),\n",
       " ('Ŋ', 'U+014A', 'ŋ', 'U+014B'),\n",
       " ('Ɓ', 'U+0181', 'ɓ', 'U+0253'),\n",
       " ('Ɗ', 'U+018A', 'ɗ', 'U+0257'),\n",
       " ('Ƴ', 'U+01B3', 'ƴ', 'U+01B4')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def code_point(c):\n",
    "   return \"U+{:04X}\".format(ord(c))\n",
    "[(c,code_point(c),d,code_point(d)) for c,d in sorted(upperWITHlowerChars)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\t', 'U+0009'),\n",
       " (' ', 'U+0020'),\n",
       " ('!', 'U+0021'),\n",
       " ('\"', 'U+0022'),\n",
       " ('&', 'U+0026'),\n",
       " (\"'\", 'U+0027'),\n",
       " ('(', 'U+0028'),\n",
       " (')', 'U+0029'),\n",
       " ('+', 'U+002B'),\n",
       " (',', 'U+002C'),\n",
       " ('-', 'U+002D'),\n",
       " ('.', 'U+002E'),\n",
       " ('/', 'U+002F'),\n",
       " ('0', 'U+0030'),\n",
       " ('1', 'U+0031'),\n",
       " ('2', 'U+0032'),\n",
       " ('3', 'U+0033'),\n",
       " ('4', 'U+0034'),\n",
       " ('5', 'U+0035'),\n",
       " ('6', 'U+0036'),\n",
       " ('7', 'U+0037'),\n",
       " ('8', 'U+0038'),\n",
       " ('9', 'U+0039'),\n",
       " (':', 'U+003A'),\n",
       " (';', 'U+003B'),\n",
       " ('<', 'U+003C'),\n",
       " ('=', 'U+003D'),\n",
       " ('>', 'U+003E'),\n",
       " ('?', 'U+003F'),\n",
       " ('[', 'U+005B'),\n",
       " (']', 'U+005D'),\n",
       " ('`', 'U+0060'),\n",
       " ('\\xa0', 'U+00A0')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(c,code_point(c)) for c in sorted(non_castable)]\n",
    "\n",
    "# ('\\t', 'U+0009' -> ('`', 'U+0060')\n",
    "#  ('\\xa0', 'U+00A0'))\n",
    "# ('A', 'U+0041' -> 'û', 'U+00FB')\n",
    "# ('Ŋ', 'U+014A' -> 'ƴ', 'U+01B4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validate Regex Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "impossible_char = '\\u0008' #utf backspace (\\u0008) is unlikely to appear in a docx, and did not appear in this one.\n",
    "s = impossible_char.join(char_counts.keys())\n",
    "re_results = [False]*len(char_counts.keys())\n",
    "for i, k in enumerate(char_counts):\n",
    "   pattern = re.escape(k)\n",
    "   # print(s)\n",
    "   try:\n",
    "      m = re.search(pattern,s) #type: ignore\n",
    "      corrected_ind = m.start()/2\n",
    "      # print(corrected_ind)\n",
    "   except: print('exception: ',repr(i))\n",
    "   # print(corrected_ind)\n",
    "   if i == corrected_ind:\n",
    "      re_results[i] = True\n",
    "   else: print('failure: ',repr(i))\n",
    "print(all(re_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_chars = [x for x in char_counts.keys() if x.upper() != x.lower()]\n",
    "stralpha = [x for x in alpha_chars if x.isalpha()]\n",
    "assert stralpha == alpha_chars, 'note that str.isalpha does NOT work safely here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#these frequencies were copied from a previous run, and only from successfully parsed objects\n",
    "#the lowest frequencies were reviewed and selections pulled from those\n",
    "   # low_freq_odd_chars = ('\\t', 72), ('5', 67), ('`', 64), ('&', 49), ('ù', 30), ('ï', 26), ('X', 25), ('!', 15), ('\"', 14), ('ò', 8), ('=', 4), ('Q', 4), ('\\xa0', 1)\n",
    "   # low_freq_odd_chars = [x[0] for x in low_freq_odd_chars]\n",
    "#numbers do not appear to be used outside of scholarly references and some multiple-root instances\n",
    "   # nums = list(range(10))\n",
    "#X for example, is almost only in english or french glosses, or scholarly references)\n",
    "   #('X', 25),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning notes\n",
    "# `new Kunari' - region in western Niger ; `nouveau Kounari' - région dans l'ouest du Niger\n",
    "   #here the ` seems to be used at the beginning of a quotation, and a normal apostrophe at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # char_counts\n",
    "# sorted_char_val = sorted(char_counts.items(), key=lambda item: (-item[1], item[0]))\n",
    "# print(sorted_char_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inconsistencies\n",
    "\n",
    "leading white spaces\n",
    "entries with root and lemma on one line\n",
    "\n",
    "\"errors\"\n",
    "   whitespace paras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
