{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Draft or create a class system to represent the pular entries. \n",
    "Ideally this will contain a way to nest entry objects under a root\n",
    "'''\n",
    "#%pip install docx\n",
    "#%pip install python-docx #this mutates docx? \n",
    "#%pip install pydantic\n",
    "#%pip install mypy\n",
    "# %pip install numpy\n",
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "from pydantic import BaseModel, ValidationError, validator, root_validator, Field, constr\n",
    "import json\n",
    "import docx\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from itertools import compress, tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# output_name = f\"{current_time}_result.txt\"\n",
    "# experiment = input(\"Enter emperiment description:\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/initialization_placeholder.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "# # Test messages\n",
    "logger.debug(\"current_time\")\n",
    "# logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'B'), ('B', 'C'), ('C', 'D'), ('D', 'E'), ('E', 'F'), ('F', 'G')]\n",
      "[('A', 'B')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "print(list(pairwise('ABCDEFG')))\n",
    "print(list(pairwise('AB')))\n",
    "print(list(pairwise('A')))\n",
    "def logger_root_validation_error_messages(e, logger_details, suppress = [], run_enumeration: Optional[int] = None) -> Union[RuntimeError, TypeError]:      \n",
    "   #TODO add ability to handle assertion errors\n",
    "   if run_enumeration is not None:\n",
    "      run_num = f\"|run#{run_enumeration}|\" #type: ignore \n",
    "   else:\n",
    "      run_num = \"\"\n",
    "   try:\n",
    "      for err in e.errors():\n",
    "         if err['type'] in suppress['type'] or err['msg'] in suppress['msg']:\n",
    "            logger.info(f\"|SUPRESSED|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"suppressed Validation Error\")\n",
    "         else:\n",
    "            logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"un-suppressed Validation Error\")\n",
    "   except:\n",
    "      logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with error: {e}\")\n",
    "      return RuntimeError(\"non-validation error\")\n",
    "   return RuntimeError(\"non-validation error\")\n",
    "\n",
    "\n",
    "def pular_str_strip_check(s:str) ->bool:\n",
    "   in_len = len(s)\n",
    "   new_s = s.strip()\n",
    "   out_len = len(new_s)\n",
    "   purported_whitespace: bool = in_len != out_len\n",
    "   return purported_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docx_Paragraph (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   # docx_document_paragraph: Optional[Any] #This should be validated below. Left optional because its inclusion causes problems with default repr and serialization\n",
    "   para_text: str = Field(..., min_length = 1) ##required, must be string, must be 1 long or more\n",
    "   para_first_line_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "   para_left_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      #extract para features, \n",
    "      new_values['para_text'] = para.text #type: ignore\n",
    "      new_values['para_first_line_indent'] = para.paragraph_format.first_line_indent #type: ignore\n",
    "      new_values['para_left_indent'] = para.paragraph_format.left_indent #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run (BaseModel):\n",
    "   \"\"\"input:   run = your_run_here\n",
    "   \n",
    "   when given a docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   run_text : str = Field(..., min_length = 1) #required, must be string, must be 1 long or more\n",
    "   run_font_name : Optional[str] = Field(...) #required, must be string or None value\n",
    "   run_font_size_pt : Optional[float] = Field(...)#Required, but must be float OR none value\n",
    "   run_bold : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "   run_italic : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      run = values.get(\"run\",False)\n",
    "      assert isinstance(run, eval('docx.text.run.Run')), 'please enter a docx run assigned to the variable \"run\", in the form of     run = your_run_here'\n",
    "      \n",
    "      new_values : Dict[str, Any] = {}\n",
    "      #loop through the runs in the paragraph and select the desired features\n",
    "      new_values['run_text'] = run.text #type: ignore\n",
    "      new_values['run_font_name'] = run.font.name #type: ignore\n",
    "      if run.font.size is not None: #type: ignore\n",
    "         new_values['run_font_size_pt'] = run.font.size.pt #type: ignore\n",
    "      else: new_values['run_font_size_pt'] = None\n",
    "      new_values['run_bold'] = run.bold #type: ignore\n",
    "      new_values['run_italic'] = run.italic #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run_List (BaseModel):\n",
    "   \"\"\"input:   run_list = your_runs_in_a_list\n",
    "   \n",
    "   when given a list of docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   #because the internals are validated, don't need to validate these other than that they were made into lists\n",
    "   run_text : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_name : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_size_pt : List[Any] = Field(...) #Required, must be list\n",
    "   run_bold : List[Any] = Field(...) #Required, must be list\n",
    "   run_italic : List[Any] = Field(...) #Required, must be list\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "      from collections import defaultdict\n",
    "      paragraph_enumeration = values.get('paragraph_enumeration',\"<<FAILURE_paragraph_enumeration>>\")\n",
    "      runs = values.get(\"run_list\",False)\n",
    "      if not runs:\n",
    "         raise ValueError('please enter a docx run list assigned to the variable \"run_list\", in the form of     run_list = your_run_list_here')\n",
    "      new_values = defaultdict(list)\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      logger_details = {'function':'parsed_run', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      \n",
    "      for run_enumumeration, run in enumerate(runs): #type: ignore\n",
    "         try:\n",
    "            parsed_run = Docx_Run(**{'run':run}) #this manner of root unpacking seems to give warnings since linter can't assess ahead of time\n",
    "            assert isinstance(parsed_run, Docx_Run), 'RUNTIME_ERR - the docx run object did not return the type expected'\n",
    "            for k,v in parsed_run.dict().items():\n",
    "               new_values[k].append(v) \n",
    "\n",
    "         except BaseException as e:\n",
    "            new_e = logger_root_validation_error_messages(e, logger_details, suppress,run_enumeration=run_enumumeration)\n",
    "            raise new_e\n",
    "             \n",
    "      return new_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docx_Paragraph_and_Runs (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "\n",
    "   class Config:\n",
    "      extra = 'allow'\n",
    "      # arbitrary_types_allowed = True\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "      \n",
    "      paragraph_enumeration: int = values.get('paragraph_enumeration',None)\n",
    "      assert isinstance(paragraph_enumeration, int), \"assertion error, bad paragraph count/paragraph_enumeration value passed. Please pass an integer\"\n",
    "      new_values['paragraph_enumeration'] = paragraph_enumeration\n",
    "\n",
    "      \n",
    "      #setting up error and logger handling\n",
    "      #suppress these errors\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      #try to extract para features, \n",
    "      logger_details = {'function':'Docx_Paragraph', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      try: \n",
    "         parsed_paras = Docx_Paragraph(**{'paragraph':para}) #type: ignore\n",
    "         for k,v in parsed_paras.dict().items():\n",
    "            new_values[k] = v\n",
    "      # except ValidationError as e:\n",
    "      #    logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "\n",
    "      #try to extract runs features\n",
    "      logger_details = {'function':'Docx_Run_List', 'paragraph_enumeration':paragraph_enumeration }    \n",
    "      try:\n",
    "         parsed_runs = Docx_Run_List(**{'run_list':para.runs, 'paragraph_enumeration':paragraph_enumeration}) #type: ignore\n",
    "         for k,v in parsed_runs.dict().items():\n",
    "            new_values[k] = v\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "         \n",
    "      return new_values\n",
    "      \n",
    "\n",
    "   def interogate__para_text(self) -> str:\n",
    "      t = getattr(self, 'para_text', \"\")\n",
    "      # \n",
    "      if len(t) == 0:\n",
    "         logger.warning('interogator did not find para_text')\n",
    "      #    print(\"no para_text with:\\n\\t\", self.dict())\n",
    "      return t\n",
    "\n",
    "   def paragraph_logger(self,level:int,msg:str,print_bool:bool):\n",
    "      if print_bool:\n",
    "         print(msg)\n",
    "      else:\n",
    "         logger.log(level,msg)\n",
    "\n",
    "\n",
    "   def single_run_feature_identify(self,params:Dict[str,Any]) -> Tuple[bool,List[bool],List[Any]]: \n",
    "      enumeration : Optional[int] = getattr(self,\"paragraph_enumeration\",None)\n",
    "      assert isinstance(enumeration, int),f\"bad value for 'paragraph_enumeration' {enumeration}\"\n",
    "      feature = params['docxFeature']\n",
    "      assert isinstance(feature,str),f\"bad value for parameter 'docxFeature'. Check params: {params}\"\n",
    "\n",
    "      list_from_runs: List[Optional[Union[float,bool]]] = getattr(self,feature,[None]) \n",
    "      mask: List[bool] = [True if x == params['value'] else False for x in list_from_runs]\n",
    "\n",
    "      if any(mask):\n",
    "         return True, mask, list_from_runs  #has Feature\n",
    "      else:\n",
    "         return False, mask, list_from_runs #does not have feature\n",
    "\n",
    "\n",
    "   def modify_run_lists(self, drop_runs: Optional[List[int]] = None, add_runs: Optional[Tuple[int, List[List[Any]]]] = None, merge_runs : bool = False): #-> Optional[Dict[str, List[List[Any]]]]\n",
    "      \"\"\"given a list of indexes as 'drop' will drop those indexes from runlists, and return those dropped\n",
    "      given a tuple with an integer index and list of lists (run aligned), will add those to entries to the runlists at that index\n",
    "      given bool merge, will greedy merge all runs with the same run features EXCEPT run_text. Run_texts will be concatenated\n",
    "      \"\"\"\n",
    "      run_list_req_features: List[str] = Docx_Run_List.schema()['required']\n",
    "      assert run_list_req_features[0] == 'run_text', \"first feature in the schema should be run_text\"\n",
    "      \n",
    "      feature_run_lists : List[List[Any]] = []\n",
    "      for f in run_list_req_features:\n",
    "         feature_run_lists.append(getattr(self,f,[]))\n",
    "      pivoted_run_lists = list(map(list, zip(*feature_run_lists)))\n",
    "      number_of_runs : int = len(pivoted_run_lists)\n",
    "      if number_of_runs < 1:\n",
    "         raise ValueError('this paragraph does not have values in the run lists')\n",
    "\n",
    "      merge_occured = False\n",
    "\n",
    "      if drop_runs is not None:\n",
    "         dropped_runs = {}\n",
    "         for ind in drop_runs:\n",
    "            dropped_runs[ind] = pivoted_run_lists.pop(ind) #mutates pivoted_run_lists\n",
    "         if number_of_runs == len(pivoted_run_lists):\n",
    "            raise RuntimeError('the runs_lists were not shortened as expected')\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "      if add_runs is not None:\n",
    "         insert_ind = add_runs[0]\n",
    "         add_lists = add_runs[1]\n",
    "         assert len(add_lists[0]) == number_of_runs, \"the added list of lists must have runs of the same length (feature space) as run_lists features in the schema: Docx_Run_List.schema()['required']\"\n",
    "         if insert_ind == -1:\n",
    "            insert_ind = number_of_runs\n",
    "         for lst in add_lists:\n",
    "            pivoted_run_lists.insert(insert_ind,lst)\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "      \n",
    "      if merge_runs is not False:\n",
    "         i = 0\n",
    "         still_merging = True\n",
    "         while still_merging:\n",
    "            pairs = list(pairwise(list(range(len(pivoted_run_lists))))) #index pairs\n",
    "            if len(pairs) < 1: #onely 1 run, which causes pairwise to yield empty lists since nothing to pair with\n",
    "               break\n",
    "            num_merged = 0\n",
    "            for a,b in pairs: #where a,b are indexes in the pivoted run list (each index is one run)\n",
    "               a -= num_merged #mutate pivot indexes after the pivot array has been mutated\n",
    "               b -= num_merged\n",
    "               if pivoted_run_lists[a][1:] == pivoted_run_lists[b][1:]: #if all features EXCEPT run_text are the same #TODO add ability to config which features to merge on\n",
    "                  pivoted_run_lists[b][0] = pivoted_run_lists[a][0] + pivoted_run_lists[b][0]\n",
    "                  pivoted_run_lists.pop(a)\n",
    "                  num_merged +=1\n",
    "                  merge_occured = True #flag for end of function, to determine if any changes need to be set to 'self'\n",
    "               else: pass \n",
    "            if num_merged < 1: #if no merges where made in this iteration, merging is done. Else keep while loop since new merges may occur with new neighbors\n",
    "               still_merging = False\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "      if any([drop_runs is not None, add_runs is not None, merge_occured]):\n",
    "         for i, f in enumerate(run_list_req_features):\n",
    "            self.__setattr__(f,feature_run_lists[i])\n",
    "\n",
    "\n",
    "      \n",
    "   def cleaner(self, execute_defaults: bool = True) -> bool : #params:Optional[Dict[str,Any]],\n",
    "      \"\"\"defaults to running \"remove_para_leading_whitespace\". This removes leading runs that are blank, and strips the first text run of any LEADING whitespace, if any is present.\n",
    "      the params dict is not implemented currently\n",
    "      returns bool value. True means cleaner would yield a valid para. False currently indicates all runs in para are whitespace.\n",
    "      \"\"\"\n",
    "      #TODO aggregate these getattrs so that every function doesn't need to get it themselves. Or simplify this with a function that has an assert bool to require it or not\n",
    "      para_enumeration = getattr(self, 'para_text',None)\n",
    "      if para_enumeration is None:\n",
    "         assert 'paragraph did not have an enumeration value'\n",
    "\n",
    "      def remove_para_leading_whitespace(start_ind : int = 0): #run \n",
    "         # try: #expect to fail when reaches the end of the list\n",
    "         para_text : Optional[str] = getattr(self, 'para_text',None)\n",
    "         if isinstance(para_text,str):\n",
    "            if len(para_text.strip()) == 0: #if para's text is ONLY whitespace\n",
    "               return False\n",
    "         run_text_list : List[str] = getattr(self, 'run_text',[''])\n",
    "         num_runs = len(run_text_list)\n",
    "\n",
    "         ind = start_ind\n",
    "         droppable_runs : List[int] = []\n",
    "         while ind < num_runs:\n",
    "            this_run_text = run_text_list[ind]\n",
    "            stripped_run = this_run_text.lstrip() #TODO pass config to this to allow control of what can and can't be dropped.\n",
    "            if len(stripped_run) == 0: #found ALL whitespace run. Need to iterate to see if next run is blank or has any leading whitespace\n",
    "               droppable_runs.append(ind) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "               logger.info(f'paragraph#{para_enumeration} with text \"\"{para_text}\"\" had a run with ONLY whitespace')\n",
    "            elif len(stripped_run) < len(this_run_text): #found run that is NOT ALL whitespace, but had SOME. Will only happen once. Can stop now since this is the true beginning of this paragraph\n",
    "               run_text_list[ind] = stripped_run\n",
    "               self.__setattr__(\"run_text\", stripped_run) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "               logger.info(f'paragraph#{para_enumeration} with text \"\"{para_text}\"\" had leading whitespace removed')\n",
    "               break\n",
    "            else: #Can stop now since this is the true beginning of this paragraph\n",
    "               break\n",
    "            ind +=1\n",
    "            \n",
    "         if len(droppable_runs) > 0: #if a whole run_text was whitespace only\n",
    "            if len(droppable_runs) == num_runs: #if the whole paragraph was whitespace only\n",
    "               raise RuntimeError(f'for paragraph#{para_enumeration}, all runs purported droppable whitespace, but para_text purported not')\n",
    "            self.modify_run_lists(drop_runs = droppable_runs) #this removes whole runs, not just modifying the run_text.\n",
    "\n",
    "      if execute_defaults:\n",
    "         remove_para_leading_whitespace()\n",
    "         self.modify_run_lists(merge_runs = True)\n",
    "\n",
    "      return True\n",
    "\n",
    "\n",
    "\n",
    "   # def paragraph_splitter(self): #almost certainly only going to be only the lemmas from roots\n",
    "      #single_run_feature_identify(condition) -> mask\n",
    "      #find index in mask where to split\n",
    "      #create a clone of the object (default para_enumeration)\n",
    "         #need to change class to allow all para enumerations to be float\n",
    "         #default para_enumeration float split size (float p_e.##?)\n",
    "      #modify_run_lists to drop the last runs from para_A, and first runs from para_B\n",
    "      #run cleaner again to remove any leading whitespace in new para?\n",
    "# Docx_Paragraph_and_Runs.update_forward_refs()\n",
    "\n",
    "\n",
    "\n",
    "# run_text=['WOOP-  ', 'woopude  ', 'var.- woofude; V. woof- (1)  ', 'Dcz  C<FJ>,Z<FJ,FT>']\n",
    "# run_font_name=[None, 'TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt']\n",
    "# run_font_size_pt=[12.0, None, None, 8.0]\n",
    "# run_italic=[None, None, True, None]\n",
    "# run_bold=[None, True, None, None]\n",
    "\n",
    "# features = [run_text,run_font_name,run_font_size_pt,run_italic,run_bold]\n",
    "# # print(features)\n",
    "# rotated_features = list(zip(*features))\n",
    "# # print(rotated_features)\n",
    "# rotated_features2 = list(zip(*rotated_features))\n",
    "# rotated_features3 = list(map(list, zip(*rotated_features)))\n",
    "# print(rotated_features2)\n",
    "# print(rotated_features3)\n",
    "# # print(list(pairwise(rotated_features3)))\n",
    "# print(features[:0]+features[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fula_Entry (BaseModel): \n",
    "   entity_word: Optional[str]\n",
    "   features: Optional[Dict[str,str]] = {}\n",
    "   paragraphs_list: List[Any]\n",
    "   paragraphs_extr : List[Docx_Paragraph_and_Runs]\n",
    "   sub_roots = []\n",
    "   lemmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total paras:  32507\n",
      "parsed paras:  32040\n",
      "handled errors:  467\n",
      "failed paras:  0\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# output_name = f\"logs_and_outputs/{current_time}_docxFileParseResult.txt\"\n",
    "# experiment = input(\"Enter emperiment description:\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "#Run docx module to parse the docx file\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "document = Document(docx_filename)\n",
    "\n",
    "\n",
    "# from collections import Counter\n",
    "# char_counts = Counter()\n",
    "\n",
    "docx_object_list = []\n",
    "parsed_object_list = []\n",
    "failed_paras_ind = []\n",
    "handled_errors = []\n",
    "\n",
    "for i, para in enumerate(document.paragraphs):\n",
    "   docx_object_list.append((i,para))\n",
    "   try:\n",
    "      entryObj = Docx_Paragraph_and_Runs(**{'paragraph': para, 'paragraph_enumeration': i})\n",
    "      # char_counts.update(entryObj.interogate__para_text())\n",
    "      parsed_object_list.append((i,entryObj))\n",
    "   except ValidationError as e:\n",
    "      suppress = {\n",
    "            # 'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "            #          ],\n",
    "            'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      for err in e.errors():\n",
    "         if err['msg'] in suppress['msg']:\n",
    "            handled_errors.append((i,para))\n",
    "            pass\n",
    "   except BaseException as e:\n",
    "      print(e)\n",
    "      failed_paras_ind.append((i,para))\n",
    "      # if not e.args[0][0].exc.args[0] == 'suppressed Validation Error':\n",
    "         # print('\\npara number: ',i)\n",
    "\n",
    "      # p_text = entryObj.interogate__para_text()\n",
    "      # if not set(p_text).isdisjoint(low_freq_odd_chars):\n",
    "      #    msg = 'rare_characters\\t\\t'+p_text\n",
    "      #    entryObj.paragraph_logger(level=40,msg = msg, print_bool=False)\n",
    "# print(len(failed_paras_ind))\n",
    "# print('i can print')\n",
    "\n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap \n",
    "#--no-stderr\n",
    "\n",
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "output_name = f\"logs_and_outputs/{current_time}_objList_processing_Output.txt\"\n",
    "experiment = input(\"Enter emperiment description:\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}objList_processing.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG, etc\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "# docx_object_list = []\n",
    "# parsed_object_list = []\n",
    "# failed_paras_ind = []\n",
    "root_ind_list = []\n",
    "lemma_ind_list = []\n",
    "some_error_ind_list = []\n",
    "reject_ind_list = []\n",
    "root_and_lemma_one_line = []\n",
    "\n",
    "for i, entryObj in parsed_object_list:\n",
    "\n",
    "   try:\n",
    "      successful_cleaner_output:bool = entryObj.cleaner()\n",
    "      # print('sucessful cleaner')\n",
    "      if not successful_cleaner_output:\n",
    "         reject_ind_list.append(i)\n",
    "         print(f'para# {i} IS ONLY whitespace. Need to drop it. #TODO')\n",
    "   except:\n",
    "      some_error_ind_list.append(f\"cleaner error on p: {i}. Text is {entryObj.interogate__para_text()}\")\n",
    "      print('error on cleaner')\n",
    "   try:\n",
    "      featureConfig = {\n",
    "      'root': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0},\n",
    "      'lemma': {'docxFeature': 'run_bold',\n",
    "               'strSummary':'fontBold', \n",
    "               'value':True},\n",
    "      }\n",
    "      \n",
    "      is_root, _, _ = entryObj.single_run_feature_identify(featureConfig['root'])\n",
    "      if is_root:\n",
    "         print('\\n\\nroot at para number: ',i)\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t',paraText)\n",
    "         root_ind_list.append(i)\n",
    "\n",
    "      is_lemma, _, _ = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      if is_lemma:\n",
    "         # entryObj.interogate__para_text()\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t\\tp#',i,'\\t\\t',paraText)\n",
    "         lemma_ind_list.append(i)\n",
    "      if is_lemma and is_root:\n",
    "         print(f'this para# {i} has BOTH lemma AND root')\n",
    "         root_and_lemma_one_line.append(i)\n",
    "\n",
    "   except BaseException as e:\n",
    "      some_error_ind_list.append(i)\n",
    "      # if not e.args[0][0].exc.args[0] == 'suppressed Validation Error':\n",
    "         # print('\\npara number: ',i)\n",
    "\n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n",
    "\n",
    "print('roots: ',len(root_ind_list))\n",
    "print('lemmas: ',len(lemma_ind_list))\n",
    "print('root_and_lemma_one_line: ',len(root_and_lemma_one_line))\n",
    "print('additional cleaner rejects: ',len(reject_ind_list))\n",
    "print('additional error rejects: ',len(some_error_ind_list))\n",
    "\n",
    "# print('additional cleaner rejects: ',reject_ind_list)\n",
    "# print('additional error rejects: ',some_error_ind_list)\n",
    "print('num entities: ',len(root_ind_list) + len(lemma_ind_list))\n",
    "num_good_paras_of_other_content= len(root_ind_list) + len(lemma_ind_list) - len(root_and_lemma_one_line) \\\n",
    "                                    + len(reject_ind_list) + len(some_error_ind_list)\n",
    "print('num_good_paras_of_other_content: ',num_good_paras_of_other_content)\n",
    "# # Test messages\n",
    "logger.debug(\"logger debug test\")\n",
    "logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#these frequencies were copied from a previous run, and only from successfully parsed objects\n",
    "#the lowest frequencies were reviewed and selections pulled from those\n",
    "   # low_freq_odd_chars = ('\\t', 72), ('5', 67), ('`', 64), ('&', 49), ('ù', 30), ('ï', 26), ('X', 25), ('!', 15), ('\"', 14), ('ò', 8), ('=', 4), ('Q', 4), ('\\xa0', 1)\n",
    "   # low_freq_odd_chars = [x[0] for x in low_freq_odd_chars]\n",
    "#numbers do not appear to be used outside of scholarly references and some multiple-root instances\n",
    "   # nums = list(range(10))\n",
    "#X for example, is almost only in english or french glosses, or scholarly references)\n",
    "   #('X', 25),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning notes\n",
    "# `new Kunari' - region in western Niger ; `nouveau Kounari' - région dans l'ouest du Niger\n",
    "   #here the ` seems to be used at the beginning of a quotation, and a normal apostrophe at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # char_counts\n",
    "# sorted_char_val = sorted(char_counts.items(), key=lambda item: (-item[1], item[0]))\n",
    "# print(sorted_char_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
