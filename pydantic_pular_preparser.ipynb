{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Header and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Draft or create a class system to represent the pular entries. \n",
    "Ideally this will contain a way to nest entry objects under a root\n",
    "'''\n",
    "#%pip install docx\n",
    "#%pip install python-docx #this mutates docx? \n",
    "#%pip install pydantic\n",
    "#%pip install mypy\n",
    "# %pip install numpy\n",
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "from pydantic import BaseModel, ValidationError, validator, root_validator, Field, constr\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import compress, tee, chain\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "from verbalexpressions import VerEx\n",
    "import pickle\n",
    "\n",
    "import docx\n",
    "from docx import Document\n",
    "from pydantic_docx import Docx_Paragraph_and_Runs #type:ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "\n",
    "# logging.setLogRecordFactory(factory)\n",
    "logging.basicConfig(handlers=[handler], level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get current datetime\n",
    "# now = datetime.now()\n",
    "# current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# # #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# # output_name = f\"{current_time}_result.txt\"\n",
    "# # experiment = input(\"Enter emperiment description:\")\n",
    "# # print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "# logger_filename = f\"logs_and_outputs/initialization_placeholder.log\"\n",
    "\n",
    "# # Creating an object\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Setting the threshold of logger to DEBUG\n",
    "# logger.setLevel(logging.ERROR)\n",
    "\n",
    "# #add encoding\n",
    "# handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "# handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "# logger.addHandler(handler) \n",
    "\n",
    "# # # Test messages\n",
    "# logger.debug(\"current_time\")\n",
    "# # logger.info(\"Just an information\")\n",
    "# # logger.warning(\"Its a Warning\")\n",
    "# # logger.error(\"Did you try to divide by zero\")\n",
    "# # logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docx Pydantic Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pairwise(iterable):\n",
    "#     # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "#     a, b = tee(iterable)\n",
    "#     next(b, None)\n",
    "#     return zip(a, b)\n",
    "    \n",
    "# def logger_root_validation_error_messages(e, logger_details, suppress = [], run_enumeration: Optional[int] = None) -> Union[RuntimeError, TypeError]:      \n",
    "#    #TODO add ability to handle assertion errors\n",
    "#    if run_enumeration is not None:\n",
    "#       run_num = f\"|run#{run_enumeration}|\" #type: ignore \n",
    "#    else:\n",
    "#       run_num = \"\"\n",
    "#    try:\n",
    "#       for err in e.errors():\n",
    "#          if err['type'] in suppress['type'] or err['msg'] in suppress['msg']:\n",
    "#             logger.info(f\"|SUPRESSED|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "#             return TypeError(\"suppressed Validation Error\")\n",
    "#          else:\n",
    "#             logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "#             return TypeError(\"un-suppressed Validation Error\")\n",
    "#    except:\n",
    "#       logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with error: {e}\")\n",
    "#       return RuntimeError(\"non-validation error\")\n",
    "#    return RuntimeError(\"non-validation error\")\n",
    "\n",
    "# def pular_str_strip_check(s:str) ->bool:\n",
    "#    in_len = len(s)\n",
    "#    new_s = s.strip()\n",
    "#    out_len = len(new_s)\n",
    "#    purported_whitespace: bool = in_len != out_len\n",
    "#    return purported_whitespace\n",
    "\n",
    "# def closest(ranger, target): #any target indeces occuring before the first ranger index will be ignored\n",
    "#    if not isinstance(target,np.ndarray):\n",
    "#       target = np.array(target)\n",
    "#    for a,b in ranger:\n",
    "#       begin = np.searchsorted(target,a)\n",
    "#       end = np.searchsorted(target,b)\n",
    "#       _, out, target = np.split(target, [begin,end])\n",
    "#       yield list(out)\n",
    "#    yield list(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Level \"Base\" Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Docx_Paragraph (BaseModel):\n",
    "#    \"\"\"input:   paragraph = your_paragraph_here\n",
    "#    when given a docx document's paragraph object, will parse it to a specified schema\n",
    "#    \"\"\"\n",
    "#    # docx_document_paragraph: Optional[Any] #This should be validated below. Left optional because its inclusion causes problems with default repr and serialization\n",
    "#    para_text: str = Field(..., min_length = 1) ##required, must be string, must be 1 long or more\n",
    "#    para_first_line_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "#    para_left_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "\n",
    "#    class Config:\n",
    "#       validate_all = True\n",
    "#       # extra = 'forbid'\n",
    "#       validate_assignment = True\n",
    "#       smart_union = True\n",
    "   \n",
    "#    @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "#    def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#       para = values.get(\"paragraph\",False)\n",
    "#       assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "\n",
    "#       new_values: Dict[str, Any] = {}\n",
    "#       #extract para features, \n",
    "#       new_values['para_text'] = para.text #type: ignore\n",
    "#       new_values['para_first_line_indent'] = para.paragraph_format.first_line_indent #type: ignore\n",
    "#       new_values['para_left_indent'] = para.paragraph_format.left_indent #type: ignore\n",
    "\n",
    "#       return new_values\n",
    "\n",
    "# class Docx_Run (BaseModel):\n",
    "#    \"\"\"input:   run = your_run_here\n",
    "#    when given a docx document paragraphs run object, will parse it to a specified schema\n",
    "#    \"\"\"\n",
    "#    run_text : str = Field(..., min_length = 1) #required, must be string, must be 1 long or more\n",
    "#    run_font_name : Optional[str] = Field(...) #required, must be string or None value\n",
    "#    run_font_size_pt : Optional[float] = Field(...)#Required, but must be float OR none value\n",
    "#    run_bold : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "#    run_italic : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "   \n",
    "#    class Config:\n",
    "#       validate_all = True\n",
    "#       # extra = 'forbid'\n",
    "#       validate_assignment = True\n",
    "#       smart_union = True\n",
    "\n",
    "#    @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "#    def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#       run = values.get(\"run\",False)\n",
    "#       assert isinstance(run, eval('docx.text.run.Run')), 'please enter a docx run assigned to the variable \"run\", in the form of     run = your_run_here'\n",
    "      \n",
    "#       new_values : Dict[str, Any] = {}\n",
    "#       #loop through the runs in the paragraph and select the desired features\n",
    "#       new_values['run_text'] = run.text #type: ignore\n",
    "#       new_values['run_font_name'] = run.font.name #type: ignore\n",
    "#       if run.font.size is not None: #type: ignore\n",
    "#          new_values['run_font_size_pt'] = run.font.size.pt #type: ignore\n",
    "#       else: new_values['run_font_size_pt'] = None\n",
    "#       new_values['run_bold'] = run.bold #type: ignore\n",
    "#       new_values['run_italic'] = run.italic #type: ignore\n",
    "\n",
    "#       return new_values\n",
    "\n",
    "# class Docx_Run_List (BaseModel): #TODO refactor this to use run-aligned lists, so run obj can be used directly, and have its schema raised\n",
    "#    \"\"\"input:   run_list = your_runs_in_a_list\n",
    "#    when given a list of docx document paragraphs run object, will parse it to a specified schema\n",
    "#    \"\"\"\n",
    "#    #because the internals are validated, don't need to validate these other than that they were made into lists\n",
    "#    run_text : List[Any] = Field(...) #Required, must be list\n",
    "#    run_font_name : List[Any] = Field(...) #Required, must be list\n",
    "#    run_font_size_pt : List[Any] = Field(...) #Required, must be list\n",
    "#    run_bold : List[Any] = Field(...) #Required, must be list\n",
    "#    run_italic : List[Any] = Field(...) #Required, must be list\n",
    "\n",
    "#    @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "#    def _docx_structure_check(cls, values: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "#       from collections import defaultdict\n",
    "#       paragraph_enumeration = values.get('paragraph_enumeration',\"<<FAILURE_paragraph_enumeration>>\")\n",
    "#       runs = values.get(\"run_list\",False)\n",
    "#       if not runs:\n",
    "#          raise ValueError('please enter a docx run list assigned to the variable \"run_list\", in the form of     run_list = your_run_list_here')\n",
    "#       new_values = defaultdict(list)\n",
    "#       suppress = {'type': ['value_error.any_str.min_length'], #ignore zero length run_text, per run validator\n",
    "#                   'msg': ['suppressed Validation Error']} #ignore suppressed errors earlier/lower in the stack      \n",
    "#       logger_details = {'function':'parsed_run', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      \n",
    "#       for run_enumumeration, run in enumerate(runs): #type: ignore\n",
    "#          try:\n",
    "#             parsed_run = Docx_Run(**{'run':run}) #this manner of root unpacking seems to give warnings since linter can't assess ahead of time\n",
    "#             assert isinstance(parsed_run, Docx_Run), 'RUNTIME_ERR - the docx run object did not return the type expected'\n",
    "#             for k,v in parsed_run.dict().items():\n",
    "#                new_values[k].append(v) \n",
    "\n",
    "#          except BaseException as e:\n",
    "#             new_e = logger_root_validation_error_messages(e, logger_details, suppress,run_enumeration=run_enumumeration)\n",
    "#             raise new_e\n",
    "             \n",
    "#       return new_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydanticObj = Docx_Run\n",
    "# for k,v in pydanticObj.schema()['properties'].items():\n",
    "#    print('{:<20s} {}'.format(str(k), str(v)))\n",
    "# print('')\n",
    "# pydanticObj = Docx_Run_List\n",
    "# for k,v in pydanticObj.schema()['properties'].items():\n",
    "#    print('{:<20s} {}'.format(str(k), str(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Class with Internal Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Docx_Paragraph_and_Runs (BaseModel):\n",
    "#    \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "#    when given a docx document's paragraph object, will parse it to a specified schema\n",
    "#    \"\"\"\n",
    "\n",
    "#    class Config:\n",
    "#       extra = 'allow'\n",
    "#       # arbitrary_types_allowed = True\n",
    "\n",
    "#    @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "#    def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#       if True: #reading/verifying inputs. Initializing needed variables/structures\n",
    "#          new_values: Dict[str, Any] = {}\n",
    "#          para = values.get(\"paragraph\",False)\n",
    "#          assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "         \n",
    "#          paragraph_enumeration: int = values.get('paragraph_enumeration',None)\n",
    "#          assert isinstance(paragraph_enumeration, int), \"assertion error, bad paragraph count/paragraph_enumeration value passed. Please pass an integer\"\n",
    "#          new_values['paragraph_enumeration'] = paragraph_enumeration\n",
    "\n",
    "#       #setting up error and logger handling\n",
    "#       #suppress these errors\n",
    "#       suppress = {'type': ['value_error.any_str.min_length'], #ignore zero length run_text, per run validator\n",
    "#                   'msg': ['suppressed Validation Error']} #ignore suppressed errors earlier/lower in the stack      \n",
    "\n",
    "#       #try to extract para features, \n",
    "#       logger_details = {'function':'Docx_Paragraph', 'paragraph_enumeration':paragraph_enumeration }\n",
    "#       try: \n",
    "#          parsed_paras = Docx_Paragraph(**{'paragraph':para}) #type: ignore\n",
    "#          for k,v in parsed_paras.dict().items():\n",
    "#             new_values[k] = v\n",
    "#       # except ValidationError as e:\n",
    "#       except BaseException as e:\n",
    "#          new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "#          raise new_e\n",
    "\n",
    "#       #try to extract runs features\n",
    "#       logger_details = {'function':'Docx_Run_List', 'paragraph_enumeration':paragraph_enumeration }    \n",
    "#       try:\n",
    "#          parsed_runs = Docx_Run_List(**{'run_list':para.runs, 'paragraph_enumeration':paragraph_enumeration}) #type: ignore\n",
    "#          for k,v in parsed_runs.dict().items():\n",
    "#             new_values[k] = v\n",
    "#       except BaseException as e:\n",
    "#          new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "#          raise new_e\n",
    "         \n",
    "#       return new_values\n",
    "      \n",
    "#    def interogate__para_text(self) -> str:\n",
    "#       t = getattr(self, 'para_text', \"\")\n",
    "#       # \n",
    "#       if len(t) == 0:\n",
    "#          logger.warning('interogator did not find para_text')\n",
    "#       #    print(\"no para_text with:\\n\\t\", self.dict())\n",
    "#       return t\n",
    "\n",
    "#    def paragraph_logger(self,level:int,msg:str,print_bool:bool):\n",
    "#       if print_bool:\n",
    "#          print(msg)\n",
    "#       else:\n",
    "#          logger.log(level,msg)\n",
    "\n",
    "#    def single_run_feature_identify(self,params:Dict[str,Any]) -> Tuple[bool,Tuple[List[bool],List[Any]],Tuple[List[bool],List[Optional[str]]]]: \n",
    "#       \"\"\"if regex provided, must be in param dict with name 'text_regex_at_feature', and must be passed as a r'pattern' raw string\n",
    "#       return tuple of ('feature boolean', feature_Tuple[boolean mask, feature list], regex_tuple[boolean mask, regex match list])\n",
    "#       \"\"\"\n",
    "#       if True: #reading/verifying inputs. Initializing needed variables/structures\n",
    "#          enumeration : Optional[int] = getattr(self,\"paragraph_enumeration\",None)\n",
    "#          assert isinstance(enumeration, int),f\"bad value for 'paragraph_enumeration' {enumeration}\"\n",
    "#          run_texts : Optional[List[str]] = getattr(self,'run_text',None)\n",
    "#          assert run_texts is not None, f\"bad value for 'run_text' {self.__repr__()}\"\n",
    "#          feature = params['docxFeature']\n",
    "#          assert isinstance(feature,str),f\"bad value for parameter 'docxFeature'. Check params: {params}\"\n",
    "#          text_regex_at_feature = params.get('text_regex_at_feature',False)\n",
    "#          regex_mask: List[bool] = []\n",
    "#          regex_matches: List[Optional[str]] = []\n",
    "\n",
    "#       values_from_runs: List[Optional[Union[float,bool]]] = getattr(self,feature,[None]) \n",
    "#       value_mask: List[bool] = [True if x == params['value'] else False for x in values_from_runs]\n",
    "      \n",
    "#       if any(value_mask):\n",
    "#          # print('text and value mask: ',run_texts,value_mask)\n",
    "#          # if text_regex_at_feature:\n",
    "#             # pattern = text_regex_at_feature\n",
    "#             # for text in run_texts:\n",
    "#             #    match = re.search(pattern, text) #type: ignore\n",
    "#             #    if match is not None:\n",
    "#             #       regex_mask.append(True)\n",
    "#             #       regex_matches.append(match.group(0))\n",
    "#             #       # print(repr(self))\n",
    "#             #    else:\n",
    "#             #       regex_mask.append(False)\n",
    "#             #       regex_matches.append(None)\n",
    "#             # print('regex and match: ',regex_mask,regex_matches)\n",
    "#             # # print(f'inside regex bool for para#{enumeration}\\tregex_mask_is: {regex_mask}\\t\\tvalue_mask is: {value_mask}')\n",
    "#             # if not any(compress(value_mask,regex_mask)):\n",
    "#             #    return False, (value_mask, values_from_runs), (regex_mask, regex_matches) #does not have feature\n",
    "#          return True, (value_mask, run_texts), (regex_mask, regex_matches)  #has Feature\n",
    "#       else:\n",
    "#          return False, (value_mask, run_texts), (regex_mask, regex_matches) #does not have feature\n",
    "\n",
    "#    def modify_run_lists(self, drop_runs: Optional[List[int]] = None, add_runs: Optional[Tuple[int, List[List[Any]]]] = None, merge_runs : bool = False): #-> Optional[Dict[str, List[List[Any]]]]\n",
    "#       \"\"\"given a list of indexes as 'drop' will drop those indexes from runlists, and return those dropped\n",
    "#       given a tuple with an integer index and list of lists (run aligned), will add those to entries to the runlists at that index\n",
    "#       given bool merge, will greedy merge all runs with the same run features EXCEPT run_text. Run_texts will be concatenated\n",
    "#       \"\"\"\n",
    "#       if True: #reading/verifying inputs. Initializing needed variables/structures\n",
    "#          run_list_req_features: List[str] = Docx_Run_List.schema()['required']\n",
    "#          assert run_list_req_features[0] == 'run_text', \"first feature in the schema should be run_text\"\n",
    "#          para_enumeration = getattr(self, 'paragraph_enumeration',None)\n",
    "#          assert para_enumeration is not None, 'paragraph did not have an enumeration value'\n",
    "\n",
    "#          feature_run_lists : List[List[Any]] = []\n",
    "#          for f in run_list_req_features:\n",
    "#             feature_run_lists.append(getattr(self,f,[]))\n",
    "#          pivoted_run_lists = list(map(list, zip(*feature_run_lists)))\n",
    "#          number_of_runs : int = len(pivoted_run_lists)\n",
    "#          if number_of_runs < 1:\n",
    "#             raise ValueError('this paragraph does not have values in the run lists')\n",
    "#          merge_occured = False\n",
    "#          beginning_repr = self.__repr__()\n",
    "\n",
    "#       if drop_runs is not None:\n",
    "#          dropped_runs = {}\n",
    "#          num_dropped = 0\n",
    "#          for ind in drop_runs:\n",
    "#             mut_ind = ind - num_dropped #mutate pivot indexes as the pivot array is mutated\n",
    "#             dropped_runs[ind] = pivoted_run_lists.pop(mut_ind) #mutates pivoted_run_lists\n",
    "#             num_dropped +=1\n",
    "#          if number_of_runs == len(pivoted_run_lists):\n",
    "#             raise RuntimeError('the runs_lists were not shortened as expected')\n",
    "#          number_of_runs = len(pivoted_run_lists)\n",
    "#          # print(dropped_runs,pivoted_run_lists)\n",
    "#          feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "#          logger.info(f'para#{para_enumeration} had runs# {drop_runs} dropped. New run_text is: {feature_run_lists[0]}')\n",
    "\n",
    "#       if add_runs is not None:\n",
    "#          insert_ind : int = add_runs[0]\n",
    "#          add_lists = add_runs[1]\n",
    "#          assert len(add_lists[0]) == number_of_runs, \"the added list of lists must have runs of the same length (feature space) as run_lists features in the schema: Docx_Run_List.schema()['required']\"\n",
    "#          if insert_ind == -1:\n",
    "#             insert_ind = number_of_runs\n",
    "#          for lst in add_lists:\n",
    "#             pivoted_run_lists.insert(insert_ind,lst)\n",
    "#          number_of_runs = len(pivoted_run_lists)\n",
    "#          feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "#       if merge_runs is not False:\n",
    "#          i = 0\n",
    "#          still_merging = True\n",
    "#          # beginning_repr = self.__repr__()\n",
    "#          while still_merging:\n",
    "#             pairs = list(pairwise(list(range(len(pivoted_run_lists))))) #index pairs\n",
    "#             if len(pairs) < 1: #onely 1 run, which causes pairwise to yield empty lists since nothing to pair with\n",
    "#                break\n",
    "#             num_merged = 0\n",
    "#             for a,b in pairs: #where a,b are indexes in the pivoted run list (each index is one run)\n",
    "#                a -= num_merged #mutate pivot indexes after the pivot array has been mutated\n",
    "#                b -= num_merged\n",
    "#                if pivoted_run_lists[a][1:] == pivoted_run_lists[b][1:]: #if all features EXCEPT run_text are the same #TODO add ability to config which features to merge on\n",
    "#                   pivoted_run_lists[b][0] = pivoted_run_lists[a][0] + pivoted_run_lists[b][0]\n",
    "#                   pivoted_run_lists.pop(a)\n",
    "#                   num_merged +=1\n",
    "#                   merge_occured = True #flag for end of function, to determine if any changes need to be set to 'self'\n",
    "#                else: pass \n",
    "#             if num_merged < 1: #if no merges where made in this iteration, merging is done. Else keep while loop since new merges may occur with new neighbors\n",
    "#                still_merging = False\n",
    "#          number_of_runs = len(pivoted_run_lists)\n",
    "#          feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "#       if any([drop_runs is not None, add_runs is not None, merge_occured]):\n",
    "#          for i, f in enumerate(run_list_req_features):\n",
    "#             self.__setattr__(f,feature_run_lists[i])\n",
    "\n",
    "#    def cleaner(self, execute_defaults: bool = True) -> bool : #params:Optional[Dict[str,Any]],\n",
    "#       \"\"\"defaults to running \"remove_para_leading_whitespace\". This removes leading runs that are blank, and strips the first text run of any LEADING whitespace, if any is present.\n",
    "#       the params dict is not implemented currently\n",
    "#       returns bool value. True means cleaner would yield a valid para. False currently indicates all runs in para are whitespace.\n",
    "#       \"\"\"\n",
    "#       if True: #reading/verifying inputs. Initializing needed variables/structures\n",
    "#          #TODO aggregate these getattrs so that every function doesn't need to get it themselves. Or simplify this with a function that has an assert bool to require it or not\n",
    "#          para_enumeration = getattr(self, 'paragraph_enumeration',None)\n",
    "#          assert para_enumeration is not None, 'paragraph did not have an enumeration value'\n",
    "\n",
    "#       def remove_para_leading_whitespace(start_ind : int = 0): #run \n",
    "#          # try: #expect to fail when reaches the end of the list\n",
    "#          para_text : Optional[str] = getattr(self, 'para_text',None)\n",
    "#          if isinstance(para_text,str):\n",
    "#             if len(para_text.strip()) == 0: #if para's text is ONLY whitespace\n",
    "#                return False\n",
    "#          run_text_list : List[str] = getattr(self, 'run_text',[''])\n",
    "#          num_runs = len(run_text_list)\n",
    "\n",
    "#          ind = start_ind\n",
    "#          droppable_runs : List[int] = [] #TODO this dropable section doesnt seem to be working correctly.\n",
    "#          while ind < num_runs:\n",
    "#             this_run_text = run_text_list[ind]\n",
    "#             stripped_run = this_run_text.lstrip() #TODO pass config to this to allow control of what can and can't be dropped.\n",
    "#             if len(stripped_run) == 0: #found ALL whitespace run. Need to iterate to see if next run is blank or has any leading whitespace\n",
    "#                droppable_runs.append(ind) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "#                logger.info(f'paragraph#{para_enumeration} with text \"{para_text}\" had a run#{ind} with ONLY whitespace')\n",
    "#             elif len(stripped_run) < len(this_run_text): #found run that is NOT ALL whitespace, but had SOME. Will only happen once. Can stop now since this is the true beginning of this paragraph\n",
    "#                run_text_list[ind] = stripped_run\n",
    "#                self.__setattr__(\"run_text\", run_text_list) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "#                logger.info(f'paragraph#{para_enumeration} with text \"{para_text}\" had leading whitespace removed')\n",
    "#                break\n",
    "#             else: #Can stop now since this is the true beginning of this paragraph\n",
    "#                break\n",
    "#             ind +=1\n",
    "            \n",
    "#          if len(droppable_runs) > 0: #if a whole run_text was whitespace only\n",
    "#             if len(droppable_runs) == num_runs: #if the whole paragraph was whitespace only\n",
    "#                raise RuntimeError(f'for paragraph#{para_enumeration}, all runs purported droppable whitespace, but para_text purported not')\n",
    "#             self.modify_run_lists(drop_runs = droppable_runs) #this removes whole runs, not just modifying the run_text.\n",
    "#             logger.info(f'paragraph#{para_enumeration} with text \"{para_text}\" tried to drop a run whitespace')\n",
    "\n",
    "#       if execute_defaults:\n",
    "#          remove_para_leading_whitespace()\n",
    "#          self.modify_run_lists(merge_runs = True)\n",
    "\n",
    "#       return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft of High Level Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Fula_Entry (BaseModel): \n",
    "#    entity_word: List[str] #root, subroot, lemma\n",
    "#    features: Optional[Dict[str,str]] = {} #contains features for this entity, ie: txt file features like location, POS, etc. Only applicable directly. Lemmas have POS, roots do not, etc\n",
    "#    paragraphs_list: Dict[int,Any] #para enumeration, docx para obj\n",
    "#    paragraphs_extr : List[Docx_Paragraph_and_Runs] #class defined above\n",
    "#    sub_roots : List['Fula_Entry'] = [] #self reference\n",
    "#    lemmas : List['Fula_Entry'] = [] #self reference\n",
    "\n",
    "   \n",
    "# if False:\n",
    "#    from __future__ import annotations\n",
    "#    from typing import ForwardRef\n",
    "#    Fula_Entry = ForwardRef('Fula_Entry')\n",
    "#    # root = root_ind_list[0]\n",
    "#    # lemma = lemma_ind_list[0]\n",
    "#    # test_entry = Fula_Entry()\n",
    "#    # print(len(root_ind_list),'\\t',root_ind_list)\n",
    "#    # print(lemma_ind_list)\n",
    "#    # print(lemma_ind_list[8:])\n",
    "#    # print(len(list(pairwise(root_ind_list))),'\\t',list(pairwise(root_ind_list)))\n",
    "\n",
    "#    # def paragraph_splitter(self): #almost certainly only going to be only the lemmas from roots\n",
    "#          #single_run_feature_identify(condition) -> mask\n",
    "#          #find index in mask where to split\n",
    "#          #create a clone of the object (default para_enumeration)\n",
    "#             #need to change class to allow all para enumerations to be float\n",
    "#             #default para_enumeration float split size (float p_e.##?)\n",
    "#          #modify_run_lists to drop the last runs from para_A, and first runs from para_B\n",
    "#          #run cleaner again to remove any leading whitespace in new para?\n",
    "#    _ = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Fula_Entry (BaseModel): \n",
    "#    entity_word: List[str] #root, subroot, lemma\n",
    "#    features: Optional[Dict[str,str]] = {} #contains features for this entity, ie: txt file features like location, POS, etc. Only applicable directly. Lemmas have POS, roots do not, etc\n",
    "#    paragraphs_list: Dict[int,Any] #para enumeration, docx para obj\n",
    "#    paragraphs_extr : List[Docx_Paragraph_and_Runs] #class defined above\n",
    "#    sub_roots : List['Fula_Entry'] = [] #self reference\n",
    "#    lemmas : List['Fula_Entry'] = [] #self reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DOCX File and Applying Pydantic Class Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total paras:  32507\n",
      "parsed paras:  32040\n",
      "handled errors:  467\n",
      "failed paras:  0\n"
     ]
    }
   ],
   "source": [
    "# # %%capture cap --no-stderr\n",
    "\n",
    "# #get current datetime\n",
    "# now = datetime.now()\n",
    "# current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# # output_name = f\"logs_and_outputs/{current_time}_docxFileParseResult.txt\"\n",
    "# # experiment = input(\"Enter emperiment description:\")\n",
    "# # print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "# logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "# # Creating an object\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Setting the threshold of logger to DEBUG\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# #add encoding\n",
    "# handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "# handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "# logger.addHandler(handler) \n",
    "\n",
    "# #Run docx module to parse the docx file\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "document = Document(docx_filename)\n",
    "\n",
    "\n",
    "\n",
    "char_counts = Counter()\n",
    "\n",
    "docx_object_list = []\n",
    "parsed_object_list = []\n",
    "failed_paras_ind = []\n",
    "handled_errors = []\n",
    "\n",
    "for i, para in enumerate(document.paragraphs):\n",
    "   docx_object_list.append((i,para))\n",
    "   try:\n",
    "      entryObj = Docx_Paragraph_and_Runs(**{'paragraph': para, 'paragraph_enumeration': i})\n",
    "      char_counts.update(entryObj.interogate__para_text())\n",
    "      parsed_object_list.append((i,entryObj))\n",
    "   except ValidationError as e:\n",
    "      suppress = {\n",
    "            # 'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "            #          ],\n",
    "            'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      for err in e.errors():\n",
    "         if err['msg'] in suppress['msg']:\n",
    "            handled_errors.append((i,para))\n",
    "            pass\n",
    "   except BaseException as e:\n",
    "      print(e)\n",
    "      raise\n",
    "      failed_paras_ind.append((i,para))\n",
    "      \n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Over Class Objects to extract Dictionary Specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Prep / notes / logger \"experiment\" input request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment time: 2022-07-29_-_08-15-57\n",
      "Experiment note: trying this again with more pieces disabled\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "output_name = f\"logs_and_outputs/{current_time}_objList_processing_Output.txt\"\n",
    "experiment = input(\"Enter emperiment description:\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "# logger_filename = f\"logs_and_outputs/{current_time}objList_processing.log\"\n",
    "\n",
    "# # Creating an object\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Setting the threshold of logger to DEBUG, etc\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# #add encoding\n",
    "# handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "# handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "# logger.addHandler(handler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 2022-07-22 07:20:18,751 paragraph#2731 with text \"\"   -IR-\"\" had leading whitespace removed\n",
    "# # # parsed_object_list = parsed_object_list[2731:3000]\n",
    "# # # def any(self, value):\n",
    "# # #    return self.add(\"([%s])\" % value)\n",
    "# # # def add(self, value):\n",
    "# # #    if isinstance(value, list):\n",
    "# # #       self.s.extend(value)\n",
    "# # #    else:\n",
    "# # #       self.s.append(value)\n",
    "# # #    return self\n",
    "# # low_alph_chars = ''.join([x.lower() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "# # up_alph_chars = ''.join([x.upper() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "# # root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "# # sub_root_beginnings = '-+('\n",
    "# # permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "# # pattern = '^['+re.escape(sub_root_beginnings)+'][^'+low_alph_chars+']+'\n",
    "# # print(pattern)\n",
    "# # print(re.search(pattern = pattern, string = '-KLHFGSLK JEs'))\n",
    "# # pattern = '^['+re.escape(sub_root_beginnings)+']['+re.escape(permissive_root_contents)+']+'\n",
    "# # print(pattern)\n",
    "# # print(re.search(pattern = pattern, string = 'KLHFGSLK JEs'))\n",
    "# noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "# nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "# print(nounPatternRegex)\n",
    "# s = 'n. dfhjkgqh (wsh) askldjhf / asdfhjkkh'\n",
    "# print(re.findall(nounPatternRegex,s))\n",
    "# print(nounPatternRegex.findall(s))\n",
    "\n",
    "# # if /, the next word is a plural. The plural also then usually has a (el) class after it. commas are multiple version\n",
    "# # so each lemma noun may have a (class). The lemma may have one or multiple plurals, which will have a modified (class)\n",
    "# # so have a nouns list? nouns may then be \n",
    "# # [('n.',{bool}), #only needed if this gets added to all POS? Or maybe theres information in its use?\n",
    "# # ([[{word}],{Optional[class]}]),  #singular\n",
    "#    #([[{word}],{Optional[class]}])]   #plural\n",
    "\n",
    "# # so nounStruct[0][1] is the flag for noun?\n",
    "#    # one POS table, and noun will get added there. But the presence of a noun indicates to look here\n",
    "# # classes may be found [sClass[1] for sClass in nounStruct[1]] will give all classes of singular, [sClass[1] for sClass in nounStruct[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration and Extraction - Dictionary Specific Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap \n",
    "#--no-stderr\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "para_text_lookup = {}\n",
    "root_ind_list = []\n",
    "subroot_ind_list = []\n",
    "lemma_ind_list = []\n",
    "some_error_ind_list = []\n",
    "reject_ind_list = []\n",
    "root_and_lemma_one_line = []\n",
    "root_lookup = {}\n",
    "lemma_lookup = {}\n",
    "pos_lookup = {}\n",
    "pos_ind_list = []\n",
    "\n",
    "up_alph_chars = [x.upper() for x in char_counts.keys() if x.upper() != x.lower()] #only uppercase alphabetical chars\n",
    "\n",
    "for i, entryObj in parsed_object_list:\n",
    "\n",
    "   try:\n",
    "      para_text_lookup[i] = entryObj.para_text\n",
    "      successful_cleaner_output:bool = entryObj.cleaner() #by default cleaner removes leading whitespace and merges adjacent runs with identical format features\n",
    "      # print('sucessful cleaner')\n",
    "      if not successful_cleaner_output:\n",
    "         reject_ind_list.append(i)\n",
    "         print(f'para# {i} IS ONLY whitespace. Need to drop it. #TODO')\n",
    "   except:\n",
    "      some_error_ind_list.append(f\"cleaner error on p: {i}. Text is {entryObj.interogate__para_text()}\")\n",
    "      print('error on cleaner')\n",
    "      raise\n",
    "   try:\n",
    "      root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "      sub_root_beginnings = '-+('\n",
    "      permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "\n",
    "      featureConfig = {\n",
    "      'root': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               # 'text_regex_at_feature': root_expression.compile()\n",
    "               },\n",
    "      'subroot': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               # 'text_regex_at_feature': subroot_expression.compile()\n",
    "               },\n",
    "      'lemma': {'docxFeature': 'run_bold',\n",
    "               'strSummary':'fontBold', \n",
    "               'value':True},\n",
    "      'lemmaPOS': {'docxFeature': 'run_italic',\n",
    "               'strSummary':'fontItalic', \n",
    "               'value':True},\n",
    "      }\n",
    "      is_subroot = False\n",
    "      # return True, (value_mask, run_text), (regex_mask, regex_matches)\n",
    "      is_root, (mask,run_text), _ = entryObj.single_run_feature_identify(featureConfig['root'])\n",
    "      if is_root:\n",
    "         rtext = ''.join(chain(compress(run_text,mask))).strip()\n",
    "         root_lookup[i] = rtext\n",
    "         #routine to distinguish between main root and subroot\n",
    "         for j, r in enumerate(compress(run_text,mask)): \n",
    "            if j==0:\n",
    "               # low_alph_chars = ''.join([x.lower() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "               up_alph_chars = ''.join([x.upper() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "               root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "               sub_root_beginnings = '-+('\n",
    "               permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "               pattern = '^['+re.escape(sub_root_beginnings)+']['+re.escape(permissive_root_contents)+']+'\n",
    "               m = re.search(pattern = pattern, string = r)\n",
    "               if m is not None:\n",
    "                  is_subroot = True\n",
    "         \n",
    "         if is_subroot:\n",
    "            print('\\n\\nsubroot at para number: ',i)\n",
    "            paraText = entryObj.interogate__para_text()\n",
    "            print('\\t',paraText)\n",
    "            subroot_ind_list.append(i)\n",
    "         else:\n",
    "            print('\\n\\nroot at para number: ',i)\n",
    "            paraText = entryObj.interogate__para_text()\n",
    "            print('\\t',paraText)\n",
    "            root_ind_list.append(i)\n",
    "      # return True, (value_mask, run_text), (regex_mask, regex_matches)\n",
    "      is_lemma, (mask, run_text), _ = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      if is_lemma:\n",
    "         # print(mask,run_text)\n",
    "         ltext = ''.join(chain(compress(run_text,mask))).strip()\n",
    "         # print(ltext)\n",
    "         lemma_lookup[i] = ltext\n",
    "         # paraText = entryObj.interogate__para_text()\n",
    "         # print('\\t\\tp#',i,'\\t\\t',paraText)\n",
    "         lemma_ind_list.append(i)\n",
    "      is_pos, (mask, run_text), _ = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      if is_pos:\n",
    "         postext = ''.join(chain(compress(run_text,mask))).strip()\n",
    "         noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "         nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "         pos_lookup[i] = postext\n",
    "         pos_ind_list.append(i)\n",
    "         if is_pos and not is_lemma:\n",
    "            entryObj.paragraph_logger(level = 10,msg = f'Unexpected structure:: para#{i} has a POS: {postext} but this para does NOT have a lemma. last lemma at para#{lemma_ind_list[-1]} was \"{lemma_lookup[lemma_ind_list[-1]]}\"',print_bool = True)\n",
    "      # if is_pos and (is_root or is_subroot):\n",
    "      if is_lemma and is_root:\n",
    "         print(f'this para# {i} has BOTH lemma AND root')\n",
    "         root_and_lemma_one_line.append(i)\n",
    "\n",
    "   except BaseException as e:\n",
    "      \n",
    "      some_error_ind_list.append(i)\n",
    "      raise e\n",
    "\n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n",
    "\n",
    "print('roots: ',len(root_ind_list))\n",
    "print('subroots: ',len(subroot_ind_list))\n",
    "print('lemmas: ',len(lemma_ind_list))\n",
    "print('root_and_lemma_one_line: ',len(root_and_lemma_one_line))\n",
    "print('additional cleaner rejects: ',len(reject_ind_list))\n",
    "print('additional error rejects: ',len(some_error_ind_list))\n",
    "\n",
    "print('num entities: ',len(root_ind_list) + len(lemma_ind_list) + len(subroot_ind_list))\n",
    "num_good_paras_of_other_content= len(root_ind_list) + len(lemma_ind_list) - len(root_and_lemma_one_line) + len(subroot_ind_list)\\\n",
    "                                    + len(reject_ind_list) + len(some_error_ind_list)\n",
    "print('num_good_paras_of_other_content: ',num_good_paras_of_other_content)\n",
    "# # Test messages\n",
    "logger.debug(\"logger debug test\")\n",
    "logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)\n",
    "\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open('parsed_objectClass_outcomes_dict.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    outcomes_dict = {}\n",
    "    outcomes_dict['parsed_object_list'] = parsed_object_list\n",
    "    outcomes_dict['para_text_lookup'] = para_text_lookup\n",
    "    outcomes_dict['root_ind_list'] = root_ind_list\n",
    "    outcomes_dict['subroot_ind_list'] = subroot_ind_list\n",
    "    outcomes_dict['lemma_ind_list'] = lemma_ind_list\n",
    "    outcomes_dict['root_and_lemma_one_line'] = root_and_lemma_one_line\n",
    "    outcomes_dict['root_lookup'] = root_lookup\n",
    "    outcomes_dict['lemma_lookup'] = lemma_lookup\n",
    "    outcomes_dict['char_counts'] = char_counts\n",
    "    pickle.dump(outcomes_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
