{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Create an easy/safe API to parse/hold/yield constructions of pydantic docx class objects.\n",
    "In theory this can be generalized and abstracted since the classes are largely absracted. \n",
    "However since this is API based, I will just focus on solutions for THIS dictionary task. These can be expanded later, or just have other APIs added\n",
    "Final objective is have a tool to pass complex commands/instructions, and have it yield a pandas dataframe of the results.\n",
    "Intermediate steps may be yield a list of lists\n",
    "I am not yet sure if there is benefit trying to have the whole dictionary in one notional object. \n",
    "I don't think so, since each operation is psuedo atomic, and any aggregate actions are intended to be done in something like pandas.\n",
    "Eventually I would want a loop for pandas operations to easily feed updates to the data. \n",
    "'''\n",
    "conclusion = '''create a pular dictionary specifc datastructure to handle the aggregate paragraphs. \n",
    "This will only be per lemma, \n",
    "as any aggregations from roots and relations of that sort can be managed in pandas and created by simple inheretance/reference\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "import pydantic\n",
    "from pydantic import ValidationError, validator, root_validator, Field, constr\n",
    "from pydantic_docx import Docx_Paragraph_and_Runs, read_docx #type:ignore\n",
    "from pydantic_docx_processor import create_sized_dataframe, expand_dataframe #type:ignore\n",
    "import re\n",
    "import json\n",
    "from itertools import compress, chain\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('feature_Frames_and_Indexes.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze\n",
    "    output = pickle.load(file, encoding='utf-8')\n",
    "(\n",
    "parsed_object_list, #:List[Tuple[int,Docx_Paragraph_and_Runs]]\n",
    "parsed_object_lookup, #:Dict[int,Docx_Paragraph_and_Runs] = dict(parsed_object_list)\n",
    "doc_para_count, #: int = int(parsed_to_dict['total_encountered_paragraphs']) #type: ignore\n",
    "char_counts, #: Counter = parsed_to_dict['char_counts'] #type: ignore\n",
    "rootFrame,#:pd.DataFrame\n",
    "rootsubpieceFrame, #:pd.DataFrame\n",
    "lemmaFrame, #:pd.DataFrame\n",
    "nonentityParaFrame, #pd.DataFrame\n",
    "cleanerOutcomesDf #pd.DataFrame\n",
    "    ) = output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan True]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "#first check the frame containing the success/failure boolean for the cleaner operation that was performed on the data in pydantic_docx ingestion\n",
    "print(cleanerOutcomesDf['cleaner_success_outcomes'].unique())\n",
    "cleanerOutcomesDf.dropna(subset=['paragraphIndex'],inplace=True)\n",
    "print(cleanerOutcomesDf['cleaner_success_outcomes'].unique())\n",
    "#results: all values with a valid paragraph will have a value in 'paragraphIndex'. ie paragraphs that did not have a suppressed error during class assignment. By default these are of only whitespace.\n",
    "#when parsing, the default was used, so any all whitespace paragraph will be nan in paragraphIndex.\n",
    "#when these values are dropped, the boolean column (indicating the successful post-processing of already created pydantic_docx class objects) yields only True values.\n",
    "#that was a complex way of saying \"none of the paragraphs had an error occur after their creation\"...\n",
    "# ...and therefor, all are \"good\" for the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following merge routine merges together the parents of one level of the heirarchy with their children. \n",
    "The parents then have their values forward-filled / duplicated to account for their headship over the child entities\n",
    "Each frame has an identity column with a boolean value indicating that index is the location of that entity.\n",
    "These are not needed for propogating headship and or child relationships, and these will not be forward-filled.\n",
    "As such, these will be removed from the entity frames before they are added to the entity heirarchy frame\n",
    "\n",
    "Precedence is as follows:\n",
    "root > subroot > lemmas > paragraphs\n",
    "\n",
    "However, the child relationship is non-exclusive to direct 'parents', such that a parent 'entity' may have a dhild from any of the levels below them.\n",
    "That said, the dictionary description and general structure would indicate paragraphs should only be direct children of Lemmas\n",
    ">paragraphs being defined as any line without an 'entity' such as root, subroot, or lemma.\n",
    ">any that are not would be a deviation. \n",
    "Therefor, any such instance needs to be captured and assessed, and should not be coerced into some other neighbors \"family\" by default, if at all.\n",
    "\n",
    "As a result, our routine for creating hierarchys must allow for any possible/permitted parent be available to match any children.\n",
    "\n",
    "So...\n",
    "\n",
    "Subroots:      possible parents are:      Roots\n",
    "Lemmas:        possible parents are:      Roots, Subroots\n",
    "Paragraphs:    possible parents are:      Roots, Subroots, Lemmas\n",
    "\n",
    "*Note: the term 'subroot' is sometimes described as 'root-subpiece', and these names may be understood interchangibly. The reason for the two names stems from two competing 'theories' or impressions of the nature of these entities. Initially I used the term subroot as a description of their subordinate position. The term subpiece has begun entering my vocabulary after discovering that many (if not all) of these are 'infixes', a core component of the Pular language. As a result any of these that are infixes are more than just a manual 'byte pair encoding' or common break point, but have specific meaning of their own.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphIndex</th>\n",
       "      <th>is</th>\n",
       "      <th>text</th>\n",
       "      <th>mask</th>\n",
       "      <th>run_text_list</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29717</th>\n",
       "      <td>29717.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>woɗeede (2)</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[woɗeede (2)  , v.pv  , F  [wojjude]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19872</th>\n",
       "      <td>19872.0</td>\n",
       "      <td>root</td>\n",
       "      <td>NDUPPAARA</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[NDUPPAARA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25069</th>\n",
       "      <td>25069.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>sippugol</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[sippugol , (ngol)  , Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9553</th>\n",
       "      <td>9553.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>haasidaaku</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[haasidaaku , (ngu,o)  , DZu  Z&lt;-&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063</th>\n",
       "      <td>4063.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>denkitaade</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[denkitaade  , v.mv+ext  , Z  &lt;FT,M&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18096</th>\n",
       "      <td>18096.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>mulgiɗidde</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[mulgiɗidde  , v.av+ext  , Z  &lt;DO,M&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16034</th>\n",
       "      <td>16034.0</td>\n",
       "      <td>root</td>\n",
       "      <td>LEFOL</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[LEFOL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>5189.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƊIRNOL</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƊIRNOL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32249</th>\n",
       "      <td>32249.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>ƴeewude,  ƴeewde</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[ƴeewude,  ƴeewde  , v.av  , DFCZ  C&lt;-&gt;,Z&lt;FJ,F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25645</th>\n",
       "      <td>25645.0</td>\n",
       "      <td>root</td>\n",
       "      <td>SU'-</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[SU'-]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paragraphIndex     is              text                  mask  \\\n",
       "index                                                                  \n",
       "29717         29717.0  lemma       woɗeede (2)  [True, False, False]   \n",
       "19872         19872.0   root         NDUPPAARA                [True]   \n",
       "25069         25069.0  lemma          sippugol  [True, False, False]   \n",
       "9553           9553.0  lemma        haasidaaku  [True, False, False]   \n",
       "4063           4063.0  lemma        denkitaade  [True, False, False]   \n",
       "18096         18096.0  lemma        mulgiɗidde  [True, False, False]   \n",
       "16034         16034.0   root             LEFOL                [True]   \n",
       "5189           5189.0   root            ƊIRNOL                [True]   \n",
       "32249         32249.0  lemma  ƴeewude,  ƴeewde  [True, False, False]   \n",
       "25645         25645.0   root              SU'-                [True]   \n",
       "\n",
       "                                           run_text_list  \n",
       "index                                                     \n",
       "29717              [woɗeede (2)  , v.pv  , F  [wojjude]]  \n",
       "19872                                        [NDUPPAARA]  \n",
       "25069                           [sippugol , (ngol)  , Y]  \n",
       "9553                 [haasidaaku , (ngu,o)  , DZu  Z<->]  \n",
       "4063               [denkitaade  , v.mv+ext  , Z  <FT,M>]  \n",
       "18096              [mulgiɗidde  , v.av+ext  , Z  <DO,M>]  \n",
       "16034                                            [LEFOL]  \n",
       "5189                                            [ƊIRNOL]  \n",
       "32249  [ƴeewude,  ƴeewde  , v.av  , DFCZ  C<->,Z<FJ,F...  \n",
       "25645                                             [SU'-]  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entity_identity_frame = rootFrame[['is']].merge(rootsubpieceFrame[['is']],how='outer', on='index').merge(lemmaFrame[['is']],how='outer',on='index')\n",
    "all_entity_identity_frame.columns = ['is_root', 'is_root_subpiece','is_lemma']\n",
    "\n",
    "# rootFrame contains the permitted parents for subroots.\n",
    "# print(rootFrame)\n",
    "rootFrame['is']='root'\n",
    "root_parent = rootFrame.copy()\n",
    "#next the frame of permitted parents for lemmas is created by combining/overlaying roots and subroots\n",
    "rootsubpieceFrame['is'] = 'root_subpiece'\n",
    "any_roots = root_parent.combine_first(rootsubpieceFrame)\n",
    "\n",
    "#finally create the permitted parents of paragraphs (though we will want to examine any paragraphs subordinate directly to a root. These may indicate some datacleaning is needed, or may need to be removed entirely)\n",
    "lemmaFrame['is'] = 'lemma'\n",
    "anyEntityFrame = any_roots.combine_first(lemmaFrame)\n",
    "\n",
    "anyEntityFrame.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these frames are set up, we can begin the routine. \n",
    "\n",
    "At each step, \n",
    "   >we merge the possible parents frame with the specific child frame.\n",
    "   >then we forward-fill the parent portion of the frame to indicate the headship for every record.\n",
    "   >- we want to 'unnest' this datastructure, and so we need to move from 'dictionary' style to 'record' style.\n",
    "   >- records require all pertinent infromation be present inside the record (records are usually defined as a single row in most industries)\n",
    "   >- so forward filling all possible parents will allow 'parentage' be infered by position / precedence on the page.\n",
    "   >  - a lemma is the child of the nearest 'parent' root UP the page from it, regardless if a root is directly below it.\n",
    "   >     - therefor forward-fill will allow these inferred relations be directly documented in each record, each row. \n",
    "   >     - SQL design says don't repeat yourself, but here we WANT to duplicate the parent value onto each child.\n",
    "   >     - This allows us to safely remove a 'child' from its context on the page without loosing who it's parents are.\n",
    "   >- then we merge in the 'frame' of next parent group, then child group, and repeat the forward filling \n",
    "   > \n",
    "   > *the process is iterative/recursive.*\n",
    "\n",
    "However, the parent/child relationships happen to align such that each next lower group of parents is composed of the parents + the child from the previous iteration...\n",
    "As a result we actually have a much simpler structure: we only need to merge each parent grouping directly, and forward fill all of those\n",
    "\n",
    "We will have some extra duplication from top parents being copied down on each level between them and their children, but we will solve this after we create it.\n",
    "\n",
    "The extra reason to group it this way is it does allow for a root to be declared on the same paragraph/line as a lemma, which is fairly common from manual review of the dictionary word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphIndex_main_root</th>\n",
       "      <th>is_main_root</th>\n",
       "      <th>text_main_root</th>\n",
       "      <th>mask_main_root</th>\n",
       "      <th>run_text_list_main_root</th>\n",
       "      <th>paragraphIndex_any_root</th>\n",
       "      <th>is_any_root</th>\n",
       "      <th>text_any_root</th>\n",
       "      <th>mask_any_root</th>\n",
       "      <th>run_text_list_any_root</th>\n",
       "      <th>paragraphIndex_any_entity</th>\n",
       "      <th>is_any_entity</th>\n",
       "      <th>text_any_entity</th>\n",
       "      <th>mask_any_entity</th>\n",
       "      <th>run_text_list_any_entity</th>\n",
       "      <th>paragraphIndex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>a</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[a  , prn,sbj,sf  , DFZH  Z&lt;-&gt;]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>a</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[a  , prn,sbj,sf  , DFZH  Z&lt;-&gt;]</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>a</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[a  , prn,sbj,sf  , DFZH  Z&lt;-&gt;]</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>root</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>-a</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[-a  , suf,pos  , F]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32502</th>\n",
       "      <td>32499.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-]</td>\n",
       "      <td>32499.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-]</td>\n",
       "      <td>32500.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>ƴuuƴude</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[ƴuuƴude  , v.av  , D]</td>\n",
       "      <td>32502.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32503</th>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32504</th>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32504.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>ƴuuƴude</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[ƴuuƴude  , v.av  , D]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32505</th>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32504.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>ƴuuƴude</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[ƴuuƴude  , v.av  , D]</td>\n",
       "      <td>32505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32506</th>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32503.0</td>\n",
       "      <td>root</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32504.0</td>\n",
       "      <td>lemma</td>\n",
       "      <td>ƴuuƴude</td>\n",
       "      <td>[True, False, False]</td>\n",
       "      <td>[ƴuuƴude  , v.av  , D]</td>\n",
       "      <td>32506.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32040 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       paragraphIndex_main_root is_main_root text_main_root mask_main_root  \\\n",
       "index                                                                        \n",
       "4                           4.0         root              A         [True]   \n",
       "5                           4.0         root              A         [True]   \n",
       "6                           4.0         root              A         [True]   \n",
       "7                           4.0         root              A         [True]   \n",
       "8                           4.0         root              A         [True]   \n",
       "...                         ...          ...            ...            ...   \n",
       "32502                   32499.0         root          ƳUUƳ-         [True]   \n",
       "32503                   32503.0         root    ƳUUƳ-  (2?)         [True]   \n",
       "32504                   32503.0         root    ƳUUƳ-  (2?)         [True]   \n",
       "32505                   32503.0         root    ƳUUƳ-  (2?)         [True]   \n",
       "32506                   32503.0         root    ƳUUƳ-  (2?)         [True]   \n",
       "\n",
       "      run_text_list_main_root  paragraphIndex_any_root is_any_root  \\\n",
       "index                                                                \n",
       "4                         [A]                      4.0        root   \n",
       "5                         [A]                      4.0        root   \n",
       "6                         [A]                      4.0        root   \n",
       "7                         [A]                      4.0        root   \n",
       "8                         [A]                      4.0        root   \n",
       "...                       ...                      ...         ...   \n",
       "32502                 [ƳUUƳ-]                  32499.0        root   \n",
       "32503           [ƳUUƳ-  (2?)]                  32503.0        root   \n",
       "32504           [ƳUUƳ-  (2?)]                  32503.0        root   \n",
       "32505           [ƳUUƳ-  (2?)]                  32503.0        root   \n",
       "32506           [ƳUUƳ-  (2?)]                  32503.0        root   \n",
       "\n",
       "      text_any_root mask_any_root run_text_list_any_root  \\\n",
       "index                                                      \n",
       "4                 A        [True]                    [A]   \n",
       "5                 A        [True]                    [A]   \n",
       "6                 A        [True]                    [A]   \n",
       "7                 A        [True]                    [A]   \n",
       "8                 A        [True]                    [A]   \n",
       "...             ...           ...                    ...   \n",
       "32502         ƳUUƳ-        [True]                [ƳUUƳ-]   \n",
       "32503   ƳUUƳ-  (2?)        [True]          [ƳUUƳ-  (2?)]   \n",
       "32504   ƳUUƳ-  (2?)        [True]          [ƳUUƳ-  (2?)]   \n",
       "32505   ƳUUƳ-  (2?)        [True]          [ƳUUƳ-  (2?)]   \n",
       "32506   ƳUUƳ-  (2?)        [True]          [ƳUUƳ-  (2?)]   \n",
       "\n",
       "       paragraphIndex_any_entity is_any_entity text_any_entity  \\\n",
       "index                                                            \n",
       "4                            4.0          root               A   \n",
       "5                            5.0         lemma               a   \n",
       "6                            5.0         lemma               a   \n",
       "7                            5.0         lemma               a   \n",
       "8                            8.0         lemma              -a   \n",
       "...                          ...           ...             ...   \n",
       "32502                    32500.0         lemma         ƴuuƴude   \n",
       "32503                    32503.0          root     ƳUUƳ-  (2?)   \n",
       "32504                    32504.0         lemma         ƴuuƴude   \n",
       "32505                    32504.0         lemma         ƴuuƴude   \n",
       "32506                    32504.0         lemma         ƴuuƴude   \n",
       "\n",
       "            mask_any_entity         run_text_list_any_entity  paragraphIndex  \n",
       "index                                                                         \n",
       "4                    [True]                              [A]             NaN  \n",
       "5      [True, False, False]  [a  , prn,sbj,sf  , DFZH  Z<->]             NaN  \n",
       "6      [True, False, False]  [a  , prn,sbj,sf  , DFZH  Z<->]             6.0  \n",
       "7      [True, False, False]  [a  , prn,sbj,sf  , DFZH  Z<->]             7.0  \n",
       "8      [True, False, False]             [-a  , suf,pos  , F]             NaN  \n",
       "...                     ...                              ...             ...  \n",
       "32502  [True, False, False]           [ƴuuƴude  , v.av  , D]         32502.0  \n",
       "32503                [True]                    [ƳUUƳ-  (2?)]             NaN  \n",
       "32504  [True, False, False]           [ƴuuƴude  , v.av  , D]             NaN  \n",
       "32505  [True, False, False]           [ƴuuƴude  , v.av  , D]         32505.0  \n",
       "32506  [True, False, False]           [ƴuuƴude  , v.av  , D]         32506.0  \n",
       "\n",
       "[32040 rows x 16 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subroot-parent columns -> merged with <- lemma-parents columns\n",
    "entityInheritanceFrame = root_parent.merge(any_roots, on='index',how ='outer',suffixes=['_main_root',''],sort=True, validate='one_to_one')\n",
    "\n",
    "#'subroot-parent' and 'lemma-parent' columns -> merged with <- 'paragraph-parents' columns\n",
    "entityInheritanceFrame = entityInheritanceFrame.merge(anyEntityFrame, on='index',how ='outer',suffixes=['_any_root','_any_entity'],sort=True, validate='one_to_one')\n",
    "\n",
    "#before forward-fill, but we want to remove \"non-paragraph\" rows from our data (if present), since we don't care about empty paragraphs as children to our 'parents' (remembering that empty paragraphs are all np.nan)\n",
    "#this should have no np.nan values in the final paragraph-parents column, but lets check before we accidentally hide the evidence\n",
    "# print(entityInheritanceFrame.shape) >>> (15266, 12)\n",
    "entityInheritanceFrame = entityInheritanceFrame.dropna(subset=['paragraphIndex_any_entity'])\n",
    "# print(entityInheritanceFrame.shape) >>> (15266, 12)\n",
    "\n",
    "#forward-fill np.nan values in the whole dataframe\n",
    "entityInheritanceFrame = entityInheritanceFrame.fillna(method = 'ffill') #this can be skipped here, as it is redundant, but its a valuable midway point, and informative to review and check our assumptions\n",
    "\n",
    "#'subroot-parent' and 'lemma-parent' and 'paragraph-parents' columns -> merged with <- 'paragraphs' columns\n",
    "#note the 'paragraphs' paraFrame contains valid and invalid paragraphs, like the cleaner-output frame we dealt with above.\n",
    "#it specifically only contains the indexes of paragraphs that are valid AND did not contain on of the entities we've been dealing with (roots, subroots, lemmas)\n",
    "nan_free_paraFrame = nonentityParaFrame.dropna() #this will remove all non-paragraph records\n",
    "allParagraphsInheritanceFrame = entityInheritanceFrame.merge(nan_free_paraFrame, on='index',how ='outer',suffixes=['','_paragraphs'],sort=True, validate='one_to_one')                                       \n",
    "#since we dropped np.nan values above, the only nan values should be introduced by the paragraph merge. Where present in the paragraph columns, these will be the declaration columns of the lemmas and roots.\n",
    "#to solve that we will forwardfill all columns BUT the new paragraphIndex column, which will fill in the parents for the new paragraphs.\n",
    "allParagraphsInheritanceFrame.loc[:,entityInheritanceFrame.columns] = allParagraphsInheritanceFrame.loc[:,entityInheritanceFrame.columns].fillna(method = 'ffill')\n",
    "\n",
    "#here, we can choose to dropna in the dataframe, which will remove all records that do not have a child paragraph in them.\n",
    "#this will be necessary for creating records to match to the wordnet, since the paragraphs will be our glosses we need.\n",
    "#however, any record that has a lemma, but does not have paragraphs still has valuable information, so we don't want to discard these completely, we just can't use them directly to search the wordnet.\n",
    "\n",
    "# identify_condition = partial(lambda x: bool(re.match('is_',x)))\n",
    "# subroot_condition = partial(lambda x: bool(re.search('_root_subpiece',x)) or bool(re.search('_anyroot',x)))\n",
    "\n",
    "# entityInheritanceFrame\n",
    "allParagraphsInheritanceFrame\n",
    "#TODO break oneliners out, break roots out of those, check numbers with other checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['paragraphIndex', 'cleaner_success_outcomes', 'is_root', 'root_text',\n",
      "       'root_mask', 'root_run_text_list'],\n",
      "      dtype='object')\n",
      "(6381, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphIndex</th>\n",
       "      <th>cleaner_success_outcomes</th>\n",
       "      <th>is_root</th>\n",
       "      <th>root_text</th>\n",
       "      <th>root_mask</th>\n",
       "      <th>root_run_text_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32502</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32503</th>\n",
       "      <td>32503.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ƳUUƳ-  (2?)</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[ƳUUƳ-  (2?)]</td>\n",
       "      <td>32503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32504</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32505</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32506</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32507 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       paragraphIndex cleaner_success_outcomes is_root    root_text root_mask  \\\n",
       "0                 NaN                      NaN     NaN          NaN       NaN   \n",
       "1                 NaN                      NaN     NaN          NaN       NaN   \n",
       "2                 NaN                      NaN     NaN          NaN       NaN   \n",
       "3                 NaN                      NaN     NaN          NaN       NaN   \n",
       "4                 4.0                     True    True            A    [True]   \n",
       "...               ...                      ...     ...          ...       ...   \n",
       "32502             NaN                      NaN     NaN          NaN       NaN   \n",
       "32503         32503.0                     True    True  ƳUUƳ-  (2?)    [True]   \n",
       "32504             NaN                      NaN     NaN          NaN       NaN   \n",
       "32505             NaN                      NaN     NaN          NaN       NaN   \n",
       "32506             NaN                      NaN     NaN          NaN       NaN   \n",
       "\n",
       "      root_run_text_list  index  \n",
       "0                    NaN      0  \n",
       "1                    NaN      1  \n",
       "2                    NaN      2  \n",
       "3                    NaN      3  \n",
       "4                    [A]      4  \n",
       "...                  ...    ...  \n",
       "32502                NaN  32502  \n",
       "32503      [ƳUUƳ-  (2?)]  32503  \n",
       "32504                NaN  32504  \n",
       "32505                NaN  32505  \n",
       "32506                NaN  32506  \n",
       "\n",
       "[32507 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(rootFrame.head())\n",
    "print(rootFrame.columns)\n",
    "entityInheritanceFrame = expand_dataframe(rootFrame,doc_para_count)\n",
    "print(rootFrame.shape)\n",
    "# entityInheritanceFrame = entityInheritanceFrame.join(rootFrame,on = 'index', how = 'outer')\n",
    "entityInheritanceFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "   \n",
    "   'txt_FulaLemmas',\n",
    "   'txt_FulaRoots',\n",
    "   \n",
    "   #extra\n",
    "   'txt_FulaAnnotations',\n",
    "   #gloss derivatives\n",
    "   'txt_FulaSenseEnglish',\n",
    "   'txt_FulaSenseFrench',\n",
    "   'txt_FulaSenseClassifications',\n",
    "   'txt_FulaSenseEnglishAnnotations',\n",
    "   'txt_FulaSenseFrenchAnnotations',\n",
    "\n",
    "   #lemma derivative\n",
    "   'txt_FulaDialects',\n",
    "   'txt_FulaPOSTags',\n",
    "   'txt_FulaSynonyms',\n",
    "\n",
    "\n",
    "   #agg\n",
    "   'txt_FulaInParenthesis',\n",
    "   'txt_POS',\n",
    "   'txt_RootOrigins'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('n\\\\.|(?<=\\\\()[^\\\\)]+|\\\\/')\n",
      "['n.', 'wsh', '/']\n",
      "['n.', 'wsh', '/']\n"
     ]
    }
   ],
   "source": [
    "# noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "# nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "# print(nounPatternRegex)\n",
    "# s = 'n. dfhjkgqh (wsh) askldjhf / asdfhjkkh'\n",
    "# print(re.findall(nounPatternRegex,s))\n",
    "# print(nounPatternRegex.findall(s))\n",
    "\n",
    "# if /, the next word is a plural. The plural also then usually has a (el) class after it. commas are multiple version\n",
    "# so each lemma noun may have a (class). The lemma may have one or multiple plurals, which will have a modified (class)\n",
    "# so have a nouns list? nouns may then be \n",
    "# [('n.',{bool}), #only needed if this gets added to all POS? Or maybe theres information in its use?\n",
    "# ([[{word}],{Optional[class]}]),  #singular\n",
    "   #([[{word}],{Optional[class]}])]   #plural\n",
    "\n",
    "# so nounStruct[0][1] is the flag for noun?\n",
    "   # one POS table, and noun will get added there. But the presence of a noun indicates to look here\n",
    "# classes may be found [sClass[1] for sClass in nounStruct[1]] will give all classes of singular, [sClass[1] for sClass in nounStruct[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAssert(dct,val,typ:type, strict = True) -> Any:\n",
    "   out = dct.get(val,None)\n",
    "   if strict and typ is type(None):\n",
    "      raise NotImplementedError(f'this get/assert function cannot \"get\" None values safely, and a {val} type was passed for dict {dict}')\n",
    "   if strict and out is None:\n",
    "      raise KeyError(f\"the provided dict {dct} did not have the key: {val}\")\n",
    "   assert isinstance(out,typ), f\"the provided dict {dct} did not give {val} with the expected type: {typ}, but instead {out}\"\n",
    "   return out\n",
    "   \n",
    "def trustyGet(obj:Docx_Paragraph_and_Runs, feat: str, silent_return = True) -> str:\n",
    "      if not silent_return:\n",
    "         output : str = getattr(obj,feat,'')\n",
    "         if len(output) > 0:\n",
    "            return output\n",
    "         else:\n",
    "            raise AttributeError(f'trusty getter could not find: {feat}')\n",
    "      else:\n",
    "         output : str = getattr(obj,feat,'')\n",
    "         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dict_entry(pydantic.BaseModel): \n",
    "   '''\n",
    "   #True is used as default value, and must be changed to False if the feature is not found. This will catch not implemented, and incomplete updates\n",
    "   '''\n",
    "   irregularities: List[str] = []\n",
    "   lemma_index: int \n",
    "   paragraphs: List[Docx_Paragraph_and_Runs] = Field(...,min_items = 1)\n",
    "   lemma: str = Field(...,min_length = 1) #article\n",
    "   root: str\n",
    "   root_subpiece: Union[str,bool] = True #these will not be updated after initialization\n",
    "   root_metadata: Dict[str,Any]  #index,root_text, root_meta_data #article \n",
    "   root_origin: Union[str,bool] = True #these will not be updated after initialization\n",
    "   #Paragraph Derivatives: Glosses and Annotations\n",
    "   lemmaLine_runs: List[str] #these will not be updated after initialization\n",
    "   englishGlossLine_runs: Union[List[str],bool] = True #these will not be updated after initialization\n",
    "   frenchGlossLine_runs: Union[List[str],bool] = True #these will not be updated after initialization\n",
    "   FulaAnnotations_runs: Union[List[List[str]],bool]= True #these will not be updated after initialization\n",
    "\n",
    "   #Gloss Derivatives: English Senses\n",
    "   FulaSenseEnglish: Union[List[str],bool] = True #list of \"senses\" split by semicolons #article\n",
    "   FulaSenseEnglish_Count: Union[int,bool] = True #number of senses in english #aka FulaSenseClassifications #article\n",
    "   FulaSenseEnglish_Annotations: Union[List[str],bool] = True #contains the annotations (in parenthesis) for a given sense, if any #article \n",
    "   FulaSenseEnglish_Synonyms: Union[List[str],bool] = True #holds bracket text for a sense, suspected all synonyms. These may all occur at the end, and may be redundant with the synonyms provided at the head of the entry\n",
    "   FulaSenseEnglish_unusedContent: Union[List[str],bool] = True #holds any run text that is not contained in the above features\n",
    "   \n",
    "   #Gloss Derivatives: French Senses\n",
    "   FulaSenseFrench: Union[List[str],bool] = True #article\n",
    "   FulaSenseFrench_Count: Union[int,bool] = True #aka FulaSenseClassifications\n",
    "   FulaSenseFrench_Annotations: Union[List[str],bool] = True\n",
    "   FulaSenseFrench_Synonyms: Union[List[str],bool] = True\n",
    "   FulaSenseFrench_unusedContent: Union[List[str],bool] = True\n",
    "   \n",
    "   #lemma derivative\n",
    "   lemmaLine_unusedContent: List[Tuple[int,str]] #= Field(...) #int is index for run to allow tracing as entries are removed\n",
    "   FulaDialects: Union[List[str],bool] = True #article\n",
    "   FulaPOSTags: Union[List[str],bool] = True #article\n",
    "   FulaPOSClass: Union[List[str],bool] = True #Noun, Adj, Verb, Adv, Prn, ..., Complicated, Indeterminate\n",
    "   FulaNoun_NounsAndClass: Union[List[Tuple[str,str]],bool] = True #Pular has unique noun classes. Lemma will be copied here beside its class (\"noun\", \"nounclass\"), and any additional singular forms will be in additional tuples in this same list\n",
    "   FulaNoun_PluralsAndClass: Union[List[Tuple[str,str]],bool] = True #Dict provides plurals for nouns. Tuples will have (\"noun\", \"nounclass\")\n",
    "   FulaSynonyms: Union[List[str],bool] = True #article\n",
    "   FulaCrossRef: Union[List[str],bool] = True\n",
    "         # FulaVerbClass:\n",
    "   \n",
    "   class Config:\n",
    "      validate_all = True\n",
    "      validate_assignment = True\n",
    "      smart_union = True  \n",
    "      extra = 'forbid'\n",
    "\n",
    "   @root_validator(pre=True)\n",
    "   def _validate_and_build(cls, values: Dict[str,Any]) -> Dict[str, Any]:\n",
    "      # print(values)\n",
    "      newValues: Dict[str,Any] = {}\n",
    "      newValues['irregularities'] = []\n",
    "\n",
    "      ###PARAGRAPHS CHECK###\n",
    "      print(values)\n",
    "      newValues['paragraphs'] = getAssert(values,'paragraphs',list) #these come in as tuples(ind,para)\n",
    "      #TODO have more rigorous checks on para since so much depends on that even in this validation\n",
    "      assert isinstance(newValues['paragraphs'][0],tuple), 'currently paras are packaged in a tuple with their original doc index'\n",
    "      newValues['paragraphs'] = [p for i,p in newValues['paragraphs']]\n",
    "      if len(newValues['paragraphs']) == 0:\n",
    "         raise ValueError('given invalid paragraphs of zero length')\n",
    "      else: #logic for subsidary paragraphs\n",
    "         lemma_line = trustyGet(newValues['paragraphs'][0],feat = 'run_text', silent_return=False)\n",
    "         newValues['lemmaLine_runs'] = lemma_line\n",
    "         if len(newValues['paragraphs']) == 1:\n",
    "            newValues['englishGlossLine_runs'] = False\n",
    "            newValues['frenchGlossLine_runs'] = False\n",
    "         elif len(newValues['paragraphs']) == 2:\n",
    "            newValues['irregularities'].append('What:ambiguous, Where:paragraphs, Why: only one')\n",
    "            newValues['englishGlossLine_runs'] = False\n",
    "            newValues['frenchGlossLine_runs'] = False\n",
    "         elif len(newValues['paragraphs']) == 3:\n",
    "            newValues['englishGlossLine_runs'] = newValues['paragraphs'][1].get_run_text()\n",
    "            newValues['frenchGlossLine_runs'] = newValues['paragraphs'][2].get_run_text()\n",
    "         else: \n",
    "            newValues['englishGlossLine_runs'] = newValues['paragraphs'][1].get_run_text()\n",
    "            newValues['frenchGlossLine_runs'] = newValues['paragraphs'][2].get_run_text()\n",
    "            newValues['FulaAnnotations_runs'] = newValues['paragraphs'][3:].get_run_text()\n",
    "      \n",
    "      \n",
    "      try: ###LEMMA CHECK###\n",
    "         #reading in lemma results from first pass of pydantic parser\n",
    "         lemma_index, lemma_mask, lemmaLine_runs = getAssert(values,'lemma',tuple)\n",
    "         #checking for correct structure\n",
    "         lemma_matched_runs = list(compress(lemmaLine_runs,lemma_mask))\n",
    "         if len(lemma_matched_runs) > 1:\n",
    "            newValues['irregularities'].append('What:Unexpected, Where: Lemmas, Why: the merge routines should aggregate adjacent runs with same features. Multiple Lemma runs should not be possible if Bold is contiguous')\n",
    "         lemma_text = ''.join(chain(lemma_matched_runs)).strip()\n",
    "            #TODO have better control for expected structure that these runs should be adjacent (and only should be one)\n",
    "         #saving values. These will have to pass type check of declared attribute types for validation to succeed\n",
    "         newValues['lemma'] = lemma_text\n",
    "         newValues['lemma_index'] = lemma_index\n",
    "         newValues['lemmaLine_runs'] = lemmaLine_runs\n",
    "         used_run_mask = lemma_mask\n",
    "      except Exception as e:\n",
    "         #exception lemma check\n",
    "         raise RuntimeError('failed lemma check') from e\n",
    "\n",
    "      try: ###ROOT CHECK###\n",
    "         #reading in root results from first pass of pydantic parser\n",
    "         #checking for correct structure\n",
    "         root_index, root_mask, rootLine_runs = getAssert(values,'root',tuple)\n",
    "         root_matched_runs = list(compress(rootLine_runs,root_mask))\n",
    "         if len(root_matched_runs) > 1:\n",
    "            newValues['irregularities'].append('What:Unexpected, Where: Root, Why: the merge routines should aggregate adjacent runs with same features. Multiple Root runs should not be possible if Fontsize is contiguous and unique')\n",
    "         root_text = ''.join(chain(root_matched_runs)).strip()\n",
    "         if root_index == newValues['lemma_index']:\n",
    "            newValues['irregularities'].append('What:inconsistent, Where: Lemmas and Root, Why:normally root and lemma are on different lines. This has them sharing, which may indicate a lack of other content')\n",
    "            used_run_mask = [any(run) for run in list(zip(used_run_mask,root_mask))]\n",
    "            rootLine_runs = False\n",
    "         newValues['root'] = root_text\n",
    "         newValues['root_metadata'] = {'root_index': root_index, 'root_runs':rootLine_runs}\n",
    "      except Exception as e:\n",
    "         #exception ROOT check\n",
    "         raise RuntimeError('failed ROOT check') from e\n",
    "\n",
    "      try: ###ROOT-Subpiece CHECK###\n",
    "         #iterating in case a subroot is present\n",
    "         root_index, root_mask, rootLine_runs = getAssert(values,'root',tuple)\n",
    "         root_matched_runs = list(compress(rootLine_runs,root_mask))\n",
    "         if len(root_matched_runs) > 1:\n",
    "            newValues['irregularities'].append('What:Unexpected, Where: Root-Subpiece, Why: the merge routines should aggregate adjacent runs with same features. Multiple Root runs should not be possible if Fontsize is contiguous and unique')\n",
    "         root_subpiece_text = ''.join(chain(root_matched_runs)).strip()\n",
    "         if root_index == newValues['lemma_index']:\n",
    "            newValues['irregularities'].append('What:inconsistent, Where: Lemmas and Root-Subpiece, Why:normally root and lemma are on different lines. This has them sharing, which may indicate a lack of other content')\n",
    "            used_run_mask = [any(run) for run in list(zip(used_run_mask,root_mask))]\n",
    "            rootLine_runs = False\n",
    "         newValues['root_subpiece'] = root_subpiece_text\n",
    "         newValues['root_metadata'] = {'root_subpiece_index': root_index, 'root_subpiece_runs':rootLine_runs}\n",
    "      except AssertionError as e:\n",
    "         newValues['root_subpiece'] = False\n",
    "      except Exception as e:\n",
    "         #exception ROOT-Subpiece check\n",
    "         raise RuntimeError('failed ROOT-Subpiece check') from e\n",
    "\n",
    "      #determining lemmaLine_unusedContent\n",
    "      unused_run_mask = [not r for r in used_run_mask]\n",
    "      print(list(compress(enumerate(newValues['lemmaLine_runs']),unused_run_mask)))\n",
    "      newValues['lemmaLine_unusedContent'] = list(compress(enumerate(newValues['lemmaLine_runs']),unused_run_mask))\n",
    "      \n",
    "      return newValues\n",
    "\n",
    "   def parse_senses(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_glossRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLine(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLineRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def get_rootOrigin(self):\n",
    "      #TODO\n",
    "      return\n",
    " \n",
    "   def give_entryText(self, joiner = '\\t') -> str: #article\n",
    "      return joiner.join([para.trustyGet('para_text') for para in self.paragraphs])\n",
    "\n",
    "   \n",
    "      \n",
    "            \n",
    "# dict_entry.parse_obj({'paragraphs':[b for a,b in parsed_object_list[5:8]]})\n",
    "\n",
    "# incoming = {\n",
    "#    'paragraphs': List[Docx_Paragraph_and_Runs],\n",
    "#    'root': List[int,str,List[bool]],\n",
    "#    'lemma': List[int,str,List[bool]],\n",
    "#    # 'pos': \n",
    "# }\n",
    "\n",
    "# [\n",
    "#    ['paras'], #obj\n",
    "#    ['parsed_paras'], #pydocx\n",
    "#    ['roots'], #name, fill forwards\n",
    "#    ['lemmas']  #text, fill forwards\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('parsed_objectClass_outcomes_dict.pkl', 'rb') as file:\n",
    "#     # Call load method to deserialze\n",
    "#     parsed_objectClass_outcomes_dict = pickle.load(file, encoding='utf-8')\n",
    "\n",
    "# parsed_object_list = parsed_objectClass_outcomes_dict['parsed_object_list'] \n",
    "# para_text_lookup = parsed_objectClass_outcomes_dict['para_text_lookup'] \n",
    "# root_ind_list = parsed_objectClass_outcomes_dict['root_ind_list'] \n",
    "# subroot_ind_list = parsed_objectClass_outcomes_dict['subroot_ind_list'] \n",
    "# lemma_ind_list = parsed_objectClass_outcomes_dict['lemma_ind_list'] \n",
    "# root_and_lemma_one_line = parsed_objectClass_outcomes_dict['root_and_lemma_one_line'] \n",
    "# root_lookup = parsed_objectClass_outcomes_dict['root_lookup'] \n",
    "# lemma_lookup = parsed_objectClass_outcomes_dict['lemma_lookup'] \n",
    "# # char_counts = parsed_objectClass_outcomes_dict['char_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "parsed_to_dict = read_docx(docx_filename)\n",
    "\n",
    "outcomes_dict = monolith_root_and_lemma_processor(parsed_to_dict['parsed_object_list'],parsed_to_dict['char_counts'])\n",
    "\n",
    "parsed_object_list = outcomes_dict['parsed_object_list'] \n",
    "para_text_lookup = outcomes_dict['para_text_lookup'] \n",
    "root_ind_list = outcomes_dict['root_ind_list'] \n",
    "subroot_ind_list = outcomes_dict['subroot_ind_list'] \n",
    "lemma_ind_list = outcomes_dict['lemma_ind_list'] \n",
    "root_and_lemma_one_line = outcomes_dict['root_and_lemma_one_line'] \n",
    "root_lookup = outcomes_dict['root_lookup'] \n",
    "lemma_lookup = outcomes_dict['lemma_lookup'] \n",
    "char_counts = outcomes_dict['char_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating lists of indexes for use in remaining processing\n",
    "\n",
    "# # # print(len(parsed_object_list)) #32040\n",
    "# #note this parsed paras are packaged with the original para number in the docx\n",
    "# #therefore 32507 is the index to use, but len of that IS 32040, since ~500 were empty paragraphs that were not parsed\n",
    "# parsed_para_indexes = [i for i,p in parsed_object_list]\n",
    "\n",
    "# lim = 32507\n",
    "# # rootInds = [x for x in root_ind_list if x < lim]\n",
    "# rootInds=root_ind_list\n",
    "# # subRootInds = [x for x in subroot_ind_list if x < lim]\n",
    "# subRootInds=subroot_ind_list\n",
    "# # lemmaInds = [x for x in lemma_ind_list if x < lim]\n",
    "# lemmaInds=lemma_ind_list\n",
    "\n",
    "# allroots = rootInds.copy()\n",
    "# allroots.extend(subRootInds)\n",
    "# allroots = sorted(allroots)\n",
    "# allEntities = allroots.copy()\n",
    "# allEntities.extend(lemmaInds)\n",
    "# allEntities = sorted(set(allEntities))\n",
    "# print('allEntities: ',len(allEntities))\n",
    "\n",
    "\n",
    "# # all_paras = [i for i,obj in parsed_object_list if i < lim]\n",
    "# normal_para = parsed_para_indexes.copy()\n",
    "# [normal_para.remove(i) for i in allEntities]\n",
    "# print('normal_para: ',len(normal_para))\n",
    "\n",
    "# # normal_para = [x for x in parsed_para_indexes if all([x not in rootInds, x not in lemmaInds, x not in subRootInds])]\n",
    "# # print(normal_para)\n",
    "# # print(allroots)\n",
    "# root_aligned_lemmas = list(closest(pairwise(allroots), lemmaInds))\n",
    "# lemma_aligned_paras = list(closest(pairwise(lemmaInds),normal_para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('entity_index_schema.pkl', 'rb') as file:\n",
    "#     # Call load method to deserialze\n",
    "#     entity_index_schema = pickle.load(file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r,v1 in entity_index_schema.items():\n",
    "#    root = (r,root_lookup[r][1],root_lookup[r][2])\n",
    "#    if isinstance(v1,dict):\n",
    "#       for k,v in v1.items():\n",
    "#          if k in subroot_ind_list:\n",
    "#             subroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paragraphs': [(8, Docx_Paragraph_and_Runs(run_italic=[None, True, None], para_left_indent=182880, run_font_name=['TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt'], paragraph_enumeration=8, run_font_size_pt=[None, None, 8.0], para_first_line_indent=-91440, run_text=['-a  ', 'suf,pos  ', 'F'], run_bold=[True, None, None], para_text='-a  suf,pos  F')), (9, Docx_Paragraph_and_Runs(run_italic=[None], para_left_indent=274320, run_font_name=['TmsRmn 10pt'], paragraph_enumeration=9, run_font_size_pt=[None], para_first_line_indent=-91440, run_text=['your (sg.) (only with certain nouns such as those which refer to close family members)'], run_bold=[None], para_text='your (sg.) (only with certain nouns such as those which refer to close family members)')), (10, Docx_Paragraph_and_Runs(run_italic=[None], para_left_indent=274320, run_font_name=['TmsRmn 10pt'], paragraph_enumeration=10, run_font_size_pt=[None], para_first_line_indent=-91440, run_text=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'], run_bold=[None], para_text='ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'))], 'root': (4, [True], ['A']), 'lemma': (8, [True, False, False], ['-a  ', 'suf,pos  ', 'F'])}\n",
      "[(1, 'suf,pos  '), (2, 'F')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_entry(irregularities=[], lemma_index=8, paragraphs=[Docx_Paragraph_and_Runs(run_italic=[None, True, None], para_left_indent=116128800, run_font_name=['TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt'], paragraph_enumeration=8, run_font_size_pt=[None, None, 8.0], para_first_line_indent=-91440, run_text=['-a  ', 'suf,pos  ', 'F'], run_bold=[True, None, None], para_text='-a  suf,pos  F'), Docx_Paragraph_and_Runs(run_italic=[None], para_left_indent=174193200, run_font_name=['TmsRmn 10pt'], paragraph_enumeration=9, run_font_size_pt=[None], para_first_line_indent=-91440, run_text=['your (sg.) (only with certain nouns such as those which refer to close family members)'], run_bold=[None], para_text='your (sg.) (only with certain nouns such as those which refer to close family members)'), Docx_Paragraph_and_Runs(run_italic=[None], para_left_indent=174193200, run_font_name=['TmsRmn 10pt'], paragraph_enumeration=10, run_font_size_pt=[None], para_first_line_indent=-91440, run_text=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'], run_bold=[None], para_text='ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)')], lemma='-a', root='A', root_subpiece='A', root_metadata={'root_subpiece_index': 4, 'root_subpiece_runs': ['A']}, root_origin=True, lemmaLine_runs=['-a  ', 'suf,pos  ', 'F'], englishGlossLine_runs=['your (sg.) (only with certain nouns such as those which refer to close family members)'], frenchGlossLine_runs=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'], FulaAnnotations_runs=True, FulaSenseEnglish=True, FulaSenseEnglish_Count=True, FulaSenseEnglish_Annotations=True, FulaSenseEnglish_Synonyms=True, FulaSenseEnglish_unusedContent=True, FulaSenseFrench=True, FulaSenseFrench_Count=True, FulaSenseFrench_Annotations=True, FulaSenseFrench_Synonyms=True, FulaSenseFrench_unusedContent=True, lemmaLine_unusedContent=[(1, 'suf,pos  '), (2, 'F')], FulaDialects=True, FulaPOSTags=True, FulaPOSClass=True, FulaNoun_NounsAndClass=True, FulaNoun_PluralsAndClass=True, FulaSynonyms=True, FulaCrossRef=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = []\n",
    "# for i in parsed_object_list[5:15]:\n",
    "#    result.append(dict_entry({i))\n",
    "# print(root_lookup[4])\n",
    "# print(para_text_lookup[3])\n",
    "# root_container = [(ind, mask, rootLine_runs) for ind, mask, rootLine_runs in getAssert(values,'root',list)]\n",
    "shorter_obj_list = parsed_object_list[:300]\n",
    "# for r, l\n",
    "inds = [8,9,10]\n",
    "\n",
    "dct = {\n",
    "   'paragraphs':[o for o in shorter_obj_list if o[0] in inds],\n",
    "   'root': (4,root_lookup[4][1],root_lookup[4][2]),\n",
    "   'lemma': (inds[0],lemma_lookup[inds[0]][1],lemma_lookup[inds[0]][2])\n",
    "}\n",
    "dict_entry.parse_obj(dct)\n",
    "\n",
    "# {'paragraphs': [(9, \n",
    "#    Docx_Paragraph_and_Runs(run_italic=[None], para_left_indent=274320, run_font_name=['TmsRmn 10pt'], paragraph_enumeration=9, run_font_size_pt=[None], para_first_line_indent=-91440, run_text=['your (sg.) (only with certain nouns such as those which refer to close family members)'], run_bold=[None], para_text='your (sg.) (only with certain nouns such as those which refer to close family members)')), (10, Docx_Paragraph_and_Runs(run_italic=[None], para_left_indent=274320, run_font_name=['TmsRmn 10pt'], paragraph_enumeration=10, run_font_size_pt=[None], para_first_line_indent=-91440, run_text=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'], run_bold=[None], para_text='ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)')), \n",
    "#    (11, Docx_Paragraph_and_Runs(run_italic=[None, True, None], para_left_indent=182880, run_font_name=['TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt'], paragraph_enumeration=11, run_font_size_pt=[None, None, 8.0], para_first_line_indent=-91440, run_text=['aan  ', 'prn,ind  ', 'DFZH  [an]:Z<->'], run_bold=[True, None, None], para_text='aan  prn,ind  DFZH  [an]:Z<->'))], 'root': [(4, [True], ['A'])], 'lemma': (5, [True, False, False], ['a  ', 'prn,sbj,sf  ', 'DFZH  Z<->'])\n",
    "#    } did not have the key: paragraphs\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
