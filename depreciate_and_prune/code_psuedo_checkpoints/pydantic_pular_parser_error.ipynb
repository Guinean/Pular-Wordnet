{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Draft or create a class system to represent the pular entries. \n",
    "Ideally this will contain a way to nest entry objects under a root\n",
    "'''\n",
    "#%pip install docx\n",
    "#%pip install python-docx #this mutates docx? \n",
    "#%pip install pydantic\n",
    "#%pip install mypy\n",
    "# %pip install numpy\n",
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "from pydantic import BaseModel, ValidationError, validator, root_validator, Field, constr\n",
    "import json\n",
    "import docx\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from itertools import compress, tee, chain\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "from verbalexpressions import VerEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# output_name = f\"{current_time}_result.txt\"\n",
    "# experiment = input(\"Enter emperiment description:\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/initialization_placeholder.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "# # Test messages\n",
    "logger.debug(\"current_time\")\n",
    "# logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "def logger_root_validation_error_messages(e, logger_details, suppress = [], run_enumeration: Optional[int] = None) -> Union[RuntimeError, TypeError]:      \n",
    "   #TODO add ability to handle assertion errors\n",
    "   if run_enumeration is not None:\n",
    "      run_num = f\"|run#{run_enumeration}|\" #type: ignore \n",
    "   else:\n",
    "      run_num = \"\"\n",
    "   try:\n",
    "      for err in e.errors():\n",
    "         if err['type'] in suppress['type'] or err['msg'] in suppress['msg']:\n",
    "            logger.info(f\"|SUPRESSED|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"suppressed Validation Error\")\n",
    "         else:\n",
    "            logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with validation? error: {err}\")\n",
    "            return TypeError(\"un-suppressed Validation Error\")\n",
    "   except:\n",
    "      logger.error(f\"|unsuppressed|{logger_details['function']}|{type(e)}|para#{logger_details['paragraph_enumeration']}{run_num}, with error: {e}\")\n",
    "      return RuntimeError(\"non-validation error\")\n",
    "   return RuntimeError(\"non-validation error\")\n",
    "\n",
    "\n",
    "def pular_str_strip_check(s:str) ->bool:\n",
    "   in_len = len(s)\n",
    "   new_s = s.strip()\n",
    "   out_len = len(new_s)\n",
    "   purported_whitespace: bool = in_len != out_len\n",
    "   return purported_whitespace\n",
    "\n",
    "value_mask = [True,False,False,True]\n",
    "regex_mask = [False,False,False,True]\n",
    "\n",
    "any(compress(value_mask,regex_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docx_Paragraph (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   # docx_document_paragraph: Optional[Any] #This should be validated below. Left optional because its inclusion causes problems with default repr and serialization\n",
    "   para_text: str = Field(..., min_length = 1) ##required, must be string, must be 1 long or more\n",
    "   para_first_line_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "   para_left_indent: Optional[int] = Field(...) #Required, but must be int OR none. https://pydantic-docs.helpmanual.io/usage/models/#required-optional-fields \n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      #extract para features, \n",
    "      new_values['para_text'] = para.text #type: ignore\n",
    "      new_values['para_first_line_indent'] = para.paragraph_format.first_line_indent #type: ignore\n",
    "      new_values['para_left_indent'] = para.paragraph_format.left_indent #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run (BaseModel):\n",
    "   \"\"\"input:   run = your_run_here\n",
    "   \n",
    "   when given a docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   run_text : str = Field(..., min_length = 1) #required, must be string, must be 1 long or more\n",
    "   run_font_name : Optional[str] = Field(...) #required, must be string or None value\n",
    "   run_font_size_pt : Optional[float] = Field(...)#Required, but must be float OR none value\n",
    "   run_bold : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "   run_italic : Optional[bool] = Field(...) #Required, but must be bool OR none value\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      run = values.get(\"run\",False)\n",
    "      assert isinstance(run, eval('docx.text.run.Run')), 'please enter a docx run assigned to the variable \"run\", in the form of     run = your_run_here'\n",
    "      \n",
    "      new_values : Dict[str, Any] = {}\n",
    "      #loop through the runs in the paragraph and select the desired features\n",
    "      new_values['run_text'] = run.text #type: ignore\n",
    "      new_values['run_font_name'] = run.font.name #type: ignore\n",
    "      if run.font.size is not None: #type: ignore\n",
    "         new_values['run_font_size_pt'] = run.font.size.pt #type: ignore\n",
    "      else: new_values['run_font_size_pt'] = None\n",
    "      new_values['run_bold'] = run.bold #type: ignore\n",
    "      new_values['run_italic'] = run.italic #type: ignore\n",
    "\n",
    "      return new_values\n",
    "\n",
    "\n",
    "class Docx_Run_List (BaseModel):\n",
    "   \"\"\"input:   run_list = your_runs_in_a_list\n",
    "   \n",
    "   when given a list of docx document paragraphs run object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "   #because the internals are validated, don't need to validate these other than that they were made into lists\n",
    "   run_text : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_name : List[Any] = Field(...) #Required, must be list\n",
    "   run_font_size_pt : List[Any] = Field(...) #Required, must be list\n",
    "   run_bold : List[Any] = Field(...) #Required, must be list\n",
    "   run_italic : List[Any] = Field(...) #Required, must be list\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "      from collections import defaultdict\n",
    "      paragraph_enumeration = values.get('paragraph_enumeration',\"<<FAILURE_paragraph_enumeration>>\")\n",
    "      runs = values.get(\"run_list\",False)\n",
    "      if not runs:\n",
    "         raise ValueError('please enter a docx run list assigned to the variable \"run_list\", in the form of     run_list = your_run_list_here')\n",
    "      new_values = defaultdict(list)\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      logger_details = {'function':'parsed_run', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      \n",
    "      for run_enumumeration, run in enumerate(runs): #type: ignore\n",
    "         try:\n",
    "            parsed_run = Docx_Run(**{'run':run}) #this manner of root unpacking seems to give warnings since linter can't assess ahead of time\n",
    "            assert isinstance(parsed_run, Docx_Run), 'RUNTIME_ERR - the docx run object did not return the type expected'\n",
    "            for k,v in parsed_run.dict().items():\n",
    "               new_values[k].append(v) \n",
    "\n",
    "         except BaseException as e:\n",
    "            new_e = logger_root_validation_error_messages(e, logger_details, suppress,run_enumeration=run_enumumeration)\n",
    "            raise new_e\n",
    "             \n",
    "      return new_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docx_Paragraph_and_Runs (BaseModel):\n",
    "   \"\"\"input:   paragraph = your_paragraph_here\n",
    "   \n",
    "   when given a docx document's paragraph object, will parse it to a specified schema\n",
    "   \"\"\"\n",
    "\n",
    "   class Config:\n",
    "      extra = 'allow'\n",
    "      # arbitrary_types_allowed = True\n",
    "\n",
    "   @root_validator(pre=True) #TODO Try have post validator for runs only?\n",
    "   def _docx_structure_check(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      new_values: Dict[str, Any] = {}\n",
    "      para = values.get(\"paragraph\",False)\n",
    "      assert isinstance(para, eval('docx.text.paragraph.Paragraph')), 'please enter a docx paragraph assigned to the variable \"paragraph\", in the form of     paragraph = your_paragraph_here'\n",
    "      \n",
    "      paragraph_enumeration: int = values.get('paragraph_enumeration',None)\n",
    "      assert isinstance(paragraph_enumeration, int), \"assertion error, bad paragraph count/paragraph_enumeration value passed. Please pass an integer\"\n",
    "      new_values['paragraph_enumeration'] = paragraph_enumeration\n",
    "\n",
    "      \n",
    "      #setting up error and logger handling\n",
    "      #suppress these errors\n",
    "      suppress = {'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "                           ],\n",
    "                  'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      #try to extract para features, \n",
    "      logger_details = {'function':'Docx_Paragraph', 'paragraph_enumeration':paragraph_enumeration }\n",
    "      try: \n",
    "         parsed_paras = Docx_Paragraph(**{'paragraph':para}) #type: ignore\n",
    "         for k,v in parsed_paras.dict().items():\n",
    "            new_values[k] = v\n",
    "      # except ValidationError as e:\n",
    "      #    logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "\n",
    "      #try to extract runs features\n",
    "      logger_details = {'function':'Docx_Run_List', 'paragraph_enumeration':paragraph_enumeration }    \n",
    "      try:\n",
    "         parsed_runs = Docx_Run_List(**{'run_list':para.runs, 'paragraph_enumeration':paragraph_enumeration}) #type: ignore\n",
    "         for k,v in parsed_runs.dict().items():\n",
    "            new_values[k] = v\n",
    "      except BaseException as e:\n",
    "         new_e = logger_root_validation_error_messages(e, logger_details, suppress)\n",
    "         raise new_e\n",
    "         \n",
    "      return new_values\n",
    "      \n",
    "\n",
    "   def interogate__para_text(self) -> str:\n",
    "      t = getattr(self, 'para_text', \"\")\n",
    "      # \n",
    "      if len(t) == 0:\n",
    "         logger.warning('interogator did not find para_text')\n",
    "      #    print(\"no para_text with:\\n\\t\", self.dict())\n",
    "      return t\n",
    "\n",
    "   def paragraph_logger(self,level:int,msg:str,print_bool:bool):\n",
    "      if print_bool:\n",
    "         print(msg)\n",
    "      else:\n",
    "         logger.log(level,msg)\n",
    "\n",
    "\n",
    "   def single_run_feature_identify(self,params:Dict[str,Any]) -> Tuple[bool,Tuple[List[bool],List[Any]],Tuple[List[bool],List[Optional[str]]]]: \n",
    "      \"\"\"if regex provided, must be in param dict with name 'text_regex_at_feature', and must be passed as a r'pattern' raw string\n",
    "      return tuple of ('feature boolean', feature_Tuple[boolean mask, feature list], regex_tuple[boolean mask, regex match list])\n",
    "      \"\"\"\n",
    "      enumeration : Optional[int] = getattr(self,\"paragraph_enumeration\",None)\n",
    "      assert isinstance(enumeration, int),f\"bad value for 'paragraph_enumeration' {enumeration}\"\n",
    "      run_texts : Optional[List[str]] = getattr(self,'run_text',None)\n",
    "      assert run_texts is not None,f\"bad value for 'run_text' {self.__repr__()}\"\n",
    "      feature = params['docxFeature']\n",
    "      assert isinstance(feature,str),f\"bad value for parameter 'docxFeature'. Check params: {params}\"\n",
    "      text_regex_at_feature = params.get('text_regex_at_feature',False)\n",
    "      regex_mask: List[bool] = []\n",
    "      regex_matches: List[Optional[str]] = []\n",
    "\n",
    "      values_from_runs: List[Optional[Union[float,bool]]] = getattr(self,feature,[None]) \n",
    "      value_mask: List[bool] = [True if x == params['value'] else False for x in values_from_runs]\n",
    "\n",
    "      if text_regex_at_feature:\n",
    "         # logger.error('inside regex bool for para#', enumeration)\n",
    "         pattern = text_regex_at_feature\n",
    "         for text in run_texts:\n",
    "            match = re.search(pattern, text) #type: ignore\n",
    "            if match is not None:\n",
    "               regex_mask.append(True)\n",
    "               regex_matches.append(match.group(0))\n",
    "            else:\n",
    "               regex_mask.append(False)\n",
    "               regex_matches.append(None)\n",
    "         logger.error('inside regex bool for para#', enumeration,'\\tregex_mask_is: ',regex_mask,'\\t\\tvalue_mask is: ',value_mask)\n",
    "         if any(compress(value_mask,regex_mask)):\n",
    "            return True, (value_mask, values_from_runs), (regex_mask, regex_matches)  #has Feature\n",
    "         else:\n",
    "            return False, (value_mask, values_from_runs), (regex_mask, regex_matches) #does not have feature   \n",
    "      else: \n",
    "         if any(value_mask):\n",
    "            return True, (value_mask, values_from_runs), (regex_mask, regex_matches)  #has Feature\n",
    "         else:\n",
    "            return False, (value_mask, values_from_runs), (regex_mask, regex_matches) #does not have feature\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "   def modify_run_lists(self, drop_runs: Optional[List[int]] = None, add_runs: Optional[Tuple[int, List[List[Any]]]] = None, merge_runs : bool = False): #-> Optional[Dict[str, List[List[Any]]]]\n",
    "      \"\"\"given a list of indexes as 'drop' will drop those indexes from runlists, and return those dropped\n",
    "      given a tuple with an integer index and list of lists (run aligned), will add those to entries to the runlists at that index\n",
    "      given bool merge, will greedy merge all runs with the same run features EXCEPT run_text. Run_texts will be concatenated\n",
    "      \"\"\"\n",
    "      run_list_req_features: List[str] = Docx_Run_List.schema()['required']\n",
    "      assert run_list_req_features[0] == 'run_text', \"first feature in the schema should be run_text\"\n",
    "      para_enumeration = getattr(self, 'paragraph_enumeration',None)\n",
    "      assert para_enumeration is not None, 'paragraph did not have an enumeration value'\n",
    "\n",
    "      feature_run_lists : List[List[Any]] = []\n",
    "      for f in run_list_req_features:\n",
    "         feature_run_lists.append(getattr(self,f,[]))\n",
    "      pivoted_run_lists = list(map(list, zip(*feature_run_lists)))\n",
    "      number_of_runs : int = len(pivoted_run_lists)\n",
    "      if number_of_runs < 1:\n",
    "         raise ValueError('this paragraph does not have values in the run lists')\n",
    "\n",
    "      merge_occured = False\n",
    "\n",
    "      if drop_runs is not None:\n",
    "         dropped_runs = {}\n",
    "         logger.info('tried to drop a run from para#',para_enumeration)\n",
    "         for ind in drop_runs:\n",
    "            dropped_runs[ind] = pivoted_run_lists.pop(ind) #mutates pivoted_run_lists\n",
    "         if number_of_runs == len(pivoted_run_lists):\n",
    "            raise RuntimeError('the runs_lists were not shortened as expected')\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "      if add_runs is not None:\n",
    "         insert_ind = add_runs[0]\n",
    "         add_lists = add_runs[1]\n",
    "         assert len(add_lists[0]) == number_of_runs, \"the added list of lists must have runs of the same length (feature space) as run_lists features in the schema: Docx_Run_List.schema()['required']\"\n",
    "         if insert_ind == -1:\n",
    "            insert_ind = number_of_runs\n",
    "         for lst in add_lists:\n",
    "            pivoted_run_lists.insert(insert_ind,lst)\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "      \n",
    "      if merge_runs is not False:\n",
    "         i = 0\n",
    "         still_merging = True\n",
    "         while still_merging:\n",
    "            pairs = list(pairwise(list(range(len(pivoted_run_lists))))) #index pairs\n",
    "            if len(pairs) < 1: #onely 1 run, which causes pairwise to yield empty lists since nothing to pair with\n",
    "               break\n",
    "            num_merged = 0\n",
    "            for a,b in pairs: #where a,b are indexes in the pivoted run list (each index is one run)\n",
    "               a -= num_merged #mutate pivot indexes after the pivot array has been mutated\n",
    "               b -= num_merged\n",
    "               if pivoted_run_lists[a][1:] == pivoted_run_lists[b][1:]: #if all features EXCEPT run_text are the same #TODO add ability to config which features to merge on\n",
    "                  pivoted_run_lists[b][0] = pivoted_run_lists[a][0] + pivoted_run_lists[b][0]\n",
    "                  pivoted_run_lists.pop(a)\n",
    "                  num_merged +=1\n",
    "                  merge_occured = True #flag for end of function, to determine if any changes need to be set to 'self'\n",
    "               else: pass \n",
    "            if num_merged < 1: #if no merges where made in this iteration, merging is done. Else keep while loop since new merges may occur with new neighbors\n",
    "               still_merging = False\n",
    "         number_of_runs : int = len(pivoted_run_lists)\n",
    "         feature_run_lists = list(map(list, zip(*pivoted_run_lists)))\n",
    "\n",
    "      if any([drop_runs is not None, add_runs is not None, merge_occured]):\n",
    "         for i, f in enumerate(run_list_req_features):\n",
    "            self.__setattr__(f,feature_run_lists[i])\n",
    "\n",
    "\n",
    "      \n",
    "   def cleaner(self, execute_defaults: bool = True) -> bool : #params:Optional[Dict[str,Any]],\n",
    "      \"\"\"defaults to running \"remove_para_leading_whitespace\". This removes leading runs that are blank, and strips the first text run of any LEADING whitespace, if any is present.\n",
    "      the params dict is not implemented currently\n",
    "      returns bool value. True means cleaner would yield a valid para. False currently indicates all runs in para are whitespace.\n",
    "      \"\"\"\n",
    "      #TODO aggregate these getattrs so that every function doesn't need to get it themselves. Or simplify this with a function that has an assert bool to require it or not\n",
    "      para_enumeration = getattr(self, 'paragraph_enumeration',None)\n",
    "      assert para_enumeration is not None, 'paragraph did not have an enumeration value'\n",
    "\n",
    "      def remove_para_leading_whitespace(start_ind : int = 0): #run \n",
    "         # try: #expect to fail when reaches the end of the list\n",
    "         para_text : Optional[str] = getattr(self, 'para_text',None)\n",
    "         if isinstance(para_text,str):\n",
    "            if len(para_text.strip()) == 0: #if para's text is ONLY whitespace\n",
    "               return False\n",
    "         run_text_list : List[str] = getattr(self, 'run_text',[''])\n",
    "         num_runs = len(run_text_list)\n",
    "\n",
    "         ind = start_ind\n",
    "         droppable_runs : List[int] = [] #TODO this dropable section doesnt seem to be working correctly.\n",
    "         while ind < num_runs:\n",
    "            this_run_text = run_text_list[ind]\n",
    "            stripped_run = this_run_text.lstrip() #TODO pass config to this to allow control of what can and can't be dropped.\n",
    "            if len(stripped_run) == 0: #found ALL whitespace run. Need to iterate to see if next run is blank or has any leading whitespace\n",
    "               droppable_runs.append(ind) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "               logger.info(f'paragraph#{para_enumeration} with text \"\"{para_text}\"\" had a run with ONLY whitespace')\n",
    "            elif len(stripped_run) < len(this_run_text): #found run that is NOT ALL whitespace, but had SOME. Will only happen once. Can stop now since this is the true beginning of this paragraph\n",
    "               run_text_list[ind] = stripped_run\n",
    "               self.__setattr__(\"run_text\", run_text_list) #TODO convert this change to a an equivalent para_indent, since this paragraph likely has incorrect indents\n",
    "               logger.info(f'paragraph#{para_enumeration} with text \"\"{para_text}\"\" had leading whitespace removed')\n",
    "               break\n",
    "            else: #Can stop now since this is the true beginning of this paragraph\n",
    "               break\n",
    "            ind +=1\n",
    "            \n",
    "         if len(droppable_runs) > 0: #if a whole run_text was whitespace only\n",
    "            if len(droppable_runs) == num_runs: #if the whole paragraph was whitespace only\n",
    "               raise RuntimeError(f'for paragraph#{para_enumeration}, all runs purported droppable whitespace, but para_text purported not')\n",
    "            self.modify_run_lists(drop_runs = droppable_runs) #this removes whole runs, not just modifying the run_text.\n",
    "            logger.info(f'paragraph#{para_enumeration} with text \"\"{para_text}\"\" tried to drop a run whitespace')\n",
    "\n",
    "      if execute_defaults:\n",
    "         remove_para_leading_whitespace()\n",
    "         self.modify_run_lists(merge_runs = True)\n",
    "\n",
    "      return True\n",
    "\n",
    "\n",
    "\n",
    "   # def paragraph_splitter(self): #almost certainly only going to be only the lemmas from roots\n",
    "      #single_run_feature_identify(condition) -> mask\n",
    "      #find index in mask where to split\n",
    "      #create a clone of the object (default para_enumeration)\n",
    "         #need to change class to allow all para enumerations to be float\n",
    "         #default para_enumeration float split size (float p_e.##?)\n",
    "      #modify_run_lists to drop the last runs from para_A, and first runs from para_B\n",
    "      #run cleaner again to remove any leading whitespace in new para?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total paras:  32507\n",
      "parsed paras:  32040\n",
      "handled errors:  467\n",
      "failed paras:  0\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "# output_name = f\"logs_and_outputs/{current_time}_docxFileParseResult.txt\"\n",
    "# experiment = input(\"Enter emperiment description:\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) \n",
    "\n",
    "#Run docx module to parse the docx file\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "document = Document(docx_filename)\n",
    "\n",
    "\n",
    "\n",
    "char_counts = Counter()\n",
    "\n",
    "docx_object_list = []\n",
    "parsed_object_list = []\n",
    "failed_paras_ind = []\n",
    "handled_errors = []\n",
    "\n",
    "for i, para in enumerate(document.paragraphs):\n",
    "   docx_object_list.append((i,para))\n",
    "   try:\n",
    "      entryObj = Docx_Paragraph_and_Runs(**{'paragraph': para, 'paragraph_enumeration': i})\n",
    "      char_counts.update(entryObj.interogate__para_text())\n",
    "      parsed_object_list.append((i,entryObj))\n",
    "   except ValidationError as e:\n",
    "      suppress = {\n",
    "            # 'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "            #          ],\n",
    "            'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      for err in e.errors():\n",
    "         if err['msg'] in suppress['msg']:\n",
    "            handled_errors.append((i,para))\n",
    "            pass\n",
    "   except BaseException as e:\n",
    "      print(e)\n",
    "      failed_paras_ind.append((i,para))\n",
    "      \n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "#create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "output_name = f\"logs_and_outputs/{current_time}_objList_processing_Output.txt\"\n",
    "experiment = input(\"Enter emperiment description:\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "logger_filename = f\"logs_and_outputs/{current_time}objList_processing.log\"\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG, etc\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#add encoding\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "logger.addHandler(handler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36memit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m             \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36mformat\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    868\u001b[0m             \u001b[0mfmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_defaultFormatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36mformat\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \"\"\"\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musesTime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36mgetMessage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7536\\1852980085.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m    \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m       \u001b[0msome_error_ind_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m       \u001b[1;31m# if not e.args[0][0].exc.args[0] == 'suppressed Validation Error':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7536\\1852980085.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m       \u001b[1;31m# return True, (value_mask, values_from_runs), (regex_mask, regex_matches)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m       \u001b[0mis_subroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentryObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msingle_run_feature_identify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureConfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subroot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mis_subroot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m          \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\nsubroot at para number: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7536\\919162417.py\u001b[0m in \u001b[0;36msingle_run_feature_identify\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m     94\u001b[0m                \u001b[0mregex_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                \u001b[0mregex_matches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m          \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inside regex bool for para#'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumeration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\tregex_mask_is: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregex_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\t\\tvalue_mask is: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m          \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregex_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalue_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_from_runs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mregex_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex_matches\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#has Feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \"\"\"\n\u001b[0;32m   1406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misEnabledFor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36m_log\u001b[1;34m(self, level, msg, args, exc_info, extra, stack_info)\u001b[0m\n\u001b[0;32m   1512\u001b[0m         record = self.makeRecord(self.name, level, fn, lno, msg, args,\n\u001b[0;32m   1513\u001b[0m                                  exc_info, func, extra, sinfo)\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \"\"\"\n\u001b[0;32m   1523\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallHandlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maddHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36mcallHandlers\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1584\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevelno\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1586\u001b[1;33m                     \u001b[0mhdlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m                 \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m    \u001b[1;31m#break out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36memit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1125\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1127\u001b[1;33m         \u001b[0mStreamHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36memit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandleError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\logging\\__init__.py\u001b[0m in \u001b[0;36mhandleError\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    945\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--- Logging error ---\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                 \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Call stack:\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m                 \u001b[1;31m# Walk the stack frame up until we're out of logging,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\traceback.py\u001b[0m in \u001b[0;36mprint_exception\u001b[1;34m(etype, value, tb, limit, file, chain)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     for line in TracebackException(\n\u001b[1;32m--> 104\u001b[1;33m             type(value), value, tb, limit=limit).format(chain=chain):\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\traceback.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, _seen)\u001b[0m\n\u001b[0;32m    506\u001b[0m         self.stack = StackSummary.extract(\n\u001b[0;32m    507\u001b[0m             \u001b[0mwalk_tb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlookup_lines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m             capture_locals=capture_locals)\n\u001b[0m\u001b[0;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;31m# Capture now to permit freeing resources: only complication is in the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\peter\\VSC-Workspace_PULAR\\.pular_venv\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \"\"\"\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture cap \n",
    "#--no-stderr\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "root_ind_list = []\n",
    "subroot_ind_list = []\n",
    "lemma_ind_list = []\n",
    "some_error_ind_list = []\n",
    "reject_ind_list = []\n",
    "root_and_lemma_one_line = []\n",
    "\n",
    "up_alph_chars = [x.upper() for x in char_counts.keys() if x.upper() != x.lower()] #only uppercase alphabetical chars\n",
    "\n",
    "for i, entryObj in parsed_object_list:\n",
    "\n",
    "   try:\n",
    "      successful_cleaner_output:bool = entryObj.cleaner() #by default cleaner removes leading whitespace and merges adjacent runs with identical format features\n",
    "      # print('sucessful cleaner')\n",
    "      if not successful_cleaner_output:\n",
    "         reject_ind_list.append(i)\n",
    "         print(f'para# {i} IS ONLY whitespace. Need to drop it. #TODO')\n",
    "   except:\n",
    "      some_error_ind_list.append(f\"cleaner error on p: {i}. Text is {entryObj.interogate__para_text()}\")\n",
    "      print('error on cleaner')\n",
    "   try:\n",
    "      root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "      sub_root_beginnings = '-+('\n",
    "      permissive_root_contents = list(chain(up_alph_chars,root_note_chars,string.digits))\n",
    "      root_expression = (VerEx().\n",
    "                     start_of_line().\n",
    "                     any(up_alph_chars).add('+').\n",
    "                     any(permissive_root_contents)\n",
    "                     )\n",
    "      subroot_expression = (VerEx().\n",
    "                        start_of_line().\n",
    "                        any(sub_root_beginnings).\n",
    "                        any(permissive_root_contents)\n",
    "                        )\n",
    "      featureConfig = {\n",
    "      'root': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               'text_regex_at_feature': root_expression.compile()\n",
    "               },\n",
    "      'subroot': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               'text_regex_at_feature': subroot_expression.compile()\n",
    "               },\n",
    "      'lemma': {'docxFeature': 'run_bold',\n",
    "               'strSummary':'fontBold', \n",
    "               'value':True},\n",
    "      }\n",
    "      # return True, (value_mask, values_from_runs), (regex_mask, regex_matches)\n",
    "      is_root, _, _ = entryObj.single_run_feature_identify(featureConfig['root'])\n",
    "      if is_root:\n",
    "         print('\\n\\nroot at para number: ',i)\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t',paraText)\n",
    "         root_ind_list.append(i)\n",
    "\n",
    "      # return True, (value_mask, values_from_runs), (regex_mask, regex_matches)\n",
    "      is_subroot, _, _ = entryObj.single_run_feature_identify(featureConfig['subroot'])\n",
    "      if is_subroot:\n",
    "         print('\\n\\nsubroot at para number: ',i)\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t',paraText)\n",
    "         subroot_ind_list.append(i)\n",
    "\n",
    "      # return True, (value_mask, values_from_runs), (regex_mask, regex_matches)\n",
    "      is_lemma, _, _ = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      if is_lemma:\n",
    "         # entryObj.interogate__para_text()\n",
    "         paraText = entryObj.interogate__para_text()\n",
    "         print('\\t\\tp#',i,'\\t\\t',paraText)\n",
    "         lemma_ind_list.append(i)\n",
    "      if is_lemma and (is_root or is_subroot):\n",
    "         print(f'this para# {i} has BOTH lemma AND root')\n",
    "         root_and_lemma_one_line.append(i)\n",
    "\n",
    "   except BaseException as e:\n",
    "      raise e\n",
    "      some_error_ind_list.append(i)\n",
    "      # if not e.args[0][0].exc.args[0] == 'suppressed Validation Error':\n",
    "         # print('\\npara number: ',i)\n",
    "\n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n",
    "\n",
    "print('roots: ',len(root_ind_list))\n",
    "print('subroots: ',len(subroot_ind_list))\n",
    "print('lemmas: ',len(lemma_ind_list))\n",
    "print('root_and_lemma_one_line: ',len(root_and_lemma_one_line))\n",
    "print('additional cleaner rejects: ',len(reject_ind_list))\n",
    "print('additional error rejects: ',len(some_error_ind_list))\n",
    "\n",
    "print('num entities: ',len(root_ind_list) + len(lemma_ind_list) + len(subroot_ind_list))\n",
    "num_good_paras_of_other_content= len(root_ind_list) + len(lemma_ind_list) - len(root_and_lemma_one_line) - len(subroot_ind_list)\\\n",
    "                                    + len(reject_ind_list) + len(some_error_ind_list)\n",
    "print('num_good_paras_of_other_content: ',num_good_paras_of_other_content)\n",
    "# # Test messages\n",
    "logger.debug(\"logger debug test\")\n",
    "logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from __future__ import annotations\n",
    "# from typing import ForwardRef\n",
    "# Fula_Entry = ForwardRef('Fula_Entry')\n",
    "\n",
    "class Fula_Entry (BaseModel): \n",
    "   entity_word: List[str] #root, subroot, lemma\n",
    "   features: Optional[Dict[str,str]] = {} #contains features for this entity, ie: txt file features like location, POS, etc. Only applicable directly. Lemmas have POS, roots do not, etc\n",
    "   paragraphs_list: Dict[int,Any] #para enumeration, docx para obj\n",
    "   paragraphs_extr : List[Docx_Paragraph_and_Runs] #class defined above\n",
    "   sub_roots : List['Fula_Entry'] = [] #self reference\n",
    "   lemmas : List['Fula_Entry'] = [] #self reference\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "# root = root_ind_list[0]\n",
    "# lemma = lemma_ind_list[0]\n",
    "# test_entry = Fula_Entry()\n",
    "# print(len(root_ind_list),'\\t',root_ind_list)\n",
    "# print(lemma_ind_list)\n",
    "# print(lemma_ind_list[8:])\n",
    "# print(len(list(pairwise(root_ind_list))),'\\t',list(pairwise(root_ind_list)))\n",
    "\n",
    "\n",
    "\n",
    "def closest(ranger, target): #any target indeces occuring before the first ranger index will be ignored\n",
    "   if not isinstance(target,np.ndarray):\n",
    "      target = np.array(target)\n",
    "   for a,b in ranger:\n",
    "      begin = np.searchsorted(target,a)\n",
    "      end = np.searchsorted(target,b)\n",
    "      _, out, target = np.split(target, [begin,end])\n",
    "      yield list(out)\n",
    "   yield list(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_paras = list(range(len(parsed_object_list)))\n",
    "normal_para = [x for x in all_paras if x not in root_ind_list and x not in lemma_ind_list]\n",
    "\n",
    "root_aligned_lemmas = list(closest(pairwise(root_ind_list), lemma_ind_list))\n",
    "lemma_aligned_paras = list(closest(pairwise(lemma_ind_list),normal_para))\n",
    "num_schema = {}\n",
    "for i, r in enumerate(root_ind_list):\n",
    "   # print('\\n\\n',r)\n",
    "   num_schema[r] = {}\n",
    "   for j, lemma in enumerate(root_aligned_lemmas[i]):\n",
    "      num_schema[r][int(lemma)] = []\n",
    "      # print('\\n\\t',lemma)\n",
    "      for k, parag in enumerate(lemma_aligned_paras[j]):\n",
    "         # print('\\t\\t',parag)\n",
    "         num_schema[r][lemma].append(int(parag))\n",
    "print(json.dumps(num_schema, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      # p_text = entryObj.interogate__para_text()\n",
    "      # if not set(p_text).isdisjoint(low_freq_odd_chars):\n",
    "      #    msg = 'rare_characters\\t\\t'+p_text\n",
    "      #    entryObj.paragraph_logger(level=40,msg = msg, print_bool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validate Whitespace behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '\\xa0', '\\t']\n"
     ]
    }
   ],
   "source": [
    "white_space_chars = [k for k in char_counts if len(k.strip()) == 0]\n",
    "print(white_space_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validate Upper/lower\n",
    "conclusion: using str.upper()/lower() functions is safe. No character in the dataset causes an error when used in those functions, and the only characters that don't cooperate to a new case are non-alphabetical characters such as numbers and punctuation. \n",
    "conclusion: using str.upper()==str.lower() is a viable way to check if a character is alphabetical or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "upper_chars:      ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ñ', 'Ŋ', 'Ɓ', 'Ɗ', 'Ƴ']\n",
      "\n",
      "lower_chars:      ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'ç', 'è', 'é', 'ê', 'î', 'ï', 'ñ', 'ò', 'ô', 'ù', 'û', 'ŋ', 'ƴ', 'ɓ', 'ɗ']\n",
      "\n",
      "non_castable:     ['\\t', ' ', '!', '\"', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '[', ']', '`', '\\xa0']\n",
      "\n",
      "error_casting:    []\n",
      "\n",
      "silent fails:     []\n",
      "\n",
      "upperWITHlowerChars:     [('A', 'a'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('E', 'e'), ('F', 'f'), ('G', 'g'), ('H', 'h'), ('I', 'i'), ('J', 'j'), ('K', 'k'), ('L', 'l'), ('M', 'm'), ('N', 'n'), ('O', 'o'), ('P', 'p'), ('Q', 'q'), ('R', 'r'), ('S', 's'), ('T', 't'), ('U', 'u'), ('V', 'v'), ('W', 'w'), ('X', 'x'), ('Y', 'y'), ('Z', 'z'), ('À', 'à'), ('Â', 'â'), ('Ç', 'ç'), ('È', 'è'), ('É', 'é'), ('Ê', 'ê'), ('Î', 'î'), ('Ï', 'ï'), ('Ñ', 'ñ'), ('Ò', 'ò'), ('Ô', 'ô'), ('Ù', 'ù'), ('Û', 'û'), ('Ŋ', 'ŋ'), ('Ɓ', 'ɓ'), ('Ɗ', 'ɗ'), ('Ƴ', 'ƴ')]\n",
      "\n",
      "found_as_one_case_only:       [(None, 'à'), (None, 'â'), (None, 'ç'), (None, 'è'), (None, 'é'), (None, 'ê'), (None, 'î'), (None, 'ï'), (None, 'ò'), (None, 'ô'), (None, 'ù'), (None, 'û')]\n"
     ]
    }
   ],
   "source": [
    "upperWITHlowerChars = set()\n",
    "upper_chars = []\n",
    "lower_chars = []\n",
    "non_castable = []\n",
    "error_casting = []\n",
    "nons = []\n",
    "found_as_one_case_only = []\n",
    "for k in char_counts:\n",
    "   try:\n",
    "      up = k.upper()\n",
    "      low = k.lower()\n",
    "      upperWITHlowerChars.add((up,low))\n",
    "      if up == low:\n",
    "         non_castable.append(k)\n",
    "         upperWITHlowerChars.remove((up,low))\n",
    "      elif k == up:\n",
    "         upper_chars.append(up)\n",
    "      elif k == low:\n",
    "         lower_chars.append(low)\n",
    "      else:\n",
    "         nons.append(k)\n",
    "   except:\n",
    "      error_casting.append(k)\n",
    "print('\\nupper_chars:     ',sorted(upper_chars))\n",
    "print('\\nlower_chars:     ',sorted(lower_chars))\n",
    "print('\\nnon_castable:    ',sorted(non_castable))\n",
    "print('\\nerror_casting:   ',sorted(error_casting))\n",
    "print('\\nsilent fails:    ',sorted(nons))\n",
    "\n",
    "print('\\nupperWITHlowerChars:    ',sorted(upperWITHlowerChars))\n",
    "for u,l in upperWITHlowerChars:\n",
    "   pair = [u,l]\n",
    "   unseen_possible_case = False\n",
    "   if l not in lower_chars and l not in non_castable:\n",
    "      pair[1] = None\n",
    "      unseen_possible_case = True\n",
    "   if u not in upper_chars and u not in non_castable:\n",
    "      pair[0] = None\n",
    "      unseen_possible_case = True\n",
    "   if unseen_possible_case:\n",
    "      found_as_one_case_only.append(tuple(pair))\n",
    "      # print(\"upper possible, but not present:     \",u)\n",
    "print('\\nfound_as_one_case_only:      ', sorted(found_as_one_case_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'U+0041', 'a', 'U+0061'),\n",
       " ('B', 'U+0042', 'b', 'U+0062'),\n",
       " ('C', 'U+0043', 'c', 'U+0063'),\n",
       " ('D', 'U+0044', 'd', 'U+0064'),\n",
       " ('E', 'U+0045', 'e', 'U+0065'),\n",
       " ('F', 'U+0046', 'f', 'U+0066'),\n",
       " ('G', 'U+0047', 'g', 'U+0067'),\n",
       " ('H', 'U+0048', 'h', 'U+0068'),\n",
       " ('I', 'U+0049', 'i', 'U+0069'),\n",
       " ('J', 'U+004A', 'j', 'U+006A'),\n",
       " ('K', 'U+004B', 'k', 'U+006B'),\n",
       " ('L', 'U+004C', 'l', 'U+006C'),\n",
       " ('M', 'U+004D', 'm', 'U+006D'),\n",
       " ('N', 'U+004E', 'n', 'U+006E'),\n",
       " ('O', 'U+004F', 'o', 'U+006F'),\n",
       " ('P', 'U+0050', 'p', 'U+0070'),\n",
       " ('Q', 'U+0051', 'q', 'U+0071'),\n",
       " ('R', 'U+0052', 'r', 'U+0072'),\n",
       " ('S', 'U+0053', 's', 'U+0073'),\n",
       " ('T', 'U+0054', 't', 'U+0074'),\n",
       " ('U', 'U+0055', 'u', 'U+0075'),\n",
       " ('V', 'U+0056', 'v', 'U+0076'),\n",
       " ('W', 'U+0057', 'w', 'U+0077'),\n",
       " ('X', 'U+0058', 'x', 'U+0078'),\n",
       " ('Y', 'U+0059', 'y', 'U+0079'),\n",
       " ('Z', 'U+005A', 'z', 'U+007A'),\n",
       " ('À', 'U+00C0', 'à', 'U+00E0'),\n",
       " ('Â', 'U+00C2', 'â', 'U+00E2'),\n",
       " ('Ç', 'U+00C7', 'ç', 'U+00E7'),\n",
       " ('È', 'U+00C8', 'è', 'U+00E8'),\n",
       " ('É', 'U+00C9', 'é', 'U+00E9'),\n",
       " ('Ê', 'U+00CA', 'ê', 'U+00EA'),\n",
       " ('Î', 'U+00CE', 'î', 'U+00EE'),\n",
       " ('Ï', 'U+00CF', 'ï', 'U+00EF'),\n",
       " ('Ñ', 'U+00D1', 'ñ', 'U+00F1'),\n",
       " ('Ò', 'U+00D2', 'ò', 'U+00F2'),\n",
       " ('Ô', 'U+00D4', 'ô', 'U+00F4'),\n",
       " ('Ù', 'U+00D9', 'ù', 'U+00F9'),\n",
       " ('Û', 'U+00DB', 'û', 'U+00FB'),\n",
       " ('Ŋ', 'U+014A', 'ŋ', 'U+014B'),\n",
       " ('Ɓ', 'U+0181', 'ɓ', 'U+0253'),\n",
       " ('Ɗ', 'U+018A', 'ɗ', 'U+0257'),\n",
       " ('Ƴ', 'U+01B3', 'ƴ', 'U+01B4')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def code_point(c):\n",
    "   return \"U+{:04X}\".format(ord(c))\n",
    "[(c,code_point(c),d,code_point(d)) for c,d in sorted(upperWITHlowerChars)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\t', 'U+0009'),\n",
       " (' ', 'U+0020'),\n",
       " ('!', 'U+0021'),\n",
       " ('\"', 'U+0022'),\n",
       " ('&', 'U+0026'),\n",
       " (\"'\", 'U+0027'),\n",
       " ('(', 'U+0028'),\n",
       " (')', 'U+0029'),\n",
       " ('+', 'U+002B'),\n",
       " (',', 'U+002C'),\n",
       " ('-', 'U+002D'),\n",
       " ('.', 'U+002E'),\n",
       " ('/', 'U+002F'),\n",
       " ('0', 'U+0030'),\n",
       " ('1', 'U+0031'),\n",
       " ('2', 'U+0032'),\n",
       " ('3', 'U+0033'),\n",
       " ('4', 'U+0034'),\n",
       " ('5', 'U+0035'),\n",
       " ('6', 'U+0036'),\n",
       " ('7', 'U+0037'),\n",
       " ('8', 'U+0038'),\n",
       " ('9', 'U+0039'),\n",
       " (':', 'U+003A'),\n",
       " (';', 'U+003B'),\n",
       " ('<', 'U+003C'),\n",
       " ('=', 'U+003D'),\n",
       " ('>', 'U+003E'),\n",
       " ('?', 'U+003F'),\n",
       " ('[', 'U+005B'),\n",
       " (']', 'U+005D'),\n",
       " ('`', 'U+0060'),\n",
       " ('\\xa0', 'U+00A0')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(c,code_point(c)) for c in sorted(non_castable)]\n",
    "\n",
    "# ('\\t', 'U+0009' -> ('`', 'U+0060')\n",
    "#  ('\\xa0', 'U+00A0'))\n",
    "# ('A', 'U+0041' -> 'û', 'U+00FB')\n",
    "# ('Ŋ', 'U+014A' -> 'ƴ', 'U+01B4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validate Regex Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "impossible_char = '\\u0008' #utf backspace (\\u0008) is unlikely to appear in a docx, and did not appear in this one.\n",
    "s = impossible_char.join(char_counts.keys())\n",
    "re_results = [False]*len(char_counts.keys())\n",
    "for i, k in enumerate(char_counts):\n",
    "   pattern = re.escape(k)\n",
    "   # print(s)\n",
    "   try:\n",
    "      m = re.search(pattern,s) #type: ignore\n",
    "      corrected_ind = m.start()/2\n",
    "      # print(corrected_ind)\n",
    "   except: print('exception: ',repr(i))\n",
    "   # print(corrected_ind)\n",
    "   if i == corrected_ind:\n",
    "      re_results[i] = True\n",
    "   else: print('failure: ',repr(i))\n",
    "print(all(re_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_chars = [x for x in char_counts.keys() if x.upper() != x.lower()]\n",
    "stralpha = [x for x in alpha_chars if x.isalpha()]\n",
    "assert stralpha == alpha_chars, 'note that str.isalpha does NOT work safely here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#these frequencies were copied from a previous run, and only from successfully parsed objects\n",
    "#the lowest frequencies were reviewed and selections pulled from those\n",
    "   # low_freq_odd_chars = ('\\t', 72), ('5', 67), ('`', 64), ('&', 49), ('ù', 30), ('ï', 26), ('X', 25), ('!', 15), ('\"', 14), ('ò', 8), ('=', 4), ('Q', 4), ('\\xa0', 1)\n",
    "   # low_freq_odd_chars = [x[0] for x in low_freq_odd_chars]\n",
    "#numbers do not appear to be used outside of scholarly references and some multiple-root instances\n",
    "   # nums = list(range(10))\n",
    "#X for example, is almost only in english or french glosses, or scholarly references)\n",
    "   #('X', 25),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning notes\n",
    "# `new Kunari' - region in western Niger ; `nouveau Kounari' - région dans l'ouest du Niger\n",
    "   #here the ` seems to be used at the beginning of a quotation, and a normal apostrophe at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # char_counts\n",
    "# sorted_char_val = sorted(char_counts.items(), key=lambda item: (-item[1], item[0]))\n",
    "# print(sorted_char_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inconsistencies\n",
    "\n",
    "leading white spaces\n",
    "entries with root and lemma on one line\n",
    "\n",
    "\"errors\"\n",
    "   whitespace paras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
