{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Header and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Draft or create a class system to represent the pular entries. \n",
    "Ideally this will contain a way to nest entry objects under a root\n",
    "'''\n",
    "#%pip install docx\n",
    "#%pip install python-docx #this mutates docx? \n",
    "#%pip install pydantic\n",
    "#%pip install mypy\n",
    "# %pip install numpy\n",
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "from pydantic import BaseModel, ValidationError, validator, root_validator, Field, constr\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import compress, tee, chain\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "from verbalexpressions import VerEx\n",
    "import pickle\n",
    "\n",
    "import docx\n",
    "from docx import Document\n",
    "from pydantic_docx import Docx_Paragraph_and_Runs #type:ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "logger_filename = f\"logs_and_outputs/{current_time}docxFileParse.log\"\n",
    "\n",
    "handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "\n",
    "# logging.setLogRecordFactory(factory)\n",
    "logging.basicConfig(handlers=[handler], level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docx Pydantic Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##THIS IS NOW A .PY SCRIPT##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DOCX File and Applying Pydantic Class Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total paras:  32507\n",
      "parsed paras:  32040\n",
      "handled errors:  467\n",
      "failed paras:  0\n"
     ]
    }
   ],
   "source": [
    "# # %%capture cap --no-stderr\n",
    "\n",
    "# #Run docx module to parse the docx file\n",
    "docx_filename = \"Fula_Dictionary-repaired.docx\"\n",
    "# docx_filename = \"pasted_docx page 1.docx\"\n",
    "document = Document(docx_filename)\n",
    "\n",
    "\n",
    "\n",
    "char_counts = Counter()\n",
    "\n",
    "docx_object_list = []\n",
    "parsed_object_list = []\n",
    "failed_paras_ind = []\n",
    "handled_errors = []\n",
    "\n",
    "for i, para in enumerate(document.paragraphs):\n",
    "   docx_object_list.append((i,para))\n",
    "   try:\n",
    "      entryObj = Docx_Paragraph_and_Runs(**{'paragraph': para, 'paragraph_enumeration': i})\n",
    "      char_counts.update(entryObj.interogate__para_text())\n",
    "      parsed_object_list.append((i,entryObj))\n",
    "   except ValidationError as e:\n",
    "      suppress = {\n",
    "            # 'type': ['value_error.any_str.min_length' #ignore zero length run_text, per run validator\n",
    "            #          ],\n",
    "            'msg': ['suppressed Validation Error'] #ignore suppressed errors earlier/lower in the stack      \n",
    "      }\n",
    "      for err in e.errors():\n",
    "         if err['msg'] in suppress['msg']:\n",
    "            handled_errors.append((i,para))\n",
    "            pass\n",
    "   except BaseException as e:\n",
    "      print(e)\n",
    "      raise\n",
    "      failed_paras_ind.append((i,para))\n",
    "      \n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Over Class Objects to extract Dictionary Specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Prep / notes / logger \"experiment\" input request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment time: 2022-07-29_-_08-15-57\n",
      "Experiment note: trying this again with more pieces disabled\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #get current datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "\n",
    "# #create file to save prints (use with jupyter magic enabled at the top of this cell: %%capture cap --no-stderr)\n",
    "output_name = f\"logs_and_outputs/{current_time}_objList_processing_Output.txt\"\n",
    "experiment = input(\"Enter emperiment description:\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "\n",
    "# logger_filename = f\"logs_and_outputs/{current_time}objList_processing.log\"\n",
    "\n",
    "# # Creating an object\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Setting the threshold of logger to DEBUG, etc\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# #add encoding\n",
    "# handler = logging.FileHandler(logger_filename, 'w', 'utf-8') \n",
    "# handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "# logger.addHandler(handler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 2022-07-22 07:20:18,751 paragraph#2731 with text \"\"   -IR-\"\" had leading whitespace removed\n",
    "# # # parsed_object_list = parsed_object_list[2731:3000]\n",
    "# # # def any(self, value):\n",
    "# # #    return self.add(\"([%s])\" % value)\n",
    "# # # def add(self, value):\n",
    "# # #    if isinstance(value, list):\n",
    "# # #       self.s.extend(value)\n",
    "# # #    else:\n",
    "# # #       self.s.append(value)\n",
    "# # #    return self\n",
    "# # low_alph_chars = ''.join([x.lower() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "# # up_alph_chars = ''.join([x.upper() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "# # root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "# # sub_root_beginnings = '-+('\n",
    "# # permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "# # pattern = '^['+re.escape(sub_root_beginnings)+'][^'+low_alph_chars+']+'\n",
    "# # print(pattern)\n",
    "# # print(re.search(pattern = pattern, string = '-KLHFGSLK JEs'))\n",
    "# # pattern = '^['+re.escape(sub_root_beginnings)+']['+re.escape(permissive_root_contents)+']+'\n",
    "# # print(pattern)\n",
    "# # print(re.search(pattern = pattern, string = 'KLHFGSLK JEs'))\n",
    "# noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "# nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "# print(nounPatternRegex)\n",
    "# s = 'n. dfhjkgqh (wsh) askldjhf / asdfhjkkh'\n",
    "# print(re.findall(nounPatternRegex,s))\n",
    "# print(nounPatternRegex.findall(s))\n",
    "\n",
    "# # if /, the next word is a plural. The plural also then usually has a (el) class after it. commas are multiple version\n",
    "# # so each lemma noun may have a (class). The lemma may have one or multiple plurals, which will have a modified (class)\n",
    "# # so have a nouns list? nouns may then be \n",
    "# # [('n.',{bool}), #only needed if this gets added to all POS? Or maybe theres information in its use?\n",
    "# # ([[{word}],{Optional[class]}]),  #singular\n",
    "#    #([[{word}],{Optional[class]}])]   #plural\n",
    "\n",
    "# # so nounStruct[0][1] is the flag for noun?\n",
    "#    # one POS table, and noun will get added there. But the presence of a noun indicates to look here\n",
    "# # classes may be found [sClass[1] for sClass in nounStruct[1]] will give all classes of singular, [sClass[1] for sClass in nounStruct[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration and Extraction - Dictionary Specific Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1010491370.py, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22604\\1010491370.py\"\u001b[1;36m, line \u001b[1;32m91\u001b[0m\n\u001b[1;33m    lemma_lookup[i] = (ltext lemma_mask)\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%capture cap \n",
    "#--no-stderr\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")\n",
    "para_text_lookup = {}\n",
    "root_ind_list = []\n",
    "subroot_ind_list = []\n",
    "lemma_ind_list = []\n",
    "some_error_ind_list = []\n",
    "reject_ind_list = []\n",
    "root_and_lemma_one_line = []\n",
    "root_lookup = {}\n",
    "lemma_lookup = {}\n",
    "pos_lookup = {}\n",
    "pos_ind_list = []\n",
    "\n",
    "up_alph_chars = [x.upper() for x in char_counts.keys() if x.upper() != x.lower()] #only uppercase alphabetical chars\n",
    "\n",
    "for i, entryObj in parsed_object_list:\n",
    "\n",
    "   try:\n",
    "      para_text_lookup[i] = entryObj.para_text\n",
    "      successful_cleaner_output:bool = entryObj.cleaner() #by default cleaner removes leading whitespace and merges adjacent runs with identical format features\n",
    "      # print('sucessful cleaner')\n",
    "      if not successful_cleaner_output:\n",
    "         reject_ind_list.append(i)\n",
    "         print(f'para# {i} IS ONLY whitespace. Need to drop it. #TODO')\n",
    "   except:\n",
    "      some_error_ind_list.append(f\"cleaner error on p: {i}. Text is {entryObj.interogate__para_text()}\")\n",
    "      print('error on cleaner')\n",
    "      raise\n",
    "   try:\n",
    "      root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "      sub_root_beginnings = '-+('\n",
    "      permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "\n",
    "      featureConfig = {\n",
    "      'root': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               # 'text_regex_at_feature': root_expression.compile()\n",
    "               },\n",
    "      'subroot': {'docxFeature': 'run_font_size_pt',\n",
    "               'strSummary':'fontSize_12.0', \n",
    "               'value':12.0,\n",
    "               # 'text_regex_at_feature': subroot_expression.compile()\n",
    "               },\n",
    "      'lemma': {'docxFeature': 'run_bold',\n",
    "               'strSummary':'fontBold', \n",
    "               'value':True},\n",
    "      'lemmaPOS': {'docxFeature': 'run_italic',\n",
    "               'strSummary':'fontItalic', \n",
    "               'value':True},\n",
    "      }\n",
    "      is_subroot = False\n",
    "      # return True, (value_mask, run_text), (regex_mask, regex_matches)\n",
    "      is_root, (root_mask,run_text), _ = entryObj.single_run_feature_identify(featureConfig['root'])\n",
    "      if is_root:\n",
    "         rtext = ''.join(chain(compress(run_text,root_mask))).strip()\n",
    "         root_lookup[i] = (rtext,root_mask)\n",
    "         #routine to distinguish between main root and subroot\n",
    "         for j, r in enumerate(compress(run_text,root_mask)): \n",
    "            if j==0:\n",
    "               # low_alph_chars = ''.join([x.lower() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "               up_alph_chars = ''.join([x.upper() for x in char_counts.keys() if x.upper() != x.lower()]) #only uppercase alphabetical chars\n",
    "               root_note_chars = '-+()? ' #characters that encode the author's notes\n",
    "               sub_root_beginnings = '-+('\n",
    "               permissive_root_contents = ''.join(list(chain(up_alph_chars,root_note_chars,string.digits)))\n",
    "               pattern = '^['+re.escape(sub_root_beginnings)+']['+re.escape(permissive_root_contents)+']+'\n",
    "               m = re.search(pattern = pattern, string = r)\n",
    "               if m is not None:\n",
    "                  is_subroot = True\n",
    "         \n",
    "         if is_subroot:\n",
    "            print('\\n\\nsubroot at para number: ',i)\n",
    "            paraText = entryObj.interogate__para_text()\n",
    "            print('\\t',paraText)\n",
    "            subroot_ind_list.append(i)\n",
    "         else:\n",
    "            print('\\n\\nroot at para number: ',i)\n",
    "            paraText = entryObj.interogate__para_text()\n",
    "            print('\\t',paraText)\n",
    "            root_ind_list.append(i)\n",
    "      # return True, (value_mask, run_text), (regex_mask, regex_matches)\n",
    "      is_lemma, (lemma_mask, run_text), _ = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      if is_lemma:\n",
    "         # print(lemma_mask,run_text)\n",
    "         ltext = ''.join(chain(compress(run_text,lemma_mask))).strip()\n",
    "         # print(ltext)\n",
    "         lemma_lookup[i] = (ltext, lemma_mask)\n",
    "         # paraText = entryObj.interogate__para_text()\n",
    "         # print('\\t\\tp#',i,'\\t\\t',paraText)\n",
    "         lemma_ind_list.append(i)\n",
    "      # is_pos, (pos_mask, run_text), _ = entryObj.single_run_feature_identify(featureConfig['lemma'])\n",
    "      # if is_pos:\n",
    "      #    postext = ''.join(chain(compress(run_text,pos_mask))).strip()\n",
    "      #    # noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "      #    # nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "      #    pos_lookup[i] = (postext,pos_mask)\n",
    "      #    pos_ind_list.append(i)\n",
    "      #    if is_pos and not is_lemma:\n",
    "      #       entryObj.paragraph_logger(level = 10,msg = f'Unexpected structure:: para#{i} has a POS: {postext} but this para does NOT have a lemma. last lemma at para#{lemma_ind_list[-1]} was \"{lemma_lookup[lemma_ind_list[-1]]}\"',print_bool = True)\n",
    "      # if is_pos and (is_root or is_subroot):\n",
    "      if is_lemma and is_root:\n",
    "         print(f'this para# {i} has BOTH lemma AND root')\n",
    "         root_and_lemma_one_line.append(i)\n",
    "\n",
    "   except BaseException as e:\n",
    "      \n",
    "      some_error_ind_list.append(i)\n",
    "      raise e\n",
    "\n",
    "print('total paras: ',len(docx_object_list))\n",
    "print('parsed paras: ',len(parsed_object_list))\n",
    "print('handled errors: ',len(handled_errors))\n",
    "print('failed paras: ',len(failed_paras_ind))\n",
    "\n",
    "assert len(docx_object_list) == len(parsed_object_list) + len(handled_errors) + len(failed_paras_ind)\n",
    "\n",
    "print('roots: ',len(root_ind_list))\n",
    "print('subroots: ',len(subroot_ind_list))\n",
    "print('lemmas: ',len(lemma_ind_list))\n",
    "print('root_and_lemma_one_line: ',len(root_and_lemma_one_line))\n",
    "print('additional cleaner rejects: ',len(reject_ind_list))\n",
    "print('additional error rejects: ',len(some_error_ind_list))\n",
    "\n",
    "print('num entities: ',len(root_ind_list) + len(lemma_ind_list) + len(subroot_ind_list))\n",
    "num_good_paras_of_other_content= len(root_ind_list) + len(lemma_ind_list) - len(root_and_lemma_one_line) + len(subroot_ind_list)\\\n",
    "                                    + len(reject_ind_list) + len(some_error_ind_list)\n",
    "print('num_good_paras_of_other_content: ',num_good_paras_of_other_content)\n",
    "# # Test messages\n",
    "logger.debug(\"logger debug test\")\n",
    "logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)\n",
    "\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open('parsed_objectClass_outcomes_dict.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    outcomes_dict = {}\n",
    "    outcomes_dict['parsed_object_list'] = parsed_object_list\n",
    "    outcomes_dict['para_text_lookup'] = para_text_lookup\n",
    "    outcomes_dict['root_ind_list'] = root_ind_list\n",
    "    outcomes_dict['subroot_ind_list'] = subroot_ind_list\n",
    "    outcomes_dict['lemma_ind_list'] = lemma_ind_list\n",
    "    outcomes_dict['root_and_lemma_one_line'] = root_and_lemma_one_line\n",
    "    outcomes_dict['root_lookup'] = root_lookup\n",
    "    outcomes_dict['lemma_lookup'] = lemma_lookup\n",
    "    # outcomes_dict['pos_lookup'] = pos_lookup\n",
    "    outcomes_dict['char_counts'] = char_counts\n",
    "    pickle.dump(outcomes_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
