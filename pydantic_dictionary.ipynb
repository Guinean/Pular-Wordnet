{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Create an easy/safe API to parse/hold/yield constructions of pydantic docx class objects.\n",
    "In theory this can be generalized and abstracted since the classes are largely absracted. \n",
    "However since this is API based, I will just focus on solutions for THIS dictionary task. These can be expanded later, or just have other APIs added\n",
    "Final objective is have a tool to pass complex commands/instructions, and have it yield a pandas dataframe of the results.\n",
    "Intermediate steps may be yield a list of lists\n",
    "I am not yet sure if there is benefit trying to have the whole dictionary in one notional object. \n",
    "I don't think so, since each operation is psuedo atomic, and any aggregate actions are intended to be done in something like pandas.\n",
    "Eventually I would want a loop for pandas operations to easily feed updates to the data. \n",
    "'''\n",
    "conclusion = '''create a pular dictionary specifc datastructure to handle the aggregate paragraphs. \n",
    "This will only be per lemma, \n",
    "as any aggregations from roots and relations of that sort can be managed in pandas and created by simple inheretance/reference\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "import pydantic\n",
    "from pydantic import ValidationError, validator, root_validator, Field, constr\n",
    "from pydantic_docx import Docx_Paragraph_and_Runs #type:ignore\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "   \n",
    "   'txt_FulaLemmas',\n",
    "   'txt_FulaRoots',\n",
    "   \n",
    "   #extra\n",
    "   'txt_FulaAnnotations',\n",
    "   #gloss derivatives\n",
    "   'txt_FulaSenseEnglish',\n",
    "   'txt_FulaSenseFrench',\n",
    "   'txt_FulaSenseClassifications',\n",
    "   'txt_FulaSenseEnglishAnnotations',\n",
    "   'txt_FulaSenseFrenchAnnotations',\n",
    "\n",
    "   #lemma derivative\n",
    "   'txt_FulaDialects',\n",
    "   'txt_FulaPOSTags',\n",
    "   'txt_FulaSynonyms',\n",
    "\n",
    "\n",
    "   #agg\n",
    "   'txt_FulaInParenthesis',\n",
    "   'txt_POS',\n",
    "   'txt_RootOrigins'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('n\\\\.|(?<=\\\\()[^\\\\)]+|\\\\/')\n",
      "['n.', 'wsh', '/']\n",
      "['n.', 'wsh', '/']\n"
     ]
    }
   ],
   "source": [
    "noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "print(nounPatternRegex)\n",
    "s = 'n. dfhjkgqh (wsh) askldjhf / asdfhjkkh'\n",
    "print(re.findall(nounPatternRegex,s))\n",
    "print(nounPatternRegex.findall(s))\n",
    "\n",
    "# if /, the next word is a plural. The plural also then usually has a (el) class after it. commas are multiple version\n",
    "# so each lemma noun may have a (class). The lemma may have one or multiple plurals, which will have a modified (class)\n",
    "# so have a nouns list? nouns may then be \n",
    "# [('n.',{bool}), #only needed if this gets added to all POS? Or maybe theres information in its use?\n",
    "# ([[{word}],{Optional[class]}]),  #singular\n",
    "   #([[{word}],{Optional[class]}])]   #plural\n",
    "\n",
    "# so nounStruct[0][1] is the flag for noun?\n",
    "   # one POS table, and noun will get added there. But the presence of a noun indicates to look here\n",
    "# classes may be found [sClass[1] for sClass in nounStruct[1]] will give all classes of singular, [sClass[1] for sClass in nounStruct[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAssert(dct,val,typ:type) -> Any:\n",
    "   out = dct.get(val,False)\n",
    "   assert isinstance(out,typ), f\"the provided dict {dct} did not give {val} with the expected type: {typ}\"\n",
    "   return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irregularities []\n",
      "paragraphs [Docx_Paragraph_and_Runs(para_text='your (sg.) (only with certain nouns such as those which refer to close family members)', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt'], run_bold=[None], run_font_size_pt=[None], paragraph_enumeration=9, run_italic=[None], run_text=['your (sg.) (only with certain nouns such as those which refer to close family members)'], para_left_indent=174193200), Docx_Paragraph_and_Runs(para_text='ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt'], run_bold=[None], run_font_size_pt=[None], paragraph_enumeration=10, run_italic=[None], run_text=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'], para_left_indent=174193200), Docx_Paragraph_and_Runs(para_text='aan  prn,ind  DFZH  [an]:Z<->', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt'], run_bold=[True, None, None], run_font_size_pt=[None, None, 8.0], paragraph_enumeration=11, run_italic=[None, True, None], run_text=['aan  ', 'prn,ind  ', 'DFZH  [an]:Z<->'], para_left_indent=116128800)]\n",
      "englishGloss para_text='ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)' para_first_line_indent=-91440 run_font_name=['TmsRmn 10pt'] run_bold=[None] run_font_size_pt=[None] paragraph_enumeration=10 run_italic=[None] run_text=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'] para_left_indent=174193200\n",
      "frenchGloss para_text='aan  prn,ind  DFZH  [an]:Z<->' para_first_line_indent=-91440 run_font_name=['TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt'] run_bold=[True, None, None] run_font_size_pt=[None, None, 8.0] paragraph_enumeration=11 run_italic=[None, True, None] run_text=['aan  ', 'prn,ind  ', 'DFZH  [an]:Z<->'] para_left_indent=116128800\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for dict_entry\nenglishGloss\n  str type expected (type=type_error.str)\nenglishGloss\n  value could not be parsed to a boolean (type=type_error.bool)\nfrenchGloss\n  str type expected (type=type_error.str)\nfrenchGloss\n  value could not be parsed to a boolean (type=type_error.bool)\nFulaAnnotations\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22964\\591775008.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m \u001b[0mdict_entry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'paragraphs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparsed_object_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\peter\\VSC-Workspace_PULAR\\.pular_venv\\lib\\site-packages\\pydantic\\main.pyd\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\peter\\VSC-Workspace_PULAR\\.pular_venv\\lib\\site-packages\\pydantic\\main.pyd\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 5 validation errors for dict_entry\nenglishGloss\n  str type expected (type=type_error.str)\nenglishGloss\n  value could not be parsed to a boolean (type=type_error.bool)\nfrenchGloss\n  str type expected (type=type_error.str)\nfrenchGloss\n  value could not be parsed to a boolean (type=type_error.bool)\nFulaAnnotations\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class dict_entry(pydantic.BaseModel):\n",
    "   irregularities: List[str] = []\n",
    "   paragraphs: List[Docx_Paragraph_and_Runs] = Field(...,min_items = 1)\n",
    "   # lemma: str = Field(...,min_length = 1) #article\n",
    "   # root: List[Tuple[int,str,str]] #index,root_text, root_meta_data #article\n",
    "   root_origin: Union[str,bool] = False\n",
    "   #Paragraph Derivatives: Glosses and Annotations\n",
    "   englishGloss: Union[str,bool] = Field(...,min_length = 1)\n",
    "   frenchGloss: Union[str,bool] = Field(...,min_length = 1)\n",
    "   FulaAnnotations: Union[List[str],bool] #= False #article\n",
    "\n",
    "   #Gloss Derivatives: senses\n",
    "   FulaSenseEnglish: Union[List[str],bool] = False #list of \"senses\" split by semicolons #article\n",
    "   FulaSenseEnglish_Count: Union[int,bool] = False #number of senses in english #aka FulaSenseClassifications #article\n",
    "   FulaSenseEnglish_Annotations: Union[List[str],bool] = False #contains the annotations (in parenthesis) for a given sense, if any #article \n",
    "   _FulaSenseEnglish_Synonyms: Union[List[str],bool] = False #holds bracket text for a sense, suspected all synonyms. These may all occur at the end, and may be redundant with the synonyms provided at the head of the entry\n",
    "   _FulaSenseEnglish_Remainder: Union[List[str],bool] = False #holds any run text that is not contained in the above features\n",
    "\n",
    "   FulaSenseFrench: Union[List[str],bool] = False #article\n",
    "   FulaSenseFrench_Count: Union[int,bool] = False #aka FulaSenseClassifications\n",
    "   FulaSenseFrench_Annotations: Union[List[str],bool] = False\n",
    "   _FulaSenseFrench_Synonyms: Union[List[str],bool] = False\n",
    "   _FulaSenseFrench_Remainder: Union[List[str],bool] = False\n",
    "   \n",
    "   #lemma derivative\n",
    "   lemmaLine_runs: List[str] #= Field(...)\n",
    "   # lemmaLine_remainder: List[str] #= Field(...)\n",
    "   FulaDialects: Union[List[str],bool] = False #article\n",
    "   FulaPOSTags: Union[List[str],bool] = False #article\n",
    "   FulaPOSClass: Union[List[str],bool] = False #Noun, Adj, Verb, Adv, Prn, ..., Complicated, Indeterminate\n",
    "   FulaNoun_NounsAndClass: Union[List[Tuple[str,str]],bool] = False #Pular has unique noun classes. Lemma will be copied here beside its class (\"noun\", \"nounclass\"), and any additional singular forms will be in additional tuples in this same list\n",
    "   FulaNoun_PluralsAndClass: Union[List[Tuple[str,str]],bool] = False #Dict provides plurals for nouns. Tuples will have (\"noun\", \"nounclass\")\n",
    "   FulaSynonyms: Union[List[str],bool] = False #article\n",
    "   FulaCrossRef: Union[List[str],bool] = False\n",
    "         # FulaVerbClass:\n",
    "   \n",
    "   class Config:\n",
    "      validate_all = True\n",
    "      # extra = 'forbid'\n",
    "      validate_assignment = True\n",
    "      smart_union = True  \n",
    "\n",
    "   @root_validator(pre=True)\n",
    "   def _validate_and_build(cls, values: Dict[str,Any]) -> Dict[str, Any]:\n",
    "      # print(values)\n",
    "      newValues: Dict[str,Any] = {}\n",
    "      newValues['irregularities'] = []\n",
    "      # newValues[] = getAssert(values,,)\n",
    "      newValues['paragraphs'] = getAssert(values,'paragraphs',list)\n",
    "      if len(newValues['paragraphs']) == 0:\n",
    "         raise ValueError('given invalid paragraphs of zero length')\n",
    "      else: #logic for subsidary paragraphs\n",
    "         if len(newValues['paragraphs']) == 1:\n",
    "            newValues['englishGloss'] = False\n",
    "            newValues['frenchGloss'] = False\n",
    "         elif len(newValues['paragraphs']) == 2:\n",
    "            newValues['irregularities'].append('ambiguous: single subsidary paragraph')\n",
    "            newValues['englishGloss'] = False\n",
    "            newValues['frenchGloss'] = False\n",
    "         elif len(newValues['paragraphs']) == 3:\n",
    "            newValues['englishGloss'] = True # newValues['paragraphs'][1]\n",
    "            newValues['frenchGloss'] = True # newValues['paragraphs'][2]\n",
    "         else: \n",
    "            newValues['englishGloss'] = True # newValues['paragraphs'][1]\n",
    "            newValues['frenchGloss'] = True # newValues['paragraphs'][2]\n",
    "            newValues['FulaAnnotations'] = True # newValues['paragraphs'][3:]\n",
    "      \n",
    "\n",
    "      [print(k,v) for k,v in newValues.items()]\n",
    "      return newValues\n",
    "\n",
    "   def parse_senses(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_glossRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLine(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLineRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def get_rootOrigin(self):\n",
    "      #TODO\n",
    "      return\n",
    "   \n",
    "\n",
    "   def give_entryText(self, joiner = '\\t') -> str: #article\n",
    "      return joiner.join([para.trustyGet('para_text') for para in self.paragraphs])\n",
    "\n",
    "   def give_root_text(self, main = False, sub = False, both = True) -> str: #article\n",
    "      if both:\n",
    "         return ''.join([r[1] for r in self.root])\n",
    "      if main:\n",
    "         return  str(self.root[0][1])\n",
    "      if sub:\n",
    "         return str(self.root[1][1])\n",
    "      raise RuntimeError('did not give valid input')\n",
    "\n",
    "   def trustyGet(self, feat: str, silent_return = True) -> str:\n",
    "      if not silent_return:\n",
    "         output : str = getattr(self,feat,'')\n",
    "         if output:\n",
    "            return output\n",
    "         else:\n",
    "            raise AttributeError(f'trusty getter could not find: {feat}')\n",
    "      else:\n",
    "         output : str = getattr(self,feat,'')\n",
    "         return output\n",
    "      \n",
    "            \n",
    "dict_entry.parse_obj({'paragraphs':[b for a,b in parsed_object_list[5:8]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('parsed_objectClass_outcomes_dict.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze\n",
    "    parsed_objectClass_outcomes_dict = pickle.load(file, encoding='utf-8')\n",
    "\n",
    "parsed_object_list = parsed_objectClass_outcomes_dict['parsed_object_list'] \n",
    "para_text_lookup = parsed_objectClass_outcomes_dict['para_text_lookup'] \n",
    "root_ind_list = parsed_objectClass_outcomes_dict['root_ind_list'] \n",
    "subroot_ind_list = parsed_objectClass_outcomes_dict['subroot_ind_list'] \n",
    "lemma_ind_list = parsed_objectClass_outcomes_dict['lemma_ind_list'] \n",
    "root_and_lemma_one_line = parsed_objectClass_outcomes_dict['root_and_lemma_one_line'] \n",
    "root_lookup = parsed_objectClass_outcomes_dict['root_lookup'] \n",
    "lemma_lookup = parsed_objectClass_outcomes_dict['lemma_lookup'] \n",
    "# char_counts = parsed_objectClass_outcomes_dict['char_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paragraphs': [(9, Docx_Paragraph_and_Runs(para_text='your (sg.) (only with certain nouns such as those which refer to close family members)', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt'], run_bold=[None], run_font_size_pt=[None], paragraph_enumeration=9, run_italic=[None], run_text=['your (sg.) (only with certain nouns such as those which refer to close family members)'], para_left_indent=174193200)), (10, Docx_Paragraph_and_Runs(para_text='ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt'], run_bold=[None], run_font_size_pt=[None], paragraph_enumeration=10, run_italic=[None], run_text=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'], para_left_indent=174193200)), (11, Docx_Paragraph_and_Runs(para_text='aan  prn,ind  DFZH  [an]:Z<->', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt'], run_bold=[True, None, None], run_font_size_pt=[None, None, 8.0], paragraph_enumeration=11, run_italic=[None, True, None], run_text=['aan  ', 'prn,ind  ', 'DFZH  [an]:Z<->'], para_left_indent=116128800))]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_entry()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = []\n",
    "# for i in parsed_object_list[5:15]:\n",
    "#    result.append(dict_entry({i))\n",
    "dict_entry.parse_obj({'paragraphs':parsed_object_list[5:8]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
