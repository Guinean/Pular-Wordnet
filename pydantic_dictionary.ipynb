{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Create an easy/safe API to parse/hold/yield constructions of pydantic docx class objects.\n",
    "In theory this can be generalized and abstracted since the classes are largely absracted. \n",
    "However since this is API based, I will just focus on solutions for THIS dictionary task. These can be expanded later, or just have other APIs added\n",
    "Final objective is have a tool to pass complex commands/instructions, and have it yield a pandas dataframe of the results.\n",
    "Intermediate steps may be yield a list of lists\n",
    "I am not yet sure if there is benefit trying to have the whole dictionary in one notional object. \n",
    "I don't think so, since each operation is psuedo atomic, and any aggregate actions are intended to be done in something like pandas.\n",
    "Eventually I would want a loop for pandas operations to easily feed updates to the data. \n",
    "'''\n",
    "conclusion = '''create a pular dictionary specifc datastructure to handle the aggregate paragraphs. \n",
    "This will only be per lemma, \n",
    "as any aggregations from roots and relations of that sort can be managed in pandas and created by simple inheretance/reference\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "import pydantic\n",
    "from pydantic import ValidationError, validator, root_validator, Field, constr\n",
    "from pydantic_docx import Docx_Paragraph_and_Runs #type:ignore\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "   \n",
    "   'txt_FulaLemmas',\n",
    "   'txt_FulaRoots',\n",
    "   \n",
    "   #extra\n",
    "   'txt_FulaAnnotations',\n",
    "   #gloss derivatives\n",
    "   'txt_FulaSenseEnglish',\n",
    "   'txt_FulaSenseFrench',\n",
    "   'txt_FulaSenseClassifications',\n",
    "   'txt_FulaSenseEnglishAnnotations',\n",
    "   'txt_FulaSenseFrenchAnnotations',\n",
    "\n",
    "   #lemma derivative\n",
    "   'txt_FulaDialects',\n",
    "   'txt_FulaPOSTags',\n",
    "   'txt_FulaSynonyms',\n",
    "\n",
    "\n",
    "   #agg\n",
    "   'txt_FulaInParenthesis',\n",
    "   'txt_POS',\n",
    "   'txt_RootOrigins'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('n\\\\.|(?<=\\\\()[^\\\\)]+|\\\\/')\n",
      "['n.', 'wsh', '/']\n",
      "['n.', 'wsh', '/']\n"
     ]
    }
   ],
   "source": [
    "noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "print(nounPatternRegex)\n",
    "s = 'n. dfhjkgqh (wsh) askldjhf / asdfhjkkh'\n",
    "print(re.findall(nounPatternRegex,s))\n",
    "print(nounPatternRegex.findall(s))\n",
    "\n",
    "# if /, the next word is a plural. The plural also then usually has a (el) class after it. commas are multiple version\n",
    "# so each lemma noun may have a (class). The lemma may have one or multiple plurals, which will have a modified (class)\n",
    "# so have a nouns list? nouns may then be \n",
    "# [('n.',{bool}), #only needed if this gets added to all POS? Or maybe theres information in its use?\n",
    "# ([[{word}],{Optional[class]}]),  #singular\n",
    "   #([[{word}],{Optional[class]}])]   #plural\n",
    "\n",
    "# so nounStruct[0][1] is the flag for noun?\n",
    "   # one POS table, and noun will get added there. But the presence of a noun indicates to look here\n",
    "# classes may be found [sClass[1] for sClass in nounStruct[1]] will give all classes of singular, [sClass[1] for sClass in nounStruct[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAssert(dct,val,typ:type) -> Any:\n",
    "   out = dct.get(val,False)\n",
    "   assert isinstance(out,typ), f\"the provided dict {dct} did not give {val} with the expected type: {typ}\"\n",
    "   return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (915979843.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22500\\915979843.py\"\u001b[1;36m, line \u001b[1;32m77\u001b[0m\n\u001b[1;33m    for ind in root_container\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class dict_entry(pydantic.BaseModel): \n",
    "   '''\n",
    "   #True is used as default value, and must be changed to False if the feature is not found. This will catch not implemented, and incomplete updates\n",
    "   '''\n",
    "   irregularities: List[str] = []\n",
    "   paragraphs: List[Docx_Paragraph_and_Runs] = Field(...,min_items = 1)\n",
    "   lemma: str = Field(...,min_length = 1) #article\n",
    "   root_withContext: List[Tuple[int,str,str]] #index,root_text, root_meta_data #article\n",
    "   root_text : str = Field(...,min_length = 1)\n",
    "   root_origin: Union[str,bool] = True\n",
    "   #Paragraph Derivatives: Glosses and Annotations\n",
    "   lemmaLine_runs: List[str] #these will not be updated after initialization\n",
    "   englishGlossLine_runs: Union[List[str],bool] = True #these will not be updated after initialization\n",
    "   frenchGlossLine_runs: Union[List[str],bool] = True #these will not be updated after initialization\n",
    "   FulaAnnotations_runs: Union[List[List[str]],bool]= True #these will not be updated after initialization\n",
    "\n",
    "   #Gloss Derivatives: English Senses\n",
    "   FulaSenseEnglish: Union[List[str],bool] = True #list of \"senses\" split by semicolons #article\n",
    "   FulaSenseEnglish_Count: Union[int,bool] = True #number of senses in english #aka FulaSenseClassifications #article\n",
    "   FulaSenseEnglish_Annotations: Union[List[str],bool] = True #contains the annotations (in parenthesis) for a given sense, if any #article \n",
    "   FulaSenseEnglish_Synonyms: Union[List[str],bool] = True #holds bracket text for a sense, suspected all synonyms. These may all occur at the end, and may be redundant with the synonyms provided at the head of the entry\n",
    "   FulaSenseEnglish_unusedContent: Union[List[str],bool] = True #holds any run text that is not contained in the above features\n",
    "   \n",
    "   #Gloss Derivatives: French Senses\n",
    "   FulaSenseFrench: Union[List[str],bool] = True #article\n",
    "   FulaSenseFrench_Count: Union[int,bool] = True #aka FulaSenseClassifications\n",
    "   FulaSenseFrench_Annotations: Union[List[str],bool] = True\n",
    "   FulaSenseFrench_Synonyms: Union[List[str],bool] = True\n",
    "   FulaSenseFrench_unusedContent: Union[List[str],bool] = True\n",
    "   \n",
    "   #lemma derivative\n",
    "   lemmaLine_unusedContent: List[str] #= Field(...)\n",
    "   FulaDialects: Union[List[str],bool] = True #article\n",
    "   FulaPOSTags: Union[List[str],bool] = True #article\n",
    "   FulaPOSClass: Union[List[str],bool] = True #Noun, Adj, Verb, Adv, Prn, ..., Complicated, Indeterminate\n",
    "   FulaNoun_NounsAndClass: Union[List[Tuple[str,str]],bool] = True #Pular has unique noun classes. Lemma will be copied here beside its class (\"noun\", \"nounclass\"), and any additional singular forms will be in additional tuples in this same list\n",
    "   FulaNoun_PluralsAndClass: Union[List[Tuple[str,str]],bool] = True #Dict provides plurals for nouns. Tuples will have (\"noun\", \"nounclass\")\n",
    "   FulaSynonyms: Union[List[str],bool] = True #article\n",
    "   FulaCrossRef: Union[List[str],bool] = True\n",
    "         # FulaVerbClass:\n",
    "   \n",
    "   class Config:\n",
    "      validate_all = True\n",
    "      validate_assignment = True\n",
    "      smart_union = True  \n",
    "      extra = 'forbid'\n",
    "\n",
    "   @root_validator(pre=True)\n",
    "   def _validate_and_build(cls, values: Dict[str,Any]) -> Dict[str, Any]:\n",
    "      # print(values)\n",
    "      newValues: Dict[str,Any] = {}\n",
    "      newValues['irregularities'] = []\n",
    "      # newValues[] = getAssert(values,,)\n",
    "      newValues['paragraphs'] = getAssert(values,'paragraphs',list)\n",
    "      if len(newValues['paragraphs']) == 0:\n",
    "         raise ValueError('given invalid paragraphs of zero length')\n",
    "      else: #logic for subsidary paragraphs\n",
    "         lemma_line = newValues['paragraphs'][0].trustyGet(feat = 'run_text', silent_return=False)\n",
    "         newValues['lemmaLine_runs'] = lemma_line\n",
    "         if len(newValues['paragraphs']) == 1:\n",
    "            newValues['englishGlossLine_runs'] = False\n",
    "            newValues['frenchGlossLine_runs'] = False\n",
    "         elif len(newValues['paragraphs']) == 2:\n",
    "            newValues['irregularities'].append('What:ambiguous, Where:paragraphs, Why: only one')\n",
    "            newValues['englishGlossLine_runs'] = False\n",
    "            newValues['frenchGlossLine_runs'] = False\n",
    "         elif len(newValues['paragraphs']) == 3:\n",
    "            newValues['englishGlossLine_runs'] = newValues['paragraphs'][1].get_run_text()\n",
    "            newValues['frenchGlossLine_runs'] = newValues['paragraphs'][2].get_run_text()\n",
    "         else: \n",
    "            newValues['englishGlossLine_runs'] = newValues['paragraphs'][1].get_run_text()\n",
    "            newValues['frenchGlossLine_runs'] = newValues['paragraphs'][2].get_run_text()\n",
    "            newValues['FulaAnnotations_runs'] = newValues['paragraphs'][3:].get_run_text()\n",
    "      \n",
    "      lemma_index, lemma_text, lemma_mask = getAssert(values,'lemma',list)\n",
    "      root_container = [[ind, text, mask] for ind, text, mask in getAssert(values,'root',list)]\n",
    "      \n",
    "      is_single_root: bool = len(root_container) == 1\n",
    "      root_lemma_sharing_line: bool = any([ind == lemma_index for ind, t, m in root_container])\n",
    "\n",
    "      #determining unused lemma-line contents\n",
    "      used_mask = lemma_mask.copy()\n",
    "      for root in root_container:\n",
    "         if root[0] == lemma_index: #root and lemma share a line\n",
    "            used_mask = [any(ind) for ind in list(zip(lemma_mask,root[2]))]\n",
    "         \n",
    "      # if root_lemma_sharing_line:\n",
    "      #    combined_mask\n",
    "      # for ind in root_container:\n",
    "\n",
    "      # lemmaLine_unusedContent = []\n",
    "      # root_index: list = \n",
    "      # if (root_container[]\n",
    "      [print(k,v) for k,v in newValues.items()]\n",
    "      return newValues\n",
    "\n",
    "   def parse_senses(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_glossRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLine(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLineRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def get_rootOrigin(self):\n",
    "      #TODO\n",
    "      return\n",
    "   \n",
    "\n",
    "   def give_entryText(self, joiner = '\\t') -> str: #article\n",
    "      return joiner.join([para.trustyGet('para_text') for para in self.paragraphs])\n",
    "\n",
    "   def give_root_text(self, main = False, sub = False, both = True) -> str: #article\n",
    "      if both:\n",
    "         return ''.join([r[1] for r in self.root])\n",
    "      if main:\n",
    "         return  str(self.root[0][1])\n",
    "      if sub:\n",
    "         return str(self.root[1][1])\n",
    "      raise RuntimeError('did not give valid input')\n",
    "\n",
    "   def trustyGet(self, feat: str, silent_return = True) -> str:\n",
    "      if not silent_return:\n",
    "         output : str = getattr(self,feat,'')\n",
    "         if len(output) > 0:\n",
    "            return output\n",
    "         else:\n",
    "            raise AttributeError(f'trusty getter could not find: {feat}')\n",
    "      else:\n",
    "         output : str = getattr(self,feat,'')\n",
    "         return output\n",
    "      \n",
    "            \n",
    "dict_entry.parse_obj({'paragraphs':[b for a,b in parsed_object_list[5:8]]})\n",
    "\n",
    "incoming = {\n",
    "   'paragraphs': List[Docx_Paragraph_and_Runs],\n",
    "   'root': List[int,str,List[bool]],\n",
    "   'lemma': List[int,str,List[bool]],\n",
    "   # 'pos': \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, False, False]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = [True, False, False, False]\n",
    "b = [False, True, False, False]\n",
    "[any(ind) for ind in list(zip(a,b))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('parsed_objectClass_outcomes_dict.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze\n",
    "    parsed_objectClass_outcomes_dict = pickle.load(file, encoding='utf-8')\n",
    "\n",
    "parsed_object_list = parsed_objectClass_outcomes_dict['parsed_object_list'] \n",
    "para_text_lookup = parsed_objectClass_outcomes_dict['para_text_lookup'] \n",
    "root_ind_list = parsed_objectClass_outcomes_dict['root_ind_list'] \n",
    "subroot_ind_list = parsed_objectClass_outcomes_dict['subroot_ind_list'] \n",
    "lemma_ind_list = parsed_objectClass_outcomes_dict['lemma_ind_list'] \n",
    "root_and_lemma_one_line = parsed_objectClass_outcomes_dict['root_and_lemma_one_line'] \n",
    "root_lookup = parsed_objectClass_outcomes_dict['root_lookup'] \n",
    "lemma_lookup = parsed_objectClass_outcomes_dict['lemma_lookup'] \n",
    "# char_counts = parsed_objectClass_outcomes_dict['char_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paragraphs': [(9, Docx_Paragraph_and_Runs(para_text='your (sg.) (only with certain nouns such as those which refer to close family members)', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt'], run_bold=[None], run_font_size_pt=[None], paragraph_enumeration=9, run_italic=[None], run_text=['your (sg.) (only with certain nouns such as those which refer to close family members)'], para_left_indent=174193200)), (10, Docx_Paragraph_and_Runs(para_text='ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt'], run_bold=[None], run_font_size_pt=[None], paragraph_enumeration=10, run_italic=[None], run_text=['ton, ta, tes (seulement pour certains substantifs, tels ceux qui se rapportent aux membres de la proche famille)'], para_left_indent=174193200)), (11, Docx_Paragraph_and_Runs(para_text='aan  prn,ind  DFZH  [an]:Z<->', para_first_line_indent=-91440, run_font_name=['TmsRmn 10pt', 'TmsRmn 10pt', 'Helv 8pt'], run_bold=[True, None, None], run_font_size_pt=[None, None, 8.0], paragraph_enumeration=11, run_italic=[None, True, None], run_text=['aan  ', 'prn,ind  ', 'DFZH  [an]:Z<->'], para_left_indent=116128800))]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_entry()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = []\n",
    "# for i in parsed_object_list[5:15]:\n",
    "#    result.append(dict_entry({i))\n",
    "dict_entry.parse_obj({'paragraphs':parsed_object_list[5:8]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
