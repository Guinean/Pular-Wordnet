{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention = '''Create an easy/safe API to parse/hold/yield constructions of pydantic docx class objects.\n",
    "In theory this can be generalized and abstracted since the classes are largely absracted. \n",
    "However since this is API based, I will just focus on solutions for THIS dictionary task. These can be expanded later, or just have other APIs added\n",
    "Final objective is have a tool to pass complex commands/instructions, and have it yield a pandas dataframe of the results.\n",
    "Intermediate steps may be yield a list of lists\n",
    "I am not yet sure if there is benefit trying to have the whole dictionary in one notional object. \n",
    "I don't think so, since each operation is psuedo atomic, and any aggregate actions are intended to be done in something like pandas.\n",
    "Eventually I would want a loop for pandas operations to easily feed updates to the data. \n",
    "'''\n",
    "conclusion = '''create a pular dictionary specifc datastructure to handle the aggregate paragraphs. \n",
    "This will only be per lemma, \n",
    "as any aggregations from roots and relations of that sort can be managed in pandas and created by simple inheretance/reference\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Any, Union, Tuple\n",
    "import pydantic\n",
    "from pydantic import ValidationError, validator, root_validator, Field, constr\n",
    "from pydantic_docx import Docx_Paragraph_and_Runs #type:ignore\n",
    "import re\n",
    "import json\n",
    "from itertools import compress, chain\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "   \n",
    "   'txt_FulaLemmas',\n",
    "   'txt_FulaRoots',\n",
    "   \n",
    "   #extra\n",
    "   'txt_FulaAnnotations',\n",
    "   #gloss derivatives\n",
    "   'txt_FulaSenseEnglish',\n",
    "   'txt_FulaSenseFrench',\n",
    "   'txt_FulaSenseClassifications',\n",
    "   'txt_FulaSenseEnglishAnnotations',\n",
    "   'txt_FulaSenseFrenchAnnotations',\n",
    "\n",
    "   #lemma derivative\n",
    "   'txt_FulaDialects',\n",
    "   'txt_FulaPOSTags',\n",
    "   'txt_FulaSynonyms',\n",
    "\n",
    "\n",
    "   #agg\n",
    "   'txt_FulaInParenthesis',\n",
    "   'txt_POS',\n",
    "   'txt_RootOrigins'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('n\\\\.|(?<=\\\\()[^\\\\)]+|\\\\/')\n",
      "['n.', 'wsh', '/']\n",
      "['n.', 'wsh', '/']\n"
     ]
    }
   ],
   "source": [
    "noun_patterns = [r'n\\.',r\"(?<=\\()[^\\)]+\",r\"\\/\"]\n",
    "nounPatternRegex = re.compile('|'.join([p for p in noun_patterns]))\n",
    "print(nounPatternRegex)\n",
    "s = 'n. dfhjkgqh (wsh) askldjhf / asdfhjkkh'\n",
    "print(re.findall(nounPatternRegex,s))\n",
    "print(nounPatternRegex.findall(s))\n",
    "\n",
    "# if /, the next word is a plural. The plural also then usually has a (el) class after it. commas are multiple version\n",
    "# so each lemma noun may have a (class). The lemma may have one or multiple plurals, which will have a modified (class)\n",
    "# so have a nouns list? nouns may then be \n",
    "# [('n.',{bool}), #only needed if this gets added to all POS? Or maybe theres information in its use?\n",
    "# ([[{word}],{Optional[class]}]),  #singular\n",
    "   #([[{word}],{Optional[class]}])]   #plural\n",
    "\n",
    "# so nounStruct[0][1] is the flag for noun?\n",
    "   # one POS table, and noun will get added there. But the presence of a noun indicates to look here\n",
    "# classes may be found [sClass[1] for sClass in nounStruct[1]] will give all classes of singular, [sClass[1] for sClass in nounStruct[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# current_time = now.strftime(\"%Y-%m-%d_-_%H-%M-%S\")\n",
    "# print(f\"Experiment time: {current_time}\\nExperiment note: {experiment}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAssert(dct,val,typ:type) -> Any:\n",
    "   out = dct.get(val,False)\n",
    "   assert isinstance(out,typ), f\"the provided dict {dct} did not give {val} with the expected type: {typ}\"\n",
    "   return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (915979843.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22500\\915979843.py\"\u001b[1;36m, line \u001b[1;32m77\u001b[0m\n\u001b[1;33m    for ind in root_container\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class dict_entry(pydantic.BaseModel): \n",
    "   '''\n",
    "   #True is used as default value, and must be changed to False if the feature is not found. This will catch not implemented, and incomplete updates\n",
    "   '''\n",
    "   irregularities: List[str] = []\n",
    "   lemma_index: int \n",
    "   paragraphs: List[Docx_Paragraph_and_Runs] = Field(...,min_items = 1)\n",
    "   lemma: str = Field(...,min_length = 1) #article\n",
    "   root: str\n",
    "   root_subpiece: Union[str,bool] = True #these will not be updated after initialization\n",
    "   root_metadata: Dict[str,Any]  #index,root_text, root_meta_data #article \n",
    "   root_origin: Union[str,bool] = True #these will not be updated after initialization\n",
    "   #Paragraph Derivatives: Glosses and Annotations\n",
    "   lemmaLine_runs: List[str] #these will not be updated after initialization\n",
    "   englishGlossLine_runs: Union[List[str],bool] = True #these will not be updated after initialization\n",
    "   frenchGlossLine_runs: Union[List[str],bool] = True #these will not be updated after initialization\n",
    "   FulaAnnotations_runs: Union[List[List[str]],bool]= True #these will not be updated after initialization\n",
    "\n",
    "   #Gloss Derivatives: English Senses\n",
    "   FulaSenseEnglish: Union[List[str],bool] = True #list of \"senses\" split by semicolons #article\n",
    "   FulaSenseEnglish_Count: Union[int,bool] = True #number of senses in english #aka FulaSenseClassifications #article\n",
    "   FulaSenseEnglish_Annotations: Union[List[str],bool] = True #contains the annotations (in parenthesis) for a given sense, if any #article \n",
    "   FulaSenseEnglish_Synonyms: Union[List[str],bool] = True #holds bracket text for a sense, suspected all synonyms. These may all occur at the end, and may be redundant with the synonyms provided at the head of the entry\n",
    "   FulaSenseEnglish_unusedContent: Union[List[str],bool] = True #holds any run text that is not contained in the above features\n",
    "   \n",
    "   #Gloss Derivatives: French Senses\n",
    "   FulaSenseFrench: Union[List[str],bool] = True #article\n",
    "   FulaSenseFrench_Count: Union[int,bool] = True #aka FulaSenseClassifications\n",
    "   FulaSenseFrench_Annotations: Union[List[str],bool] = True\n",
    "   FulaSenseFrench_Synonyms: Union[List[str],bool] = True\n",
    "   FulaSenseFrench_unusedContent: Union[List[str],bool] = True\n",
    "   \n",
    "   #lemma derivative\n",
    "   lemmaLine_unusedContent: List[Tuple[int,str]] #= Field(...) #int is index for run to allow tracing as entries are removed\n",
    "   FulaDialects: Union[List[str],bool] = True #article\n",
    "   FulaPOSTags: Union[List[str],bool] = True #article\n",
    "   FulaPOSClass: Union[List[str],bool] = True #Noun, Adj, Verb, Adv, Prn, ..., Complicated, Indeterminate\n",
    "   FulaNoun_NounsAndClass: Union[List[Tuple[str,str]],bool] = True #Pular has unique noun classes. Lemma will be copied here beside its class (\"noun\", \"nounclass\"), and any additional singular forms will be in additional tuples in this same list\n",
    "   FulaNoun_PluralsAndClass: Union[List[Tuple[str,str]],bool] = True #Dict provides plurals for nouns. Tuples will have (\"noun\", \"nounclass\")\n",
    "   FulaSynonyms: Union[List[str],bool] = True #article\n",
    "   FulaCrossRef: Union[List[str],bool] = True\n",
    "         # FulaVerbClass:\n",
    "   \n",
    "   class Config:\n",
    "      validate_all = True\n",
    "      validate_assignment = True\n",
    "      smart_union = True  \n",
    "      extra = 'forbid'\n",
    "\n",
    "   @root_validator(pre=True)\n",
    "   def _validate_and_build(cls, values: Dict[str,Any]) -> Dict[str, Any]:\n",
    "      # print(values)\n",
    "      newValues: Dict[str,Any] = {}\n",
    "      newValues['irregularities'] = []\n",
    "\n",
    "      ###PARAGRAPHS CHECK###\n",
    "      newValues['paragraphs'] = getAssert(values,'paragraphs',list)\n",
    "      if len(newValues['paragraphs']) == 0:\n",
    "         raise ValueError('given invalid paragraphs of zero length')\n",
    "      else: #logic for subsidary paragraphs\n",
    "         lemma_line = newValues['paragraphs'][0].trustyGet(feat = 'run_text', silent_return=False)\n",
    "         newValues['lemmaLine_runs'] = lemma_line\n",
    "         if len(newValues['paragraphs']) == 1:\n",
    "            newValues['englishGlossLine_runs'] = False\n",
    "            newValues['frenchGlossLine_runs'] = False\n",
    "         elif len(newValues['paragraphs']) == 2:\n",
    "            newValues['irregularities'].append('What:ambiguous, Where:paragraphs, Why: only one')\n",
    "            newValues['englishGlossLine_runs'] = False\n",
    "            newValues['frenchGlossLine_runs'] = False\n",
    "         elif len(newValues['paragraphs']) == 3:\n",
    "            newValues['englishGlossLine_runs'] = newValues['paragraphs'][1].get_run_text()\n",
    "            newValues['frenchGlossLine_runs'] = newValues['paragraphs'][2].get_run_text()\n",
    "         else: \n",
    "            newValues['englishGlossLine_runs'] = newValues['paragraphs'][1].get_run_text()\n",
    "            newValues['frenchGlossLine_runs'] = newValues['paragraphs'][2].get_run_text()\n",
    "            newValues['FulaAnnotations_runs'] = newValues['paragraphs'][3:].get_run_text()\n",
    "      \n",
    "      ###LEMMA CHECK###\n",
    "      #reading in lemma results from first pass of pydantic parser\n",
    "      lemma_index, lemma_mask, lemmaLine_runs = getAssert(values,'lemma',list)\n",
    "      #checking for correct structure\n",
    "      lemma_matched_runs = list(compress(lemmaLine_runs,lemma_mask))\n",
    "      if len(lemma_matched_runs) > 1:\n",
    "         newValues['irregularities'].append('What:Unexpected, Where: Lemmas, Why: the merge routines should aggregate adjacent runs with same features. Multiple Lemma runs should not be possible if Bold is contiguous')\n",
    "      lemma_text = ''.join(chain(lemma_matched_runs)).strip()\n",
    "         #TODO have better control for expected structure that these runs should be adjacent (and only should be one)\n",
    "      #saving values. These will have to pass type check of declared attribute types for validation to succeed\n",
    "      newValues['lemma'] = lemma_text\n",
    "      newValues['lemma_index'] = lemma_index\n",
    "      newValues['lemmaLine_runs'] = lemmaLine_runs\n",
    "      used_run_mask = lemma_mask\n",
    "      ###ROOT CHECK###\n",
    "      #reading in lemma results from first pass of pydantic parser\n",
    "      root_container = [(ind, mask, rootLine_runs) for ind, mask, rootLine_runs in getAssert(values,'root',list)]\n",
    "      #checking for correct structure\n",
    "      root_index, root_mask, rootLine_runs = root_container[0]\n",
    "      root_matched_runs = list(compress(rootLine_runs,root_mask))\n",
    "      if len(root_matched_runs) > 1:\n",
    "         newValues['irregularities'].append('What:Unexpected, Where: Root, Why: the merge routines should aggregate adjacent runs with same features. Multiple Root runs should not be possible if Fontsize is contiguous and unique')\n",
    "      root_text = ''.join(chain(root_matched_runs)).strip()\n",
    "      if root_index == newValues['lemma_index']:\n",
    "         newValues['irregularities'].append('What:inconsistent, Where: Lemmas and Root, Why:normally root and lemma are on different lines. This has them sharing, which may indicate a lack of other content')\n",
    "         used_run_mask = [any(run) for run in list(zip(used_run_mask,root_mask))]\n",
    "         rootLine_runs = False\n",
    "      newValues['root'] = root_text\n",
    "      newValues['root_metadata'] = {'root_index': root_index, 'root_runs':rootLine_runs}\n",
    "      ###ROOT-Subpiece CHECK###\n",
    "      #iterating in case a subroot is present\n",
    "      if len(root_container) == 2:\n",
    "         root_index, root_mask, rootLine_runs = root_container[1]\n",
    "         root_matched_runs = list(compress(rootLine_runs,root_mask))\n",
    "         if len(root_matched_runs) > 1:\n",
    "            newValues['irregularities'].append('What:Unexpected, Where: Root-Subpiece, Why: the merge routines should aggregate adjacent runs with same features. Multiple Root runs should not be possible if Fontsize is contiguous and unique')\n",
    "         root_subpiece_text = ''.join(chain(root_matched_runs)).strip()\n",
    "         if root_index == newValues['lemma_index']:\n",
    "            newValues['irregularities'].append('What:inconsistent, Where: Lemmas and Root-Subpiece, Why:normally root and lemma are on different lines. This has them sharing, which may indicate a lack of other content')\n",
    "            used_run_mask = [any(run) for run in list(zip(used_run_mask,root_mask))]\n",
    "            rootLine_runs = False\n",
    "         newValues['root_subpiece'] = root_subpiece_text\n",
    "         newValues['root_metadata'] = {'root_subpiece_index': root_index, 'root_subpiece_runs':rootLine_runs}\n",
    "      else:\n",
    "         newValues['root_subpiece'] = False\n",
    "      #ROOT INPUT INTEGRITY CHECK\n",
    "      #controlling for unconstrained input root_container list\n",
    "      if len(root_container) > 2:\n",
    "         raise NotImplementedError('more than two roots were passed. This is either incorrect or not supported')\n",
    "      \n",
    "      #determining lemmaLine_unusedContent\n",
    "      unused_run_mask = [not r for r in used_run_mask]\n",
    "      newValues['lemmaLine_unusedContent'] = compress(enumerate(newValues['lemmaLine_runs']),unused_run_mask)\n",
    "      \n",
    "      return newValues\n",
    "\n",
    "   def parse_senses(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_glossRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLine(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def parse_lemmaLineRemainder(self):\n",
    "      #TODO\n",
    "      return\n",
    "\n",
    "   def get_rootOrigin(self):\n",
    "      #TODO\n",
    "      return\n",
    " \n",
    "   def give_entryText(self, joiner = '\\t') -> str: #article\n",
    "      return joiner.join([para.trustyGet('para_text') for para in self.paragraphs])\n",
    "\n",
    "   def trustyGet(self, feat: str, silent_return = True) -> str:\n",
    "      if not silent_return:\n",
    "         output : str = getattr(self,feat,'')\n",
    "         if len(output) > 0:\n",
    "            return output\n",
    "         else:\n",
    "            raise AttributeError(f'trusty getter could not find: {feat}')\n",
    "      else:\n",
    "         output : str = getattr(self,feat,'')\n",
    "         return output\n",
    "      \n",
    "            \n",
    "dict_entry.parse_obj({'paragraphs':[b for a,b in parsed_object_list[5:8]]})\n",
    "\n",
    "# incoming = {\n",
    "#    'paragraphs': List[Docx_Paragraph_and_Runs],\n",
    "#    'root': List[int,str,List[bool]],\n",
    "#    'lemma': List[int,str,List[bool]],\n",
    "#    # 'pos': \n",
    "# }\n",
    "\n",
    "# [\n",
    "#    ['paras'], #obj\n",
    "#    ['parsed_paras'], #pydocx\n",
    "#    ['roots'], #name, fill forwards\n",
    "#    ['lemmas']  #text, fill forwards\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('parsed_objectClass_outcomes_dict.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze\n",
    "    parsed_objectClass_outcomes_dict = pickle.load(file, encoding='utf-8')\n",
    "\n",
    "parsed_object_list = parsed_objectClass_outcomes_dict['parsed_object_list'] \n",
    "para_text_lookup = parsed_objectClass_outcomes_dict['para_text_lookup'] \n",
    "root_ind_list = parsed_objectClass_outcomes_dict['root_ind_list'] \n",
    "subroot_ind_list = parsed_objectClass_outcomes_dict['subroot_ind_list'] \n",
    "lemma_ind_list = parsed_objectClass_outcomes_dict['lemma_ind_list'] \n",
    "root_and_lemma_one_line = parsed_objectClass_outcomes_dict['root_and_lemma_one_line'] \n",
    "root_lookup = parsed_objectClass_outcomes_dict['root_lookup'] \n",
    "lemma_lookup = parsed_objectClass_outcomes_dict['lemma_lookup'] \n",
    "# char_counts = parsed_objectClass_outcomes_dict['char_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_entry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9000\\1222998942.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# for i in parsed_object_list[5:15]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#    result.append(dict_entry({i))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdict_entry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'paragraphs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mparsed_object_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_entry' is not defined"
     ]
    }
   ],
   "source": [
    "# result = []\n",
    "# for i in parsed_object_list[5:15]:\n",
    "#    result.append(dict_entry({i))\n",
    "# dict_entry.parse_obj({'paragraphs':parsed_object_list[5:8]})\n",
    "\n",
    "dct = {\n",
    "   'paragraphs':parsed_object_list[5:8],\n",
    "   'root': ()\n",
    "}\n",
    "ind, mask, rootLine_runs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('.pular_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923e031a042b0333d984d7caca79dafbd2f9b4aa22c38d0c8e773771fd0f73dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
